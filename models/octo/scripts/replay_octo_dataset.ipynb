{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-11 07:18:43.592019: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-02-11 07:18:43.592085: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-02-11 07:18:43.593875: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-11 07:18:45.090443: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/ubc/cs/research/ubc_ml/gguz/miniconda3/envs/octo-env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "pybullet build time: Jan 29 2025 23:16:28\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "from functools import partial\n",
    "import os\n",
    "\n",
    "from absl import app, flags, logging\n",
    "import flax\n",
    "from flax.traverse_util import flatten_dict\n",
    "import jax\n",
    "from jax.sharding import Mesh, NamedSharding, PartitionSpec\n",
    "from ml_collections import config_flags, ConfigDict\n",
    "import optax\n",
    "import tensorflow as tf\n",
    "import tqdm\n",
    "import wandb\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.append(\"../..\")\n",
    "sys.path.append(\"/ubc/cs/research/nlp/grigorii/projects/openvla/\")\n",
    "\n",
    "from octo.data.dataset import make_single_dataset\n",
    "from octo.model.octo_model import OctoModel\n",
    "from octo.utils.jax_utils import initialize_compilation_cache\n",
    "from octo.utils.spec import ModuleSpec\n",
    "from octo.utils.train_callbacks import (\n",
    "    RolloutVisualizationCallback,\n",
    "    SaveCallback,\n",
    "    ValidationCallback,\n",
    "    VisualizationCallback,\n",
    ")\n",
    "\n",
    "from openvla.experiments.robot.calvin.calvin_utils import get_calvin_env\n",
    "\n",
    "\n",
    "\n",
    "from octo.utils.train_utils import (\n",
    "    check_config_diff,\n",
    "    create_optimizer,\n",
    "    format_name_with_config,\n",
    "    merge_params,\n",
    "    process_text,\n",
    "    Timer,\n",
    "    TrainState,\n",
    ")\n",
    "\n",
    "try:\n",
    "    from jax_smi import initialise_tracking  # type: ignore\n",
    "\n",
    "    initialise_tracking()\n",
    "except ImportError:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Low-level actions one-by-one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiments.robot.calvin.multistep_sequences_low_level import get_low_level_sequences\n",
    "from torchvision.transforms import Resize\n",
    "from itertools import product\n",
    "import numpy as np\n",
    "from experiments.robot.calvin.calvin_utils import get_calvin_env, add_text\n",
    "from experiments.robot.calvin.rollout_video import RolloutVideo\n",
    "import hydra\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from experiments.robot.calvin.calvin_utils import get_calvin_env, resize_image, get_video_tag, count_success, get_env_state_for_initial_condition\n",
    "import torch\n",
    "import json\n",
    "from experiments.robot.robot_utils import (\n",
    "    DATE_TIME,\n",
    "    get_action,\n",
    "    get_image_resize_size,\n",
    "    get_model,\n",
    "    invert_gripper_action,\n",
    "    normalize_gripper_action,\n",
    "    set_seed_everywhere,\n",
    ")\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">02/11 [07:19:13] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> | &gt;&gt; No image inputs matching <span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'image_wrist'</span>,<span style=\"font-weight: bold\">)</span> were found.Skipping      <a href=\"file:///ubc/cs/research/nlp/grigorii/projects/octo_original/octo/octo/model/components/tokenizers.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">tokenizers.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///ubc/cs/research/nlp/grigorii/projects/octo_original/octo/octo/model/components/tokenizers.py#110\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">110</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                 </span>         tokenizer entirely.                                                     <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                 </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m02/11 [07:19:13]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m | >> No image inputs matching \u001b[1m(\u001b[0m\u001b[32m'image_wrist'\u001b[0m,\u001b[1m)\u001b[0m were found.Skipping      \u001b]8;id=937300;file:///ubc/cs/research/nlp/grigorii/projects/octo_original/octo/octo/model/components/tokenizers.py\u001b\\\u001b[2mtokenizers.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=703067;file:///ubc/cs/research/nlp/grigorii/projects/octo_original/octo/octo/model/components/tokenizers.py#110\u001b\\\u001b[2m110\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                 \u001b[0m         tokenizer entirely.                                                     \u001b[2m                 \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                 </span><span style=\"color: #808000; text-decoration-color: #808000\">WARNING </span> | &gt;&gt; Skipping observation tokenizer: obs_wrist                         <a href=\"file:///ubc/cs/research/nlp/grigorii/projects/octo_original/octo/octo/model/octo_module.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">octo_module.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///ubc/cs/research/nlp/grigorii/projects/octo_original/octo/octo/model/octo_module.py#195\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">195</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                \u001b[0m\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m | >> Skipping observation tokenizer: obs_wrist                         \u001b]8;id=131607;file:///ubc/cs/research/nlp/grigorii/projects/octo_original/octo/octo/model/octo_module.py\u001b\\\u001b[2mocto_module.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=820408;file:///ubc/cs/research/nlp/grigorii/projects/octo_original/octo/octo/model/octo_module.py#195\u001b\\\u001b[2m195\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                 </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> | &gt;&gt; repeating task tokens at each timestep to perform cross-modal     <a href=\"file:///ubc/cs/research/nlp/grigorii/projects/octo_original/octo/octo/model/octo_module.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">octo_module.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///ubc/cs/research/nlp/grigorii/projects/octo_original/octo/octo/model/octo_module.py#220\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">220</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                 </span>         attention                                                              <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                  </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m | >> repeating task tokens at each timestep to perform cross-modal     \u001b]8;id=646653;file:///ubc/cs/research/nlp/grigorii/projects/octo_original/octo/octo/model/octo_module.py\u001b\\\u001b[2mocto_module.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=346494;file:///ubc/cs/research/nlp/grigorii/projects/octo_original/octo/octo/model/octo_module.py#220\u001b\\\u001b[2m220\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                 \u001b[0m         attention                                                              \u001b[2m                  \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "checkpoint_path = '/ubc/cs/research/ubc_ml/gguz/exp_data/octo_finetune/experiment_2_20250207_133938_small_2'\n",
    "video_save_dir = Path(\"/ubc/cs/research/nlp/grigorii/projects/openvla/experiments/robot/calvin/debug_videos/\") / Path(checkpoint_path).name\n",
    "video_save_dir.mkdir(exist_ok=True)\n",
    "model = OctoModel.load_pretrained(checkpoint_path)\n",
    "processor = None\n",
    "\n",
    "with open(Path(checkpoint_path) / 'finetune_config.json', 'r') as file:\n",
    "    finetune_config = json.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GETTING THE DEVICE ID!*****************************************\n",
      "RESULT OF EGL OPTIONS EGL device choice: -1 of 10.\n",
      "\n",
      "RESULT INSIDE LOOP ***********************, id is  0\n",
      "Stdout:  Starting EGL query\n",
      "Loaded EGL 1.5 after reload.\n",
      "GL_VENDOR=NVIDIA Corporation\n",
      "CUDA_DEVICE=0\n",
      "GL_RENDERER=NVIDIA RTX A5000/PCIe/SSE2\n",
      "GL_VERSION=3.3.0 NVIDIA 550.120\n",
      "GL_SHADING_LANGUAGE_VERSION=3.30 NVIDIA via Cg compiler\n",
      "Completeing EGL query\n",
      "\n",
      "Stderr:  EGL device choice: 0 of 10 (from EGL_VISIBLE_DEVICE)\n",
      "\n",
      "The match:  <re.Match object; span=(77, 90), match='CUDA_DEVICE=0'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EGL device choice: 0 of 10 (from EGL_VISIBLE_DEVICES)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">02/11 [07:19:24] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> | &gt;&gt; Loading robot                                                            <a href=\"file:///ubc/cs/research/nlp/grigorii/projects/openvla/experiments/robot/calvin/calvin_env/calvin_env/robot/robot.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">robot.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///ubc/cs/research/nlp/grigorii/projects/openvla/experiments/robot/calvin/calvin_env/calvin_env/robot/robot.py#40\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">40</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m02/11 [07:19:24]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m | >> Loading robot                                                            \u001b]8;id=885468;file:///ubc/cs/research/nlp/grigorii/projects/openvla/experiments/robot/calvin/calvin_env/calvin_env/robot/robot.py\u001b\\\u001b[2mrobot.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=786903;file:///ubc/cs/research/nlp/grigorii/projects/openvla/experiments/robot/calvin/calvin_env/calvin_env/robot/robot.py#40\u001b\\\u001b[2m40\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                 </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> | &gt;&gt; No OpenGL_accelerate module loaded: No module named          <a href=\"file:///ubc/cs/research/ubc_ml/gguz/miniconda3/envs/octo-env/lib/python3.10/site-packages/OpenGL/acceleratesupport.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">acceleratesupport.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///ubc/cs/research/ubc_ml/gguz/miniconda3/envs/octo-env/lib/python3.10/site-packages/OpenGL/acceleratesupport.py#17\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">17</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                 </span>         <span style=\"color: #008000; text-decoration-color: #008000\">'OpenGL_accelerate'</span>                                               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                       </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m | >> No OpenGL_accelerate module loaded: No module named          \u001b]8;id=439062;file:///ubc/cs/research/ubc_ml/gguz/miniconda3/envs/octo-env/lib/python3.10/site-packages/OpenGL/acceleratesupport.py\u001b\\\u001b[2macceleratesupport.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=526507;file:///ubc/cs/research/ubc_ml/gguz/miniconda3/envs/octo-env/lib/python3.10/site-packages/OpenGL/acceleratesupport.py#17\u001b\\\u001b[2m17\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                 \u001b[0m         \u001b[32m'OpenGL_accelerate'\u001b[0m                                               \u001b[2m                       \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">02/11 [07:19:25] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> | &gt;&gt; Loading configuration from:                                           <a href=\"file:///ubc/cs/research/nlp/grigorii/projects/openvla/experiments/robot/calvin/calvin_env/tacto/tacto/renderer.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">renderer.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///ubc/cs/research/nlp/grigorii/projects/openvla/experiments/robot/calvin/calvin_env/tacto/tacto/renderer.py#73\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">73</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                 </span>         <span style=\"color: #800080; text-decoration-color: #800080\">/ubc/cs/research/nlp/grigorii/projects/openvla/experiments/robot/calvin/ca</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">              </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                 </span>         <span style=\"color: #800080; text-decoration-color: #800080\">lvin_env/conf/digit_sensor/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">config_digit.yml</span>                                <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">              </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m02/11 [07:19:25]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m | >> Loading configuration from:                                           \u001b]8;id=188992;file:///ubc/cs/research/nlp/grigorii/projects/openvla/experiments/robot/calvin/calvin_env/tacto/tacto/renderer.py\u001b\\\u001b[2mrenderer.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=438353;file:///ubc/cs/research/nlp/grigorii/projects/openvla/experiments/robot/calvin/calvin_env/tacto/tacto/renderer.py#73\u001b\\\u001b[2m73\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                 \u001b[0m         \u001b[35m/ubc/cs/research/nlp/grigorii/projects/openvla/experiments/robot/calvin/ca\u001b[0m \u001b[2m              \u001b[0m\n",
       "\u001b[2;36m                 \u001b[0m         \u001b[35mlvin_env/conf/digit_sensor/\u001b[0m\u001b[95mconfig_digit.yml\u001b[0m                                \u001b[2m              \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "received depth0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                 </span><span style=\"color: #808000; text-decoration-color: #808000\">WARNING </span> | &gt;&gt;                                                                      <a href=\"file:///ubc/cs/research/ubc_ml/gguz/miniconda3/envs/octo-env/lib/python3.10/warnings.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">warnings.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///ubc/cs/research/ubc_ml/gguz/miniconda3/envs/octo-env/lib/python3.10/warnings.py#109\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">109</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                 </span>         <span style=\"color: #800080; text-decoration-color: #800080\">/ubc/cs/research/ubc_ml/gguz/miniconda3/envs/octo-env/lib/python3.10/site</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                 </span>         <span style=\"color: #800080; text-decoration-color: #800080\">-packages/urdfpy/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">urdf.py</span>:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2169</span>: RuntimeWarning: invalid value encountered  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                 </span>         in divide                                                                 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                 </span>           value = value <span style=\"color: #800080; text-decoration-color: #800080\">/</span> <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">np.linalg.norm</span><span style=\"font-weight: bold\">(</span>value<span style=\"font-weight: bold\">)</span>                                   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                 </span>                                                                                   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                \u001b[0m\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m | >>                                                                      \u001b]8;id=631682;file:///ubc/cs/research/ubc_ml/gguz/miniconda3/envs/octo-env/lib/python3.10/warnings.py\u001b\\\u001b[2mwarnings.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=268850;file:///ubc/cs/research/ubc_ml/gguz/miniconda3/envs/octo-env/lib/python3.10/warnings.py#109\u001b\\\u001b[2m109\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                 \u001b[0m         \u001b[35m/ubc/cs/research/ubc_ml/gguz/miniconda3/envs/octo-env/lib/python3.10/site\u001b[0m \u001b[2m               \u001b[0m\n",
       "\u001b[2;36m                 \u001b[0m         \u001b[35m-packages/urdfpy/\u001b[0m\u001b[95murdf.py\u001b[0m:\u001b[1;36m2169\u001b[0m: RuntimeWarning: invalid value encountered  \u001b[2m               \u001b[0m\n",
       "\u001b[2;36m                 \u001b[0m         in divide                                                                 \u001b[2m               \u001b[0m\n",
       "\u001b[2;36m                 \u001b[0m           value = value \u001b[35m/\u001b[0m \u001b[1;35mnp.linalg.norm\u001b[0m\u001b[1m(\u001b[0mvalue\u001b[1m)\u001b[0m                                   \u001b[2m               \u001b[0m\n",
       "\u001b[2;36m                 \u001b[0m                                                                                   \u001b[2m               \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "config_path='/ubc/cs/research/nlp/grigorii/projects/openvla/experiments/robot/calvin/conf/med_tasks_config.yaml'\n",
    "env, calvin_cfg = get_calvin_env(\n",
    "    config_path,\n",
    "    device_id=0,\n",
    ")\n",
    "calvin_cfg.image_obs_keys = finetune_config['dataset_kwargs']['image_obs_keys']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from typing import Dict, Set\n",
    "\n",
    "import numpy as np\n",
    "from omegaconf import ListConfig\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "\n",
    "REL_EPS = 0.03\n",
    "EPSILON_ROTATION_DEGREES = 10\n",
    "EPSILON_SLIDER_DRAWER =  0.05\n",
    "EPSILON_BLOCK_POS = 0.1\n",
    "\n",
    "class LowLevelTasks:\n",
    "    def __init__(self, tasks):\n",
    "        \"\"\"\n",
    "        A task is defined as a specific change between the start_info and end_info dictionaries.\n",
    "        Use config file in conf/tasks/ to define tasks using the base task functions defined in this class\n",
    "        \"\"\"\n",
    "\n",
    "        # register task functions from config file\n",
    "        self.tasks = {name: partial(getattr(self, args[0]), *args[1:]) for name, args in dict(tasks).items()}\n",
    "        # dictionary mapping from task name to task id\n",
    "        self.task_to_id = {name: i for i, name in enumerate(self.tasks.keys())}\n",
    "        # dictionary mapping from task id to task name\n",
    "        self.id_to_task = {i: name for i, name in enumerate(self.tasks.keys())}\n",
    "\n",
    "\n",
    "    def get_task_info(self, start_info: Dict, end_info: Dict) -> Set:\n",
    "        \"\"\"\n",
    "        start_info: dict with scene info and robot info\n",
    "        end_info: dict with scene info and robot info\n",
    "        returns set with achieved tasks\n",
    "        \"\"\"\n",
    "        # call functions that are registered in self.tasks\n",
    "        return {\n",
    "            task_name\n",
    "            for task_name, function in self.tasks.items()\n",
    "            if function(start_info=start_info, end_info=end_info)\n",
    "        }\n",
    "\n",
    "    def get_task_info_for_set(self, start_info: Dict, end_info: Dict, task_filter: Set) -> Set:\n",
    "        \"\"\"\n",
    "        start_info: dict with scene info and robot info\n",
    "        end_info: dict with scene info and robot info\n",
    "        task_filter: set with task names to check\n",
    "        returns set with achieved tasks\n",
    "        \"\"\"\n",
    "        # call functions that are registered in self.tasks\n",
    "        return {\n",
    "            task_name\n",
    "            for task_name, function in self.tasks.items()\n",
    "            if task_name in task_filter and function(start_info=start_info, end_info=end_info)\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def stack_objects(max_vel=1, start_info=None, end_info=None):\n",
    "        obj_uids = set(obj[\"uid\"] for obj in start_info[\"scene_info\"][\"movable_objects\"].values())\n",
    "\n",
    "        for obj_name in start_info[\"scene_info\"][\"movable_objects\"]:\n",
    "            obj_start_info = start_info[\"scene_info\"][\"movable_objects\"][obj_name]\n",
    "            obj_end_info = end_info[\"scene_info\"][\"movable_objects\"][obj_name]\n",
    "            obj_start_contacts = set(c[2] for c in obj_start_info[\"contacts\"])\n",
    "            obj_end_contacts = set(c[2] for c in obj_end_info[\"contacts\"])\n",
    "\n",
    "            if (\n",
    "                not len(obj_uids & obj_start_contacts)\n",
    "                and len(obj_uids & obj_end_contacts)\n",
    "                and not len(obj_end_contacts - obj_uids)\n",
    "            ):\n",
    "                # object velocity may not exceed max_vel for successful stack\n",
    "                if np.all(np.abs(obj_end_info[\"current_lin_vel\"]) < max_vel) and np.all(\n",
    "                    np.abs(obj_end_info[\"current_ang_vel\"]) < max_vel\n",
    "                ):\n",
    "                    return True\n",
    "        return False\n",
    "\n",
    "\n",
    "    @property\n",
    "    def num_tasks(self):\n",
    "        return len(self.tasks)\n",
    "\n",
    "    @staticmethod\n",
    "    def rotate_object(\n",
    "        obj_names, z_degrees, x_y_threshold=30, z_threshold=180, movement_threshold=0.1, start_info=None, end_info=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Returns True if the object with obj_name was rotated more than z_degrees degrees around the z-axis while not\n",
    "        being rotated more than x_y_threshold degrees around the x or y axis.\n",
    "        z_degrees is negative for clockwise rotations and positive for counter-clockwise rotations.\n",
    "        \"\"\"\n",
    "        found_grasped = False\n",
    "        for name in obj_names:\n",
    "            if is_block_grasped(obj_name=name, state_info=start_info):\n",
    "                obj_name = name\n",
    "                found_grasped = True\n",
    "                break\n",
    "        \n",
    "        if not found_grasped:\n",
    "            return False\n",
    "        \n",
    "        obj_start_info = start_info[\"scene_info\"][\"movable_objects\"][obj_name]\n",
    "        obj_end_info = end_info[\"scene_info\"][\"movable_objects\"][obj_name]\n",
    "        start_orn = R.from_quat(obj_start_info[\"current_orn\"])\n",
    "        end_orn = R.from_quat(obj_end_info[\"current_orn\"])\n",
    "        rotation = end_orn * start_orn.inv()\n",
    "        x, y, z = rotation.as_euler(\"xyz\", degrees=True)\n",
    "\n",
    "        start_pos = np.array(obj_start_info[\"current_pos\"])\n",
    "        end_pos = np.array(obj_end_info[\"current_pos\"])\n",
    "        pos_diff = end_pos - start_pos\n",
    "        if np.linalg.norm(pos_diff) > movement_threshold:\n",
    "            return False\n",
    "\n",
    "        end_contacts = set(c[2] for c in obj_end_info[\"contacts\"])\n",
    "        robot_uid = {start_info[\"robot_info\"][\"uid\"]}\n",
    "\n",
    "        # object should be in contact with ground\n",
    "        if len(end_contacts - robot_uid) > 0:\n",
    "            return False\n",
    "\n",
    "        if z_degrees > 0:\n",
    "            return z_degrees < z < z_threshold and abs(x) < x_y_threshold and abs(y) < x_y_threshold\n",
    "        else:\n",
    "            return z_degrees > z > -z_threshold and abs(x) < x_y_threshold and abs(y) < x_y_threshold\n",
    "\n",
    "    @staticmethod\n",
    "    def push_object(x_direction, y_direction, start_info, end_info):\n",
    "        \"\"\"\n",
    "        Returns True if the object with 'obj_name' was moved more than 'x_direction' meters in x direction\n",
    "        (or 'y_direction' meters in y direction analogously).\n",
    "        Note that currently x and y pushes are mutually exclusive, meaning that one of the arguments has to be 0.\n",
    "        The sign matters, e.g. pushing an object to the right when facing the table coincides with a movement in\n",
    "        positive x-direction.\n",
    "        \"\"\"\n",
    "        assert x_direction * y_direction == 0 and x_direction + y_direction != 0\n",
    "        # Find the block which is in contact with the gripper\n",
    "        found_block = False\n",
    "        robot_uid = start_info[\"robot_info\"][\"uid\"]\n",
    "\n",
    "        for block_name, block_info in start_info[\"scene_info\"][\"movable_objects\"].items():\n",
    "            for c in block_info['contacts']:\n",
    "                if c[2] == robot_uid:\n",
    "                    obj_name = block_name\n",
    "                    found_block = True\n",
    "                    break\n",
    "        \n",
    "        if not found_block:\n",
    "            return False\n",
    "\n",
    "        obj_start_info = start_info[\"scene_info\"][\"movable_objects\"][obj_name]\n",
    "        obj_end_info = end_info[\"scene_info\"][\"movable_objects\"][obj_name]\n",
    "        start_pos = np.array(obj_start_info[\"current_pos\"])\n",
    "        end_pos = np.array(obj_end_info[\"current_pos\"])\n",
    "        pos_diff = end_pos - start_pos\n",
    "\n",
    "        # contacts excluding robot\n",
    "        start_contacts = set((c[2], c[4]) for c in obj_start_info[\"contacts\"] if c[2] != robot_uid)\n",
    "        end_contacts = set((c[2], c[4]) for c in obj_end_info[\"contacts\"] if c[2] != robot_uid)\n",
    "\n",
    "        # computing set difference to check if object had surface contact (excluding robot) at both times\n",
    "        surface_contact = len(start_contacts) > 0 and len(end_contacts) > 0 and start_contacts <= end_contacts\n",
    "        if not surface_contact:\n",
    "            return False\n",
    "\n",
    "        if x_direction > 0:\n",
    "            return pos_diff[0] > x_direction\n",
    "        elif x_direction < 0:\n",
    "            return pos_diff[0] < x_direction\n",
    "\n",
    "        if y_direction > 0:\n",
    "            return pos_diff[1] > y_direction\n",
    "        elif y_direction < 0:\n",
    "            return pos_diff[1] < y_direction\n",
    "\n",
    "    @staticmethod\n",
    "    def lift_grasped_object(surface_bodies=None, start_info=None, end_info=None):\n",
    "        \"\"\"\n",
    "        Returns True if the object with 'obj_name' was grasped by the robot and lifted more than 'z_direction' meters.\n",
    "        \"\"\"\n",
    "        found_grasped = False\n",
    "        for name in ['block_red', 'block_blue', 'block_pink']:\n",
    "            if is_block_grasped(obj_name=name, state_info=start_info):\n",
    "                obj_name = name\n",
    "                found_grasped = True\n",
    "                break\n",
    "        \n",
    "        if not found_grasped:\n",
    "            return False\n",
    "        \n",
    "        obj_start_info = start_info[\"scene_info\"][\"movable_objects\"][obj_name]\n",
    "        obj_end_info = end_info[\"scene_info\"][\"movable_objects\"][obj_name]\n",
    "\n",
    "        start_pos = np.array(obj_start_info[\"current_pos\"])\n",
    "        end_pos = np.array(obj_end_info[\"current_pos\"])\n",
    "        pos_diff = end_pos - start_pos\n",
    "        z_diff = pos_diff[2]\n",
    "\n",
    "        robot_uid = start_info[\"robot_info\"][\"uid\"]\n",
    "        end_contacts = set(c[2] for c in obj_end_info[\"contacts\"])\n",
    "\n",
    "        #print(start_info[\"scene_info\"][\"fixed_objects\"]['table'][\"links\"])\n",
    "        #print(start_info[\"scene_info\"][\"movable_objects\"]['block_red'])\n",
    "\n",
    "\n",
    "        for surface_link in ['base_link', 'plank_link', 'drawer_link']:\n",
    "            surface_uid = start_info[\"scene_info\"][\"fixed_objects\"]['table'][\"uid\"]\n",
    "            surface_link_id = start_info[\"scene_info\"][\"fixed_objects\"]['table'][\"links\"][surface_link]\n",
    "            start_contacts_links = set((c[2], c[4]) for c in obj_start_info[\"contacts\"])\n",
    "            end_contacts_links = set((c[2], c[4]) for c in obj_end_info[\"contacts\"])\n",
    "            surface_criterion = (surface_uid, surface_link_id) in start_contacts_links and (surface_uid, surface_link_id) not in end_contacts_links\n",
    "\n",
    "            if not surface_criterion:\n",
    "                continue\n",
    "\n",
    "            start_pos = np.array(obj_start_info[\"current_pos\"])\n",
    "            end_pos = np.array(obj_end_info[\"current_pos\"])\n",
    "            pos_diff = np.abs(end_pos[:2] - start_pos[:2]).sum()\n",
    "\n",
    "            start_orn = R.from_quat(obj_start_info[\"current_orn\"])\n",
    "            end_orn = R.from_quat(obj_end_info[\"current_orn\"])\n",
    "            rotation = end_orn * start_orn.inv()\n",
    "            x, y, z = rotation.as_euler(\"xyz\", degrees=True)\n",
    "\n",
    "            #print(f\"Pos diff {pos_diff} z_degrees {z} z_diff {z_diff}\")\n",
    "\n",
    "            if pos_diff > EPSILON_BLOCK_POS:\n",
    "                return False\n",
    "\n",
    "            z_degrees = EPSILON_ROTATION_DEGREES\n",
    "            if z > z_degrees:\n",
    "                return False\n",
    "\n",
    "            z_degrees = -EPSILON_ROTATION_DEGREES\n",
    "            if z < z_degrees:\n",
    "                return False\n",
    "\n",
    "            z_direction = 0.03\n",
    "            if (\n",
    "                # robot still holding the object\n",
    "                z_diff > z_direction\n",
    "                and robot_uid in end_contacts\n",
    "                and len(end_contacts) == 1\n",
    "                and surface_criterion):\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def push_object_into(obj_name, src_body, src_link, dest_body, dest_link, start_info=None, end_info=None):\n",
    "        \"\"\"\n",
    "        obj_name is either a list of object names or a string\n",
    "        Returns True if the object / any of the objects changes contact from src_body to dest_body.\n",
    "        The robot may neither touch the object at start nor end.\n",
    "        \"\"\"\n",
    "        if isinstance(obj_name, (list, ListConfig)):\n",
    "            return any(\n",
    "                LowLevelTasks.push_object_into(ob, src_body, src_link, dest_body, dest_link, start_info, end_info)\n",
    "                for ob in obj_name\n",
    "            )\n",
    "        \n",
    "        robot_uid = start_info[\"robot_info\"][\"uid\"]\n",
    "\n",
    "        src_uid = start_info[\"scene_info\"][\"fixed_objects\"][src_body][\"uid\"]\n",
    "        src_link_id = start_info[\"scene_info\"][\"fixed_objects\"][src_body][\"links\"][src_link]\n",
    "        dest_uid = end_info[\"scene_info\"][\"fixed_objects\"][dest_body][\"uid\"]\n",
    "        dest_link_id = end_info[\"scene_info\"][\"fixed_objects\"][dest_body][\"links\"][dest_link]\n",
    "\n",
    "        start_contacts = set((c[2], c[4]) for c in start_info[\"scene_info\"][\"movable_objects\"][obj_name][\"contacts\"])\n",
    "        end_contacts = set((c[2], c[4]) for c in end_info[\"scene_info\"][\"movable_objects\"][obj_name][\"contacts\"])\n",
    "        \n",
    "        return (\n",
    "            robot_uid not in start_contacts | end_contacts\n",
    "            and len(start_contacts) == 1\n",
    "            and (src_uid, src_link_id) in start_contacts\n",
    "            and (dest_uid, dest_link_id) in end_contacts\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def move_door_abs(joint_name, start_threshold, end_threshold, start_info, end_info):\n",
    "        \"\"\"\n",
    "        Returns True if the joint specified by 'obj_name' and 'joint_name' (e.g. a door or drawer)\n",
    "        is moved from at least 'start_threshold' to 'end_threshold'.\n",
    "        \"\"\"\n",
    "        # TODO: ADD THAT OBJECT IS IN CONTACT WITH GRIPPER ALEADY\n",
    "\n",
    "        start_joint_state = start_info[\"scene_info\"][\"doors\"][joint_name][\"current_state\"][0]\n",
    "        end_joint_state = end_info[\"scene_info\"][\"doors\"][joint_name][\"current_state\"][0]\n",
    "\n",
    "        if start_threshold < end_threshold:\n",
    "            return start_joint_state < start_threshold < end_threshold < end_joint_state\n",
    "        elif start_threshold > end_threshold:\n",
    "            return start_joint_state > start_threshold > end_threshold > end_joint_state\n",
    "        else:\n",
    "            raise ValueError\n",
    "\n",
    "    @staticmethod\n",
    "    def move_door_rel(joint_name, threshold, start_info, end_info):\n",
    "        \"\"\"\n",
    "        Returns True if the joint specified by 'obj_name' and 'joint_name' (e.g. a door or drawer)\n",
    "        is moved from at least 'start_threshold' to 'end_threshold'.\n",
    "        \"\"\"\n",
    "        robot_contacts_start = set(c[4] for c in start_info[\"robot_info\"][\"contacts\"])\n",
    "\n",
    "        if joint_name == 'base__drawer':\n",
    "            drawer_link_id = start_info['scene_info']['fixed_objects']['table']['links']['drawer_link']\n",
    "            if drawer_link_id not in robot_contacts_start:\n",
    "                return False\n",
    "        elif joint_name == 'base__slide':\n",
    "            slider_link_id = start_info['scene_info']['fixed_objects']['table']['links']['slide_link']\n",
    "            if slider_link_id not in robot_contacts_start:\n",
    "                return False\n",
    "\n",
    "\n",
    "        start_joint_state = start_info[\"scene_info\"][\"doors\"][joint_name][\"current_state\"]\n",
    "        end_joint_state = end_info[\"scene_info\"][\"doors\"][joint_name][\"current_state\"]\n",
    "\n",
    "        return (\n",
    "            0 < threshold < end_joint_state - start_joint_state or 0 > threshold > end_joint_state - start_joint_state\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def place_gripper_over_block(object_name, start_info=None, end_info=None):\n",
    "        # Before grasping a block, check for whether the gripper is near the block.\n",
    "        # TODO: Make sure the gripper isn't moving over a closed drawer with blocks inside.\n",
    "        target_block_start = [v for k, v in start_info[\"scene_info\"][\"movable_objects\"].items() if k == object_name]\n",
    "        target_block_end = [v for k, v in end_info[\"scene_info\"][\"movable_objects\"].items() if k == object_name]\n",
    "\n",
    "        if len(target_block_start) != 1 or len(target_block_end) != 1:\n",
    "            return False\n",
    "        \n",
    "        robot_contacts_start = set(c[2] for c in start_info[\"robot_info\"][\"contacts\"])\n",
    "        robot_contacts_end = set(c[2] for c in end_info[\"robot_info\"][\"contacts\"])\n",
    "\n",
    "        both_contacts_empty = len(robot_contacts_start) == 0 and len(robot_contacts_end) == 0\n",
    "        only_1_contact = len(robot_contacts_start) == 1 and len(robot_contacts_end) == 1 and list(robot_contacts_start)[0] == list(robot_contacts_end)[0]\n",
    "        # contacts should stay the same\n",
    "        if not (both_contacts_empty or only_1_contact):\n",
    "            return False\n",
    "\n",
    "        start_gripper_pos = np.array(start_info['robot_info']['tcp_pos'])\n",
    "        end_gripper_pos = np.array(end_info['robot_info']['tcp_pos'])\n",
    "\n",
    "        target_block_start_pos = np.array(target_block_start[0]['current_pos'])\n",
    "        # block in drawer and drawer is closed\n",
    "        if block_in_drawer(target_block_start_pos) and is_drawer_closed(start_info['scene_info']['fixed_objects']['table']['links']['drawer_link']):\n",
    "            return False\n",
    "        \n",
    "        target_block_end_pos = np.array(target_block_end[0]['current_pos'])\n",
    "\n",
    "        # Make sure initial and final target block pos is approx equal\n",
    "        block_movement = np.sum(np.abs(target_block_end_pos - target_block_start_pos))\n",
    "        disp_eps = 0.001\n",
    "        \n",
    "        if block_movement > disp_eps:\n",
    "            return False\n",
    "        \n",
    "        # Find x-y coord boundaries for where the gripper should reach (relative to the target block?)\n",
    "        start_obj_gripper_diff = np.sqrt(np.sum(np.square(target_block_start_pos[:2] - start_gripper_pos[:2])))\n",
    "        end_obj_gripper_diff = np.sqrt(np.sum(np.square(target_block_end_pos[:2] - end_gripper_pos[:2])))\n",
    "\n",
    "        # Make sure gripper wasn't too close to the block in the first place.\n",
    "        if start_obj_gripper_diff < REL_EPS:\n",
    "            return False\n",
    "        \n",
    "        # Make sure gripper isn't too far from the block.\n",
    "        if end_obj_gripper_diff > REL_EPS:\n",
    "            return False\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    @staticmethod\n",
    "    def place_grasped_block_over_surface(dest_link, start_info=None, end_info=None):\n",
    "\n",
    "        robot_uid = {start_info[\"robot_info\"][\"uid\"]}\n",
    "\n",
    "        for obj_name in start_info[\"scene_info\"][\"movable_objects\"]:\n",
    "\n",
    "            obj_start_info = start_info[\"scene_info\"][\"movable_objects\"][obj_name]\n",
    "            obj_end_info = end_info[\"scene_info\"][\"movable_objects\"][obj_name]\n",
    "\n",
    "            obj_start_pos = start_info[\"scene_info\"][\"movable_objects\"][obj_name]['current_pos']\n",
    "            obj_end_pos = end_info[\"scene_info\"][\"movable_objects\"][obj_name]['current_pos']\n",
    "\n",
    "            obj_start_contacts = set(c[2] for c in obj_start_info[\"contacts\"])\n",
    "            obj_end_contacts = set(c[2] for c in obj_end_info[\"contacts\"])\n",
    "\n",
    "            if dest_link == 'plank_link':\n",
    "                if not in_slider_area(obj_end_pos) or in_slider_area(obj_start_pos):\n",
    "                    continue\n",
    "\n",
    "            elif dest_link == 'drawer_link':\n",
    "                if not in_drawer_area(obj_end_pos) or in_drawer_area(obj_start_pos):\n",
    "                    continue\n",
    "\n",
    "            elif dest_link == 'table_link':\n",
    "                if not in_table_area(obj_end_pos) or (is_block_over_other_block(obj_name, end_info)) or (in_table_area(obj_start_pos) and not is_block_over_other_block(obj_name, start_info)):\n",
    "                    continue\n",
    "\n",
    "            # Robot still holding the object\n",
    "            if len(obj_start_contacts) == 1 and len(obj_end_contacts) == 1 and \\\n",
    "                len(obj_end_contacts - robot_uid) == 0 and len(obj_start_contacts - robot_uid) == 0:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    @staticmethod\n",
    "    def grasp_block(obj_name, start_info=None, end_info=None):\n",
    "        \"\"\"\n",
    "        Grasp a block nearby the gripper, specified by object_name. Typically called after \n",
    "        place_gripper_over_block.\n",
    "        \"\"\"\n",
    "        # TODO: WORK IN PROGRESS, WHAT OTHER CRITERIA FOR GRASPING OTHER THAN CONTACT?\n",
    "        #       HOW TO MAKE SURE BLOCK CAN BE LIFTED? GRIPPER WIDTH?\n",
    "        #if not is_gripper_near_block(obj_name, state_info=start_info):\n",
    "        #    return False\n",
    "        robot_contacts_start = set(c[2] for c in start_info[\"robot_info\"][\"contacts\"])\n",
    "        robot_contacts_end = set(c[2] for c in end_info[\"robot_info\"][\"contacts\"])\n",
    "\n",
    "        if len(robot_contacts_start) > 0:\n",
    "            #print(\"Robot is touching something in the beginning!\")\n",
    "            return False\n",
    "\n",
    "        #print(\"Contact passed!\")\n",
    "        obj_start_info = start_info[\"scene_info\"][\"movable_objects\"][obj_name]\n",
    "        obj_end_info = end_info[\"scene_info\"][\"movable_objects\"][obj_name]\n",
    "        \n",
    "        # check the block wasn't lifted from the surface it was on in the beginning\n",
    "        for surface_link in ['drawer_link', 'plank_link', 'base_link']:\n",
    "            surface_uid = start_info[\"scene_info\"][\"fixed_objects\"]['table'][\"uid\"]\n",
    "            surface_link_id = start_info[\"scene_info\"][\"fixed_objects\"]['table'][\"links\"][surface_link]\n",
    "\n",
    "            start_contacts_links = set((c[2], c[4]) for c in obj_start_info[\"contacts\"])\n",
    "            end_contacts_links = set((c[2], c[4]) for c in obj_end_info[\"contacts\"])\n",
    "            #print(\"start links:\", start_contacts_links)\n",
    "            #print(\"end links: \", end_contacts_links)\n",
    "            if (surface_uid, surface_link_id) in start_contacts_links and (surface_uid, surface_link_id) not in end_contacts_links:\n",
    "                return False\n",
    "        #print(\"Surface criterion passed!\")\n",
    "\n",
    "        # the block wasn't moved/lifted too much\n",
    "        start_pos = np.array(obj_start_info[\"current_pos\"])\n",
    "        end_pos = np.array(obj_end_info[\"current_pos\"])\n",
    "        pos_diff = np.abs(end_pos - start_pos).sum()\n",
    "        if pos_diff > EPSILON_BLOCK_POS:\n",
    "            return False\n",
    "        \n",
    "        end_gripper_width = end_info[\"robot_info\"][\"gripper_opening_width\"]\n",
    "        #print(end_gripper_width)\n",
    "        if not (0.03 < end_gripper_width < 0.06):\n",
    "            return False\n",
    "\n",
    "        return is_block_grasped(obj_name, end_info) \n",
    "\n",
    "\n",
    "    @staticmethod   \n",
    "    def grasp_slider(start_info=None, end_info=None):\n",
    "        robot_contacts_start = set(c[4] for c in start_info[\"robot_info\"][\"contacts\"])\n",
    "        robot_contacts_end = set(c[4] for c in end_info[\"robot_info\"][\"contacts\"])\n",
    "\n",
    "        if len(robot_contacts_start) > 0:\n",
    "            return False\n",
    "        if len(robot_contacts_end) != 1:\n",
    "            return False\n",
    "\n",
    "        if LowLevelTasks.move_door_rel('base__slide', 0.15, start_info, end_info) or LowLevelTasks.move_door_rel('base__slide', -0.15, start_info, end_info):\n",
    "            return False\n",
    "        slider_link_id = start_info['scene_info']['fixed_objects']['table']['links']['slide_link']\n",
    "\n",
    "        if list(robot_contacts_end)[0] == slider_link_id:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    @staticmethod   \n",
    "    def grasp_drawer(start_info=None, end_info=None):\n",
    "        robot_contacts_start = set(c[4] for c in start_info[\"robot_info\"][\"contacts\"])\n",
    "        robot_contacts_end = set(c[4] for c in end_info[\"robot_info\"][\"contacts\"])\n",
    "\n",
    "        if len(robot_contacts_start) > 0:\n",
    "            return False\n",
    "        if len(robot_contacts_end) != 1:\n",
    "            return False\n",
    "\n",
    "        if LowLevelTasks.move_door_rel('base__drawer', 0.12, start_info, end_info) or LowLevelTasks.move_door_rel('base__drawer', -0.12, start_info, end_info):\n",
    "            return False\n",
    "\n",
    "        drawer_link_id = start_info['scene_info']['fixed_objects']['table']['links']['drawer_link']\n",
    "\n",
    "        if list(robot_contacts_end)[0] == drawer_link_id:\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    @staticmethod   \n",
    "    def ungrasp_drawer(start_info=None, end_info=None):\n",
    "        robot_contacts_start = set(c[4] for c in start_info[\"robot_info\"][\"contacts\"])\n",
    "        robot_contacts_end = set(c[4] for c in end_info[\"robot_info\"][\"contacts\"])\n",
    "        drawer_link_id = start_info['scene_info']['fixed_objects']['table']['links']['drawer_link']\n",
    "\n",
    "        if len(robot_contacts_start) != 1 or list(robot_contacts_start)[0] != drawer_link_id:\n",
    "            return False\n",
    "        \n",
    "        if LowLevelTasks.move_door_rel('base__drawer', EPSILON_SLIDER_DRAWER, start_info, end_info) or LowLevelTasks.move_door_rel('base__drawer', -EPSILON_SLIDER_DRAWER, start_info, end_info):\n",
    "            return False\n",
    "\n",
    "        if len(robot_contacts_end) == 0:\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    @staticmethod   \n",
    "    def ungrasp_slider(start_info=None, end_info=None):\n",
    "        robot_contacts_start = set(c[4] for c in start_info[\"robot_info\"][\"contacts\"])\n",
    "        robot_contacts_end = set(c[4] for c in end_info[\"robot_info\"][\"contacts\"])\n",
    "        slider_link_id = start_info['scene_info']['fixed_objects']['table']['links']['slide_link']\n",
    "\n",
    "        if len(robot_contacts_start) != 1 or list(robot_contacts_start)[0] != slider_link_id:\n",
    "            return False\n",
    "        \n",
    "        if LowLevelTasks.move_door_rel('base__slide', EPSILON_SLIDER_DRAWER, start_info, end_info) or LowLevelTasks.move_door_rel('base__slide', -EPSILON_SLIDER_DRAWER, start_info, end_info):\n",
    "            return False\n",
    "\n",
    "        if len(robot_contacts_end) == 0:\n",
    "            return True\n",
    "        \n",
    "        return False\n",
    "\n",
    "\n",
    "    @staticmethod   \n",
    "    def ungrasp_block(object_names, start_info=None, end_info=None):\n",
    "        \"\"\"\n",
    "        Grasp a block nearby the gripper, specified by object_name. Typically called after \n",
    "        place_gripper_over_block.\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO: WORK IN PROGRESS, WHAT OTHER CRITERIA FOR GRASPING OTHER THAN CONTACT?\n",
    "        #       HOW TO MAKE SURE THE BLOCK CAN BE LIFTED? GRIPPER WIDTH?\n",
    "        for object_name in object_names:\n",
    "            if is_block_grasped(object_name, start_info) and not is_block_grasped(object_name, end_info):\n",
    "                print(\"The grasped block is \", object_name)\n",
    "                obj_start_info = start_info[\"scene_info\"][\"movable_objects\"][object_name]\n",
    "                obj_end_info = end_info[\"scene_info\"][\"movable_objects\"][object_name]\n",
    "                start_pos = np.array(obj_start_info[\"current_pos\"])\n",
    "                end_pos = np.array(obj_end_info[\"current_pos\"])\n",
    "                pos_xy_diff = np.abs(end_pos[:2] - start_pos[:2]).sum()\n",
    "                if pos_xy_diff > REL_EPS:\n",
    "                    print(f\"{object_name} moved xy too much! by {pos_xy_diff}\")\n",
    "                    continue\n",
    "                \n",
    "                start_orn = R.from_quat(obj_start_info[\"current_orn\"])\n",
    "                end_orn = R.from_quat(obj_end_info[\"current_orn\"])\n",
    "                rotation = end_orn * start_orn.inv()\n",
    "                x, y, z = rotation.as_euler(\"xyz\", degrees=True)\n",
    "                z_degrees = 45\n",
    "                if z > z_degrees:\n",
    "                    print(f\"{object_name} rotated too much! by {z}\")\n",
    "                    continue\n",
    "\n",
    "                z_degrees = -45\n",
    "                if z < z_degrees:\n",
    "                    print(f\"{object_name} rotated too much! by {z}\")\n",
    "                    continue\n",
    "\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    @staticmethod\n",
    "    def contact_block(block_name, side, start_info, end_info):\n",
    "        \"\"\"\n",
    "            1) The robot should not contact anything in the beginning.\n",
    "            2) In the end, the robot should contact the correct block on the correct side (left or right)\n",
    "                (relative to the static camera)\n",
    "            3) Also, the block should not be lifted.\n",
    "        \"\"\"\n",
    "        robot_contacts_start = [c[2] for c in start_info[\"robot_info\"][\"contacts\"]]\n",
    "        robot_contacts_end = [c[2] for c in end_info[\"robot_info\"][\"contacts\"]]\n",
    "        robot_contacts_links = [c[3] for c in end_info[\"robot_info\"][\"contacts\"]]\n",
    "\n",
    "        if len(robot_contacts_start) > 0:\n",
    "            return False\n",
    "        \n",
    "        if len(robot_contacts_end) != 1:\n",
    "            return False\n",
    "        \n",
    "        block_end_info = end_info[\"scene_info\"][\"movable_objects\"][block_name]\n",
    "        block_end_contacts = set([c[2] for c in block_end_info[\"contacts\"]])\n",
    "\n",
    "        robot_uid = {start_info[\"robot_info\"][\"uid\"]}\n",
    "        # Robot is touching the block and there are other contacts too (e.g. floor)\n",
    "        if not (len(block_end_contacts) > 1 and len(block_end_contacts - robot_uid) < len(block_end_contacts)):\n",
    "            return False\n",
    "\n",
    "        \n",
    "        block_start_info = start_info[\"scene_info\"][\"movable_objects\"][block_name]\n",
    "\n",
    "        start_pos = np.array(block_start_info[\"current_pos\"])\n",
    "        end_pos = np.array(block_end_info[\"current_pos\"])\n",
    "        pos_diff = np.abs(end_pos - start_pos).sum()\n",
    "\n",
    "        if pos_diff > EPSILON_BLOCK_POS:\n",
    "            return False\n",
    "\n",
    "        left_gripper_outer = 11\n",
    "        right_gripper_outer = 9\n",
    "        # Right/left side of cube\n",
    "        if side == 'right' and robot_contacts_links[0] == left_gripper_outer:\n",
    "            return True\n",
    "        elif side == 'left' and robot_contacts_links[0] == right_gripper_outer:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    @staticmethod\n",
    "    def toggle_light(light_name, start_state, end_state, start_info, end_info):\n",
    "        # TODO: Add that the robot needs to be over the switch already?\n",
    "        return (\n",
    "            start_info[\"scene_info\"][\"lights\"][light_name][\"logical_state\"] == start_state\n",
    "            and end_info[\"scene_info\"][\"lights\"][light_name][\"logical_state\"] == end_state\n",
    "        )\n",
    "\n",
    "def is_block_grasped(obj_name, state_info=None):\n",
    "        robot_contacts_end = np.array([c[2] for c in state_info[\"robot_info\"][\"contacts\"]])\n",
    "\n",
    "        if len(robot_contacts_end) == 0:\n",
    "            return False\n",
    "        \n",
    "        #print(\"Robot contacts end: \", robot_contacts_end)\n",
    "        target_obj_uid = state_info[\"scene_info\"][\"movable_objects\"][obj_name]['uid']\n",
    "        #print(\"Target uid: \", target_obj_uid)\n",
    "        # At least 2 points of contact with the target block\n",
    "        if np.sum(robot_contacts_end == target_obj_uid) < 2:\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "def is_gripper_near_block(obj_name, state_info=None):\n",
    "        target_block_data = [v for k, v in state_info[\"scene_info\"][\"movable_objects\"].items() if k == obj_name]\n",
    "\n",
    "        if len(target_block_data) != 1:\n",
    "            return False\n",
    "        \n",
    "        gripper_pos = np.array(state_info['robot_info']['tcp_pos'])\n",
    "        target_block_pos = np.array(target_block_data[0]['current_pos'])\n",
    "        start_obj_gripper_diff = np.sqrt(np.sum(np.square(target_block_pos[:2] - gripper_pos[:2])))\n",
    "        return start_obj_gripper_diff < REL_EPS\n",
    "\n",
    "def is_block_over_other_block(block_name, state_info=None):\n",
    "\n",
    "    target_block_data = [v for k, v in state_info[\"scene_info\"][\"movable_objects\"].items() if k == block_name]\n",
    "    other_blocks_data = [v for k, v in state_info[\"scene_info\"][\"movable_objects\"].items() if k != block_name]\n",
    "\n",
    "    target_block_pos = np.array(target_block_data[0]['current_pos'])\n",
    "    other_block_pos_1 = np.array(other_blocks_data[0]['current_pos'])\n",
    "    other_block_pos_2 = np.array(other_blocks_data[0]['current_pos'])\n",
    "\n",
    "    target_other_1_diff = np.sqrt(np.sum(np.square(target_block_pos[:2] - other_block_pos_1[:2])))\n",
    "    target_other_2_diff = np.sqrt(np.sum(np.square(target_block_pos[:2] - other_block_pos_2[:2])))\n",
    "    # Rectangle (pink) largest side is 0.1 in length \n",
    "    return target_other_1_diff < 0.1 or target_other_2_diff < 0.1\n",
    "\n",
    "\n",
    "def block_in_drawer(block_coord):\n",
    "    return 0.357 < block_coord[2] < 0.363\n",
    "\n",
    "def is_drawer_closed(drawer_link_val):\n",
    "    return drawer_link_val <= 0.03\n",
    "\n",
    "def in_table_area(xyz):\n",
    "    x, y, z = xyz[0], xyz[1], xyz[2]\n",
    "    x_match = (x >= -0.38 and x <= 0.35)\n",
    "    y_match = (y >=- 0.14 and y <= -0.03)\n",
    "    return x_match and y_match\n",
    "\n",
    "\n",
    "def in_slider_area(xyz):\n",
    "    x, y, z = xyz[0], xyz[1], xyz[2]\n",
    "    x_match = (x >= -0.38 and x <= 0.18)\n",
    "    y_match = (y >= 0.05 and y <= 0.12)\n",
    "    return x_match and y_match\n",
    "\n",
    "def in_drawer_area(xyz):\n",
    "    x, y, z = xyz[0], xyz[1], xyz[2]\n",
    "    #x_match = (x >= -0.38 and x <= 0.18)\n",
    "    y_match = y < -0.15\n",
    "    return y_match\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Low-level tasks one-by-one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import cv2\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_policy(model, env, cfg, processor, num_videos=0, save_dir=None):\n",
    "    #task_oracle_low_level = hydra.utils.instantiate(cfg.low_level_tasks)\n",
    "    task_oracle_high_level = hydra.utils.instantiate(cfg.tasks)\n",
    "\n",
    "    task_oracle_low_level = LowLevelTasks(cfg.low_level_tasks.tasks)\n",
    "\n",
    "    print(\"The oracle: \", task_oracle_low_level)\n",
    "    val_annotations_low_level = cfg.low_level_annotations\n",
    "    val_annotations_high_level = cfg.annotations\n",
    "\n",
    "    # video stuff\n",
    "    if num_videos > 0:\n",
    "        rollout_video = RolloutVideo(\n",
    "            logger=logger,\n",
    "            empty_cache=False,\n",
    "            log_to_file=True,\n",
    "            save_dir=save_dir,\n",
    "            resolution_scale=1,\n",
    "        )\n",
    "    else:\n",
    "        rollout_video = None\n",
    "\n",
    "    eval_sequences = get_low_level_sequences(cfg.num_sequences)\n",
    "    #for seq in eval_sequences:\n",
    "        #print(seq[0])\n",
    "        #print(seq[1])\n",
    "        #if seq[1][0] == 'stack_block':\n",
    "        #    init_state = seq[0]\n",
    "        #    if init_state['blue_block'] == 'slider_left' and init_state['slider'] == 'left' and seq[1][1][2] == 'place_grasped_block_over_blue_block':\n",
    "        #        raise Exception('Invalid state combo!')\n",
    "\n",
    "    results = []\n",
    "    \n",
    "    high_level_started = Counter()\n",
    "    high_level_completed = Counter()\n",
    "    low_level_started = Counter()\n",
    "    low_level_completed = Counter()\n",
    "    counters = {\n",
    "        'high_level_started': high_level_started,\n",
    "        'high_level_completed': high_level_completed,\n",
    "        'low_level_started': low_level_started,\n",
    "        'low_level_completed': low_level_completed,\n",
    "    }\n",
    "\n",
    "    if not cfg.debug:\n",
    "        eval_sequences = tqdm(eval_sequences, position=0, leave=True)\n",
    "    for i, (initial_state, eval_sequence) in enumerate(eval_sequences):\n",
    "        record = i < num_videos\n",
    "        #initial_state = {'led': 0, 'lightbulb': 0, 'slider': 'left', 'drawer': 'closed', 'red_block': 'slider_left', 'blue_block': 'table', 'pink_block': 'slider_right', 'grasped': 0, 'contact': 0, 'red_block_lifted': 0, 'blue_block_lifted': 0, 'pink_block_lifted': 0}\n",
    "        #initial_state = {'led': 0, 'lightbulb': 0, 'slider': 'left', 'drawer': 'closed', 'red_block': 'slider_right', 'blue_block': 'table', 'pink_block': 'table', 'grasped': 0, 'contact': 0, 'red_block_lifted': 0, 'blue_block_lifted': 0, 'pink_block_lifted': 0}\n",
    "        #eval_sequence = ('stack_block', ['grasp_blue_block', 'lift_grasped_block', 'place_grasped_block_over_pink_block', 'ungrasp_block'])\n",
    "\n",
    "        high_level_task, eval_seq = eval_sequence\n",
    "        high_level_started[high_level_task] += 1\n",
    "\n",
    "        result = evaluate_sequence(\n",
    "            env, model, task_oracle_low_level, initial_state, eval_seq, val_annotations_low_level, cfg, processor, record, rollout_video, i,\n",
    "            low_level_started=low_level_started, low_level_completed=low_level_completed\n",
    "        )\n",
    "\n",
    "        results.append(result)\n",
    "        if result == len(eval_seq):\n",
    "            high_level_completed[high_level_task] += 1\n",
    "\n",
    "        if record:\n",
    "            rollout_video.write_to_tmp()\n",
    "            #print(\"Terminating early!\")\n",
    "            #break\n",
    "        if not cfg.debug:\n",
    "            success_rates = count_success(results)\n",
    "            average_rate = sum(success_rates) / len(success_rates) * 5\n",
    "            description = \" \".join([f\"{i + 1}/5 : {v * 100:.1f}% |\" for i, v in enumerate(success_rates)])\n",
    "            description += f\" Average: {average_rate:.1f} |\"\n",
    "            eval_sequences.set_description(description)\n",
    "\n",
    "    if num_videos > 0:\n",
    "        # log rollout videos\n",
    "        rollout_video._log_videos_to_file(0, save_as_video=False)\n",
    "    print(\"High_started: \", high_level_started)\n",
    "    print(\"High_completed: \", high_level_completed)\n",
    "    print(\"Low_started: \", low_level_started)\n",
    "    print(\"Low_completed: \", low_level_completed)\n",
    "\n",
    "\n",
    "    return results, average_rate, success_rates, counters\n",
    "\n",
    "def join_vis_lang_jup(img, lang_text):\n",
    "    \"\"\"Takes as input an image and a language instruction and visualizes them with cv2\"\"\"\n",
    "    img = img.copy()\n",
    "    img = cv2.resize(img, (500, 500))\n",
    "    print(\"visualizing!\")\n",
    "    #clear_output(wait=True)\n",
    "    add_text(img, lang_text)\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "    time.sleep(0.1)\n",
    "\n",
    "\n",
    "def evaluate_sequence(\n",
    "    env, model, task_checker, initial_state, eval_sequence, val_annotations, cfg, processor, record, rollout_video, i, \n",
    "    low_level_started, low_level_completed\n",
    "):\n",
    "    robot_obs, scene_obs = get_env_state_for_initial_condition(initial_state)\n",
    "    env.reset(robot_obs=robot_obs, scene_obs=scene_obs)\n",
    "    if record:\n",
    "        caption = \" | \".join(eval_sequence)\n",
    "        rollout_video.new_video(tag=get_video_tag(i), caption=caption)\n",
    "\n",
    "    success_counter = 0\n",
    "    if cfg.debug:\n",
    "        time.sleep(1)\n",
    "        print()\n",
    "        print()\n",
    "        print(f\"Evaluating sequence: {' -> '.join(eval_sequence)}\")\n",
    "        print(\"Subtask: \", end=\"\")\n",
    "    print(\"Evaluating sequence: \", eval_sequence)\n",
    "    print('Initial state: ', initial_state)\n",
    "    for subtask in eval_sequence:\n",
    "        low_level_started[subtask] += 1\n",
    "        print(\"Evaluating task \", subtask)\n",
    "        if record:\n",
    "            rollout_video.new_subtask()\n",
    "        success = rollout(env, model, task_checker, cfg, subtask, val_annotations, processor=processor, record=record, rollout_video=rollout_video)\n",
    "        if record:\n",
    "            rollout_video.draw_outcome(success)\n",
    "        if success:\n",
    "            low_level_completed[subtask] += 1\n",
    "            success_counter += 1\n",
    "        else:\n",
    "            return success_counter\n",
    "    return success_counter\n",
    "\n",
    "def rollout(env, model, task_oracle, cfg, subtask, val_annotations, processor, record=False, rollout_video=None):\n",
    "    if cfg.debug:\n",
    "        print(f\"{subtask} \", end=\"\")\n",
    "        time.sleep(0.5)\n",
    "    obs = env.get_obs()\n",
    "    # get lang annotation for subtask\n",
    "    lang_annotation = val_annotations[subtask][0]\n",
    "    #print(\"Instruction: \", lang_annotation)\n",
    "    #print(\"Setting a different instruction!\")\n",
    "    #lang_annotation = 'put the blue block on top of red block'\n",
    "    # get language goal embedding\n",
    "    if processor == None:\n",
    "        print(lang_annotation)\n",
    "        tasks = model.create_tasks(texts=[lang_annotation])\n",
    "        from octo.utils.train_callbacks import supply_rng\n",
    "        policy_fn = supply_rng(\n",
    "                partial(\n",
    "                    model.sample_actions,\n",
    "                    unnormalization_statistics=model.dataset_statistics[\"action\"],\n",
    "                ),\n",
    "            )\n",
    "        window_size = 4\n",
    "        act_step = 4\n",
    "\n",
    "    #model.reset()\n",
    "    start_info = env.get_info()\n",
    "    past_obs = None\n",
    "\n",
    "    for step in range(cfg.ep_len):\n",
    "        if processor == None:\n",
    "            if act_step > 0 and act_step % window_size == 0:\n",
    "                act_step = 0\n",
    "                \n",
    "                static_2 = resize_image(obs['rgb_obs']['rgb_static'], (256, 256), primary_octo=True)\n",
    "                gripper_2 = resize_image(obs['rgb_obs']['rgb_gripper'], (128, 128))\n",
    "                if past_obs:\n",
    "                    static_1 = resize_image(past_obs['rgb_obs']['rgb_static'], (256, 256), primary_octo=True)\n",
    "                    gripper_1 = resize_image(past_obs['rgb_obs']['rgb_gripper'], (128, 128))\n",
    "                    image_primary = np.stack([static_1, static_2])\n",
    "                    image_wrist = np.stack([gripper_1, gripper_2])\n",
    "                    timestep_pad_mask = np.array([[True, True]])\n",
    "\n",
    "                else:\n",
    "                    image_primary = np.stack([np.zeros((256, 256, 3)), static_2])\n",
    "                    image_wrist = np.stack([np.zeros((128, 128, 3)), gripper_2])\n",
    "                    timestep_pad_mask = np.array([[False, True]])\n",
    "                pad_mask_dict = {\n",
    "                    \"image_primary\": np.array([[True, True]]),\n",
    "                    #\"image_wrist\": np.array([[True, True]]),\n",
    "                    \"timestep\": np.array([[False, False]]),\n",
    "                }\n",
    "                \n",
    "                #image_primary = np.expand_dims(resize_image(obs['rgb_obs']['rgb_static'], (256, 256), primary_octo=True), 0)\n",
    "                #image_wrist = np.expand_dims(resize_image(obs['rgb_obs']['rgb_gripper'], (128, 128)), 0)\n",
    "                #timestep_pad_mask = np.array([[True]])\n",
    "                #pad_mask_dict = {\n",
    "                #    \"image_primary\": np.array([[True]]),\n",
    "                    #\"image_wrist\": np.array([[True]]),\n",
    "                #    \"timestep\": np.array([[True]]),\n",
    "                #}\n",
    "                observation = {\n",
    "                        \"image_primary\": np.expand_dims(image_primary, 0),  # uint8\n",
    "                        #\"image_wrist\": np.expand_dims(image_wrist, 0),      # uint8\n",
    "                        \"timestep_pad_mask\": timestep_pad_mask,\n",
    "                        \"pad_mask_dict\": pad_mask_dict,\n",
    "                        \"timestep\": np.array([[step-1, step]]),\n",
    "                }\n",
    "                if 'wrist' in cfg.image_obs_keys:\n",
    "                    observation['image_wrist'] = np.expand_dims(image_wrist, 0)\n",
    "                    pad_mask_dict[\"image_wrist\"] = np.array([[True, True]])\n",
    "\n",
    "                act_buffer = policy_fn(observation, tasks)\n",
    "                act_buffer = np.array(act_buffer[0])\n",
    "                action = act_buffer[act_step]\n",
    "            else:\n",
    "                action = act_buffer[act_step]\n",
    "            act_step += 1\n",
    "\n",
    "        else:\n",
    "            \n",
    "            observation = {\n",
    "                'full_image': resize_image(obs['rgb_obs']['rgb_static'], (224, 224))\n",
    "            }\n",
    "\n",
    "            action = get_action(\n",
    "                cfg,\n",
    "                model,\n",
    "                observation,\n",
    "                task_label=lang_annotation,\n",
    "                processor=processor,\n",
    "            )\n",
    "        \n",
    "        # Rescale gripper actions from [0, 1] to [-1, 1] and binarize\n",
    "        action = normalize_gripper_action(action)\n",
    "        past_obs = obs\n",
    "        obs, _, _, current_info = env.step(action)\n",
    "\n",
    "        #img = env.render(mode=\"rgb_array\")\n",
    "        #join_vis_lang_jup(img, lang_annotation)\n",
    "\n",
    "\n",
    "        if record:\n",
    "            # update video\n",
    "            frame_aug = torch.zeros((3, 224, 448))\n",
    "            resize = Resize(224, antialias=True)\n",
    "            frame_aug[:, :, :224] = resize(torch.tensor(obs[\"rgb_obs\"][\"rgb_static\"]).permute(2, 0, 1))\n",
    "            closest_obs = 0\n",
    "            if isinstance(closest_obs, int):\n",
    "                closest_obs = torch.zeros((3, 224, 224))\n",
    "            frame_aug[:, :, 224:] = closest_obs.squeeze()\n",
    "\n",
    "            rollout_video.update(frame_aug.unsqueeze(0).unsqueeze(0), step=step)\n",
    "\n",
    "        # check if current step solves a task\n",
    "        current_task_info = task_oracle.get_task_info_for_set(start_info, current_info, {subtask})\n",
    "\n",
    "        if len(current_task_info) > 0:\n",
    "            if cfg.debug:\n",
    "                print(colored(\"success\", \"green\"), end=\" \")\n",
    "            if record:\n",
    "                rollout_video.add_language_instruction(lang_annotation)\n",
    "            return True\n",
    "    if cfg.debug:\n",
    "        print(colored(\"fail\", \"red\"), end=\" \")\n",
    "    if record:\n",
    "        rollout_video.add_language_instruction(lang_annotation)\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The oracle:  <__main__.LowLevelTasks object at 0x72fbf4fc9450>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating sequence:  ['grasp_slider', 'move_slider_left', 'ungrasp_slider']\n",
      "Initial state:  {'led': 0, 'lightbulb': 0, 'slider': 'right', 'drawer': 'closed', 'red_block': 'table', 'blue_block': 'table', 'pink_block': 'slider_right', 'grasped': 0, 'contact': 0, 'red_block_lifted': 0, 'blue_block_lifted': 0, 'pink_block_lifted': 0}\n",
      "Evaluating task  grasp_slider\n",
      "grasp the slider handle\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">02/10 [18:45:46] </span><span style=\"color: #808000; text-decoration-color: #808000\">WARNING </span> | &gt;&gt; <span style=\"color: #008000; text-decoration-color: #008000\">'observations'</span> is missing items compared to example_batch:         <a href=\"file:///ubc/cs/research/nlp/grigorii/projects/octo_original/octo/octo/model/octo_model.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">octo_model.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///ubc/cs/research/nlp/grigorii/projects/octo_original/octo/octo/model/octo_model.py#533\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">533</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                 </span>         <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'task_completed'</span><span style=\"font-weight: bold\">}</span>                                                      <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                 </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m02/10 [18:45:46]\u001b[0m\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m | >> \u001b[32m'observations'\u001b[0m is missing items compared to example_batch:         \u001b]8;id=150092;file:///ubc/cs/research/nlp/grigorii/projects/octo_original/octo/octo/model/octo_model.py\u001b\\\u001b[2mocto_model.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=140722;file:///ubc/cs/research/nlp/grigorii/projects/octo_original/octo/octo/model/octo_model.py#533\u001b\\\u001b[2m533\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                 \u001b[0m         \u001b[1m{\u001b[0m\u001b[32m'task_completed'\u001b[0m\u001b[1m}\u001b[0m                                                      \u001b[2m                 \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">02/10 [18:45:47] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> | &gt;&gt; No image inputs matching <span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'image_wrist'</span>,<span style=\"font-weight: bold\">)</span> were found.Skipping      <a href=\"file:///ubc/cs/research/nlp/grigorii/projects/octo_original/octo/octo/model/components/tokenizers.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">tokenizers.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///ubc/cs/research/nlp/grigorii/projects/octo_original/octo/octo/model/components/tokenizers.py#110\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">110</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                 </span>         tokenizer entirely.                                                     <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                 </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m02/10 [18:45:47]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m | >> No image inputs matching \u001b[1m(\u001b[0m\u001b[32m'image_wrist'\u001b[0m,\u001b[1m)\u001b[0m were found.Skipping      \u001b]8;id=969560;file:///ubc/cs/research/nlp/grigorii/projects/octo_original/octo/octo/model/components/tokenizers.py\u001b\\\u001b[2mtokenizers.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=960232;file:///ubc/cs/research/nlp/grigorii/projects/octo_original/octo/octo/model/components/tokenizers.py#110\u001b\\\u001b[2m110\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                 \u001b[0m         tokenizer entirely.                                                     \u001b[2m                 \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                 </span><span style=\"color: #808000; text-decoration-color: #808000\">WARNING </span> | &gt;&gt; Skipping observation tokenizer: obs_wrist                         <a href=\"file:///ubc/cs/research/nlp/grigorii/projects/octo_original/octo/octo/model/octo_module.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">octo_module.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///ubc/cs/research/nlp/grigorii/projects/octo_original/octo/octo/model/octo_module.py#195\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">195</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                \u001b[0m\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m | >> Skipping observation tokenizer: obs_wrist                         \u001b]8;id=724168;file:///ubc/cs/research/nlp/grigorii/projects/octo_original/octo/octo/model/octo_module.py\u001b\\\u001b[2mocto_module.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=940444;file:///ubc/cs/research/nlp/grigorii/projects/octo_original/octo/octo/model/octo_module.py#195\u001b\\\u001b[2m195\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                 </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> | &gt;&gt; repeating task tokens at each timestep to perform cross-modal     <a href=\"file:///ubc/cs/research/nlp/grigorii/projects/octo_original/octo/octo/model/octo_module.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">octo_module.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///ubc/cs/research/nlp/grigorii/projects/octo_original/octo/octo/model/octo_module.py#220\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">220</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                 </span>         attention                                                              <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                  </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m | >> repeating task tokens at each timestep to perform cross-modal     \u001b]8;id=490191;file:///ubc/cs/research/nlp/grigorii/projects/octo_original/octo/octo/model/octo_module.py\u001b\\\u001b[2mocto_module.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=852656;file:///ubc/cs/research/nlp/grigorii/projects/octo_original/octo/octo/model/octo_module.py#220\u001b\\\u001b[2m220\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                 \u001b[0m         attention                                                              \u001b[2m                  \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">02/10 [18:45:56] </span><span style=\"color: #808000; text-decoration-color: #808000\">WARNING </span> | &gt;&gt; <span style=\"color: #008000; text-decoration-color: #008000\">'observations'</span> is missing items compared to example_batch:         <a href=\"file:///ubc/cs/research/nlp/grigorii/projects/octo_original/octo/octo/model/octo_model.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">octo_model.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///ubc/cs/research/nlp/grigorii/projects/octo_original/octo/octo/model/octo_model.py#533\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">533</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                 </span>         <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'task_completed'</span><span style=\"font-weight: bold\">}</span>                                                      <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                 </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m02/10 [18:45:56]\u001b[0m\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m | >> \u001b[32m'observations'\u001b[0m is missing items compared to example_batch:         \u001b]8;id=804291;file:///ubc/cs/research/nlp/grigorii/projects/octo_original/octo/octo/model/octo_model.py\u001b\\\u001b[2mocto_model.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=716666;file:///ubc/cs/research/nlp/grigorii/projects/octo_original/octo/octo/model/octo_model.py#533\u001b\\\u001b[2m533\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                 \u001b[0m         \u001b[1m{\u001b[0m\u001b[32m'task_completed'\u001b[0m\u001b[1m}\u001b[0m                                                      \u001b[2m                 \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                 </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> | &gt;&gt; No image inputs matching <span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'image_wrist'</span>,<span style=\"font-weight: bold\">)</span> were found.Skipping      <a href=\"file:///ubc/cs/research/nlp/grigorii/projects/octo_original/octo/octo/model/components/tokenizers.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">tokenizers.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///ubc/cs/research/nlp/grigorii/projects/octo_original/octo/octo/model/components/tokenizers.py#110\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">110</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                 </span>         tokenizer entirely.                                                     <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                 </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m | >> No image inputs matching \u001b[1m(\u001b[0m\u001b[32m'image_wrist'\u001b[0m,\u001b[1m)\u001b[0m were found.Skipping      \u001b]8;id=640861;file:///ubc/cs/research/nlp/grigorii/projects/octo_original/octo/octo/model/components/tokenizers.py\u001b\\\u001b[2mtokenizers.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=166747;file:///ubc/cs/research/nlp/grigorii/projects/octo_original/octo/octo/model/components/tokenizers.py#110\u001b\\\u001b[2m110\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                 \u001b[0m         tokenizer entirely.                                                     \u001b[2m                 \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                 </span><span style=\"color: #808000; text-decoration-color: #808000\">WARNING </span> | &gt;&gt; Skipping observation tokenizer: obs_wrist                         <a href=\"file:///ubc/cs/research/nlp/grigorii/projects/octo_original/octo/octo/model/octo_module.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">octo_module.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///ubc/cs/research/nlp/grigorii/projects/octo_original/octo/octo/model/octo_module.py#195\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">195</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                \u001b[0m\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m | >> Skipping observation tokenizer: obs_wrist                         \u001b]8;id=259031;file:///ubc/cs/research/nlp/grigorii/projects/octo_original/octo/octo/model/octo_module.py\u001b\\\u001b[2mocto_module.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=802008;file:///ubc/cs/research/nlp/grigorii/projects/octo_original/octo/octo/model/octo_module.py#195\u001b\\\u001b[2m195\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                 </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> | &gt;&gt; repeating task tokens at each timestep to perform cross-modal     <a href=\"file:///ubc/cs/research/nlp/grigorii/projects/octo_original/octo/octo/model/octo_module.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">octo_module.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///ubc/cs/research/nlp/grigorii/projects/octo_original/octo/octo/model/octo_module.py#220\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">220</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                 </span>         attention                                                              <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                  </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m | >> repeating task tokens at each timestep to perform cross-modal     \u001b]8;id=786040;file:///ubc/cs/research/nlp/grigorii/projects/octo_original/octo/octo/model/octo_module.py\u001b\\\u001b[2mocto_module.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=715908;file:///ubc/cs/research/nlp/grigorii/projects/octo_original/octo/octo/model/octo_module.py#220\u001b\\\u001b[2m220\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                 \u001b[0m         attention                                                              \u001b[2m                  \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1/5 : 0.0% | 2/5 : 0.0% | 3/5 : 0.0% | 4/5 : 0.0% | 5/5 : 0.0% | Average: 0.0 |:   2%|         | 1/50 [00:43<35:51, 43.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating sequence:  ['grasp_blue_block', 'lift_grasped_block']\n",
      "Initial state:  {'led': 0, 'lightbulb': 0, 'slider': 'right', 'drawer': 'closed', 'red_block': 'table', 'blue_block': 'table', 'pink_block': 'slider_left', 'grasped': 0, 'contact': 0, 'red_block_lifted': 0, 'blue_block_lifted': 0, 'pink_block_lifted': 0}\n",
      "Evaluating task  grasp_blue_block\n",
      "grasp the blue block\n",
      "Evaluating task  lift_grasped_block\n",
      "lift the grasped block\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1/5 : 50.0% | 2/5 : 50.0% | 3/5 : 0.0% | 4/5 : 0.0% | 5/5 : 0.0% | Average: 1.0 |:   4%|         | 2/50 [00:51<18:07, 22.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating sequence:  ['contact_pink_block_left', 'push_block_right']\n",
      "Initial state:  {'led': 0, 'lightbulb': 0, 'slider': 'right', 'drawer': 'closed', 'red_block': 'table', 'blue_block': 'slider_right', 'pink_block': 'table', 'grasped': 0, 'contact': 0, 'red_block_lifted': 0, 'blue_block_lifted': 0, 'pink_block_lifted': 0}\n",
      "Evaluating task  contact_pink_block_left\n",
      "touch the pink block on its left side\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1/5 : 33.3% | 2/5 : 33.3% | 3/5 : 0.0% | 4/5 : 0.0% | 5/5 : 0.0% | Average: 0.7 |:   6%|         | 3/50 [01:15<18:15, 23.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating sequence:  ['grasp_red_block', 'lift_grasped_block']\n",
      "Initial state:  {'led': 0, 'lightbulb': 0, 'slider': 'right', 'drawer': 'closed', 'red_block': 'table', 'blue_block': 'slider_right', 'pink_block': 'slider_left', 'grasped': 0, 'contact': 0, 'red_block_lifted': 0, 'blue_block_lifted': 0, 'pink_block_lifted': 0}\n",
      "Evaluating task  grasp_red_block\n",
      "grasp the red block\n",
      "Evaluating task  lift_grasped_block\n",
      "lift the grasped block\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1/5 : 50.0% | 2/5 : 25.0% | 3/5 : 0.0% | 4/5 : 0.0% | 5/5 : 0.0% | Average: 0.8 |:   8%|         | 4/50 [01:50<21:24, 27.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating sequence:  ['grasp_red_block', 'lift_grasped_block', 'place_grasped_block_over_slider', 'ungrasp_block']\n",
      "Initial state:  {'led': 0, 'lightbulb': 0, 'slider': 'right', 'drawer': 'closed', 'red_block': 'table', 'blue_block': 'slider_left', 'pink_block': 'table', 'grasped': 0, 'contact': 0, 'red_block_lifted': 0, 'blue_block_lifted': 0, 'pink_block_lifted': 0}\n",
      "Evaluating task  grasp_red_block\n",
      "grasp the red block\n",
      "Evaluating task  lift_grasped_block\n",
      "lift the grasped block\n",
      "Evaluating task  place_grasped_block_over_slider\n",
      "place the grasped block over the sliding cabinet\n",
      "Evaluating task  ungrasp_block\n",
      "ungrasp the block\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.1831850009859753\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.18586364002968456\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.18667560895690316\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.18661662903361378\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.18659302551633206\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.18658398903608925\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.1865793094559232\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.18657701599527704\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.1865807216708609\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.18653536686100633\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.1865719812711949\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.18659283124005988\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.1866302210655293\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.18662264875709111\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.1866165420255232\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.1866132951519993\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.18661163794304692\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.18661077290293301\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.18661034476742278\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.1866100709419198\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.1866099328810168\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.18660984911251885\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.1865975341391347\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.18660552471158118\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.18660840652318755\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.18660944313749153\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.1866098162458925\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.1866099185311798\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.1866099592749525\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.18660984629399943\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.18660989978994785\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.18660985886040707\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.18660993255997357\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.1866108356196293\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.18661218513774913\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.18661287759494058\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.18661313989283257\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.1866131966888894\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.18661314812900825\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.1866130455561354\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.18661291529314455\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.1866127708355146\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.18661261910226593\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.18661246364072806\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.1866123062695917\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.18661214792128678\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.18661198907386392\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.1866118299724165\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.18661167074260188\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.18661151144884405\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.1866113521241727\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.18661119278552174\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.18661103344157326\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.18661087409677837\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.18661071475341917\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.18661055541266564\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.18661039607511756\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.18661023674108235\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.18661007741071803\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.18660991808410513\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.18660975876128502\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.18660959944227895\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.18660944012709776\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.18660928081574704\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.18660912150822967\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.18660896220454704\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.18660880290469983\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.18660864360868842\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.186608484316513\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.18660832502817357\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.18660816574367026\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.18660800646300288\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.18660784718617165\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.18660768791317633\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.18660752864401692\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.1866073693786934\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.18660721011720585\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.1866070508595542\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.18660689160573837\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.18660673235575834\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.18660657310961393\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.18660641386730525\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.18660625462883224\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.18660609539419498\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.18660593616339344\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.18660577693642744\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.18660561771329695\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.18660545849400187\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.1866052992785423\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.18660514006691836\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.18660498085912966\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.18660482165517642\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.1866046624550584\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.18660450325877578\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.1866043440663284\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.1866041848777163\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.18660402569293938\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.18660386651199767\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.186603707334891\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.1866035481616196\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.18660338899218315\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.18660322982658167\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.18660307066481535\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.1866029115068838\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.18660275235278725\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.18660259320252562\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.1866024340560988\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.18660227491350687\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.1866021157747497\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.18660195663982715\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.18660179750873945\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.1866016383814864\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.18660147925806797\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.18660132013848418\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.18660116102273494\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.1866010019108202\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.18660084280274006\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.18660068369849434\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.18660052459808313\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.1866003655015062\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.18660020640876362\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.18660004731985547\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.18659988823478157\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.18659972915354184\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.18659957007613645\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.18659941100256525\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.18659925193282834\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.1865990928669254\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.18659893380485656\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.18659877474662195\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.1865986156922212\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.1865984566416544\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.18659829759492175\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.18659813855202295\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.1865973025777646\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.18659638947668186\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.18659700436412782\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.1865972427324135\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.18659728744702975\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.1865972328800229\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.18659645425479668\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.18659498798536725\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.18659421900306322\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.18659374764020034\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.1865934989999477\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.1865838179676369\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.18658839603934602\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.18659074625633257\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.18659195516730345\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.18659257486181882\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.1865928898354057\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.1865930532427419\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.18653600176329996\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.18656383421243264\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.18657811475216302\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.18658545048642472\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.18658921634273687\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.18659114925730907\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.18921155302466938\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.194052348454885\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.1997563405539648\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.20347687230804176\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.2036522153864773\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.20350500979714442\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.20341384116576053\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.20339167368858493\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.2033838264769321\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.20337979903009426\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.20337773418760177\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.20337693700252904\n",
      "The grasped block is  block_red\n",
      "block_red moved xy too much! by 0.2033762794399415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1/5 : 60.0% | 2/5 : 40.0% | 3/5 : 20.0% | 4/5 : 0.0% | 5/5 : 0.0% | Average: 1.2 |:  10%|         | 5/50 [02:53<30:25, 40.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating sequence:  ['grasp_red_block', 'rotate_grasped_block_right', 'ungrasp_block']\n",
      "Initial state:  {'led': 0, 'lightbulb': 0, 'slider': 'right', 'drawer': 'closed', 'red_block': 'table', 'blue_block': 'slider_left', 'pink_block': 'slider_right', 'grasped': 0, 'contact': 0, 'red_block_lifted': 0, 'blue_block_lifted': 0, 'pink_block_lifted': 0}\n",
      "Evaluating task  grasp_red_block\n",
      "grasp the red block\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1/5 : 50.0% | 2/5 : 33.3% | 3/5 : 16.7% | 4/5 : 0.0% | 5/5 : 0.0% | Average: 1.0 |:  12%|        | 6/50 [03:18<25:54, 35.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating sequence:  ['grasp_blue_block', 'lift_grasped_block']\n",
      "Initial state:  {'led': 0, 'lightbulb': 0, 'slider': 'right', 'drawer': 'closed', 'red_block': 'slider_right', 'blue_block': 'table', 'pink_block': 'table', 'grasped': 0, 'contact': 0, 'red_block_lifted': 0, 'blue_block_lifted': 0, 'pink_block_lifted': 0}\n",
      "Evaluating task  grasp_blue_block\n",
      "grasp the blue block\n",
      "Evaluating task  lift_grasped_block\n",
      "lift the grasped block\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1/5 : 57.1% | 2/5 : 28.6% | 3/5 : 14.3% | 4/5 : 0.0% | 5/5 : 0.0% | Average: 1.0 |:  14%|        | 7/50 [03:44<23:01, 32.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating sequence:  ['grasp_blue_block', 'lift_grasped_block', 'place_grasped_block_over_slider', 'ungrasp_block']\n",
      "Initial state:  {'led': 0, 'lightbulb': 0, 'slider': 'right', 'drawer': 'closed', 'red_block': 'slider_right', 'blue_block': 'table', 'pink_block': 'slider_left', 'grasped': 0, 'contact': 0, 'red_block_lifted': 0, 'blue_block_lifted': 0, 'pink_block_lifted': 0}\n",
      "Evaluating task  grasp_blue_block\n",
      "grasp the blue block\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1/5 : 50.0% | 2/5 : 25.0% | 3/5 : 12.5% | 4/5 : 0.0% | 5/5 : 0.0% | Average: 0.9 |:  16%|        | 8/50 [04:03<19:34, 27.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating sequence:  ['grasp_pink_block', 'lift_grasped_block', 'place_grasped_block_over_slider', 'ungrasp_block']\n",
      "Initial state:  {'led': 0, 'lightbulb': 0, 'slider': 'right', 'drawer': 'closed', 'red_block': 'slider_right', 'blue_block': 'slider_left', 'pink_block': 'table', 'grasped': 0, 'contact': 0, 'red_block_lifted': 0, 'blue_block_lifted': 0, 'pink_block_lifted': 0}\n",
      "Evaluating task  grasp_pink_block\n",
      "grasp the pink block\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1/5 : 44.4% | 2/5 : 22.2% | 3/5 : 11.1% | 4/5 : 0.0% | 5/5 : 0.0% | Average: 0.8 |:  18%|        | 9/50 [04:17<16:12, 23.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating sequence:  ['grasp_blue_block', 'rotate_grasped_block_left', 'ungrasp_block']\n",
      "Initial state:  {'led': 0, 'lightbulb': 0, 'slider': 'right', 'drawer': 'closed', 'red_block': 'slider_left', 'blue_block': 'table', 'pink_block': 'table', 'grasped': 0, 'contact': 0, 'red_block_lifted': 0, 'blue_block_lifted': 0, 'pink_block_lifted': 0}\n",
      "Evaluating task  grasp_blue_block\n",
      "grasp the blue block\n",
      "Evaluating task  rotate_grasped_block_left\n",
      "rotate the grasped block 90 degrees to the left\n",
      "Evaluating task  ungrasp_block\n",
      "ungrasp the block\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1/5 : 50.0% | 2/5 : 30.0% | 3/5 : 20.0% | 4/5 : 0.0% | 5/5 : 0.0% | Average: 1.0 |:  20%|        | 10/50 [04:26<12:42, 19.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The grasped block is  block_blue\n",
      "Evaluating sequence:  ['grasp_slider', 'move_slider_left', 'ungrasp_slider']\n",
      "Initial state:  {'led': 0, 'lightbulb': 0, 'slider': 'right', 'drawer': 'closed', 'red_block': 'slider_left', 'blue_block': 'table', 'pink_block': 'slider_right', 'grasped': 0, 'contact': 0, 'red_block_lifted': 0, 'blue_block_lifted': 0, 'pink_block_lifted': 0}\n",
      "Evaluating task  grasp_slider\n",
      "grasp the slider handle\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1/5 : 45.5% | 2/5 : 27.3% | 3/5 : 18.2% | 4/5 : 0.0% | 5/5 : 0.0% | Average: 0.9 |:  22%|       | 11/50 [04:43<12:00, 18.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating sequence:  ['grasp_red_block', 'lift_grasped_block', 'place_grasped_block_over_pink_block', 'ungrasp_block']\n",
      "Initial state:  {'led': 0, 'lightbulb': 0, 'slider': 'right', 'drawer': 'closed', 'red_block': 'slider_left', 'blue_block': 'slider_right', 'pink_block': 'table', 'grasped': 0, 'contact': 0, 'red_block_lifted': 0, 'blue_block_lifted': 0, 'pink_block_lifted': 0}\n",
      "Evaluating task  grasp_red_block\n",
      "grasp the red block\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1/5 : 41.7% | 2/5 : 25.0% | 3/5 : 16.7% | 4/5 : 0.0% | 5/5 : 0.0% | Average: 0.8 |:  24%|       | 12/50 [04:59<11:05, 17.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating sequence:  ['grasp_blue_block', 'lift_grasped_block', 'place_grasped_block_over_drawer', 'ungrasp_block']\n",
      "Initial state:  {'led': 0, 'lightbulb': 0, 'slider': 'right', 'drawer': 'open', 'red_block': 'table', 'blue_block': 'table', 'pink_block': 'slider_right', 'grasped': 0, 'contact': 0, 'red_block_lifted': 0, 'blue_block_lifted': 0, 'pink_block_lifted': 0}\n",
      "Evaluating task  grasp_blue_block\n",
      "grasp the blue block\n",
      "Evaluating task  lift_grasped_block\n",
      "lift the grasped block\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1/5 : 46.2% | 2/5 : 23.1% | 3/5 : 15.4% | 4/5 : 0.0% | 5/5 : 0.0% | Average: 0.8 |:  26%|       | 13/50 [05:32<13:46, 22.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating sequence:  ['contact_blue_block_right', 'push_block_left']\n",
      "Initial state:  {'led': 0, 'lightbulb': 0, 'slider': 'right', 'drawer': 'open', 'red_block': 'table', 'blue_block': 'table', 'pink_block': 'slider_left', 'grasped': 0, 'contact': 0, 'red_block_lifted': 0, 'blue_block_lifted': 0, 'pink_block_lifted': 0}\n",
      "Evaluating task  contact_blue_block_right\n",
      "touch the blue block on its right side\n",
      "Evaluating task  push_block_left\n",
      "push the block towards the left\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1/5 : 50.0% | 2/5 : 21.4% | 3/5 : 14.3% | 4/5 : 0.0% | 5/5 : 0.0% | Average: 0.9 |:  28%|       | 14/50 [06:02<14:44, 24.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating sequence:  ['grasp_pink_block', 'lift_grasped_block']\n",
      "Initial state:  {'led': 0, 'lightbulb': 0, 'slider': 'right', 'drawer': 'open', 'red_block': 'table', 'blue_block': 'slider_right', 'pink_block': 'table', 'grasped': 0, 'contact': 0, 'red_block_lifted': 0, 'blue_block_lifted': 0, 'pink_block_lifted': 0}\n",
      "Evaluating task  grasp_pink_block\n",
      "grasp the pink block\n",
      "Evaluating task  lift_grasped_block\n",
      "lift the grasped block\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1/5 : 53.3% | 2/5 : 20.0% | 3/5 : 13.3% | 4/5 : 0.0% | 5/5 : 0.0% | Average: 0.9 |:  30%|       | 15/50 [06:25<14:11, 24.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating sequence:  ['grasp_slider', 'move_slider_left', 'ungrasp_slider']\n",
      "Initial state:  {'led': 0, 'lightbulb': 0, 'slider': 'right', 'drawer': 'open', 'red_block': 'table', 'blue_block': 'slider_right', 'pink_block': 'slider_left', 'grasped': 0, 'contact': 0, 'red_block_lifted': 0, 'blue_block_lifted': 0, 'pink_block_lifted': 0}\n",
      "Evaluating task  grasp_slider\n",
      "grasp the slider handle\n",
      "Evaluating task  move_slider_left\n",
      "move the handle to the left\n",
      "Evaluating task  ungrasp_slider\n",
      "ungrasp the slider handle\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1/5 : 56.2% | 2/5 : 25.0% | 3/5 : 12.5% | 4/5 : 0.0% | 5/5 : 0.0% | Average: 0.9 |:  32%|      | 16/50 [07:05<16:20, 28.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating sequence:  ['grasp_pink_block', 'lift_grasped_block']\n",
      "Initial state:  {'led': 0, 'lightbulb': 0, 'slider': 'right', 'drawer': 'open', 'red_block': 'table', 'blue_block': 'slider_left', 'pink_block': 'table', 'grasped': 0, 'contact': 0, 'red_block_lifted': 0, 'blue_block_lifted': 0, 'pink_block_lifted': 0}\n",
      "Evaluating task  grasp_pink_block\n",
      "grasp the pink block\n",
      "Evaluating task  lift_grasped_block\n",
      "lift the grasped block\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1/5 : 58.8% | 2/5 : 23.5% | 3/5 : 11.8% | 4/5 : 0.0% | 5/5 : 0.0% | Average: 0.9 |:  34%|      | 17/50 [07:26<14:37, 26.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating sequence:  ['contact_red_block_left', 'push_block_right']\n",
      "Initial state:  {'led': 0, 'lightbulb': 0, 'slider': 'right', 'drawer': 'open', 'red_block': 'table', 'blue_block': 'slider_left', 'pink_block': 'slider_right', 'grasped': 0, 'contact': 0, 'red_block_lifted': 0, 'blue_block_lifted': 0, 'pink_block_lifted': 0}\n",
      "Evaluating task  contact_red_block_left\n",
      "touch the red block on its left side\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1/5 : 55.6% | 2/5 : 22.2% | 3/5 : 11.1% | 4/5 : 0.0% | 5/5 : 0.0% | Average: 0.9 |:  36%|      | 18/50 [07:50<13:49, 25.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating sequence:  ['grasp_pink_block', 'lift_grasped_block', 'place_grasped_block_over_drawer', 'ungrasp_block']\n",
      "Initial state:  {'led': 0, 'lightbulb': 0, 'slider': 'right', 'drawer': 'open', 'red_block': 'slider_right', 'blue_block': 'table', 'pink_block': 'table', 'grasped': 0, 'contact': 0, 'red_block_lifted': 0, 'blue_block_lifted': 0, 'pink_block_lifted': 0}\n",
      "Evaluating task  grasp_pink_block\n",
      "grasp the pink block\n",
      "Evaluating task  lift_grasped_block\n",
      "lift the grasped block\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1/5 : 57.9% | 2/5 : 21.1% | 3/5 : 10.5% | 4/5 : 0.0% | 5/5 : 0.0% | Average: 0.9 |:  38%|      | 19/50 [08:14<13:04, 25.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating sequence:  ['contact_blue_block_left', 'push_block_right']\n",
      "Initial state:  {'led': 0, 'lightbulb': 0, 'slider': 'right', 'drawer': 'open', 'red_block': 'slider_right', 'blue_block': 'table', 'pink_block': 'slider_left', 'grasped': 0, 'contact': 0, 'red_block_lifted': 0, 'blue_block_lifted': 0, 'pink_block_lifted': 0}\n",
      "Evaluating task  contact_blue_block_left\n",
      "touch the blue block on its left side\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1/5 : 55.0% | 2/5 : 20.0% | 3/5 : 10.0% | 4/5 : 0.0% | 5/5 : 0.0% | Average: 0.8 |:  40%|      | 20/50 [08:31<11:21, 22.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating sequence:  ['contact_pink_block_left', 'push_block_right']\n",
      "Initial state:  {'led': 0, 'lightbulb': 0, 'slider': 'right', 'drawer': 'open', 'red_block': 'slider_right', 'blue_block': 'slider_left', 'pink_block': 'table', 'grasped': 0, 'contact': 0, 'red_block_lifted': 0, 'blue_block_lifted': 0, 'pink_block_lifted': 0}\n",
      "Evaluating task  contact_pink_block_left\n",
      "touch the pink block on its left side\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1/5 : 52.4% | 2/5 : 19.0% | 3/5 : 9.5% | 4/5 : 0.0% | 5/5 : 0.0% | Average: 0.8 |:  42%|     | 21/50 [08:46<09:52, 20.43s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating sequence:  ['contact_blue_block_left', 'push_block_right']\n",
      "Initial state:  {'led': 0, 'lightbulb': 0, 'slider': 'right', 'drawer': 'open', 'red_block': 'slider_left', 'blue_block': 'table', 'pink_block': 'table', 'grasped': 0, 'contact': 0, 'red_block_lifted': 0, 'blue_block_lifted': 0, 'pink_block_lifted': 0}\n",
      "Evaluating task  contact_blue_block_left\n",
      "touch the blue block on its left side\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1/5 : 50.0% | 2/5 : 18.2% | 3/5 : 9.1% | 4/5 : 0.0% | 5/5 : 0.0% | Average: 0.8 |:  44%|     | 22/50 [09:08<09:42, 20.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating sequence:  ['grasp_blue_block', 'lift_grasped_block', 'place_grasped_block_over_slider', 'ungrasp_block']\n",
      "Initial state:  {'led': 0, 'lightbulb': 0, 'slider': 'right', 'drawer': 'open', 'red_block': 'slider_left', 'blue_block': 'table', 'pink_block': 'slider_right', 'grasped': 0, 'contact': 0, 'red_block_lifted': 0, 'blue_block_lifted': 0, 'pink_block_lifted': 0}\n",
      "Evaluating task  grasp_blue_block\n",
      "grasp the blue block\n",
      "Evaluating task  lift_grasped_block\n",
      "lift the grasped block\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1/5 : 52.2% | 2/5 : 17.4% | 3/5 : 8.7% | 4/5 : 0.0% | 5/5 : 0.0% | Average: 0.8 |:  46%|     | 23/50 [09:34<10:01, 22.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating sequence:  ['grasp_pink_block', 'lift_grasped_block']\n",
      "Initial state:  {'led': 0, 'lightbulb': 0, 'slider': 'right', 'drawer': 'open', 'red_block': 'slider_left', 'blue_block': 'slider_right', 'pink_block': 'table', 'grasped': 0, 'contact': 0, 'red_block_lifted': 0, 'blue_block_lifted': 0, 'pink_block_lifted': 0}\n",
      "Evaluating task  grasp_pink_block\n",
      "grasp the pink block\n",
      "Evaluating task  lift_grasped_block\n",
      "lift the grasped block\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1/5 : 54.2% | 2/5 : 16.7% | 3/5 : 8.3% | 4/5 : 0.0% | 5/5 : 0.0% | Average: 0.8 |:  48%|     | 24/50 [10:02<10:27, 24.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating sequence:  ['grasp_blue_block', 'lift_grasped_block', 'place_grasped_block_over_red_block', 'ungrasp_block']\n",
      "Initial state:  {'led': 0, 'lightbulb': 0, 'slider': 'left', 'drawer': 'closed', 'red_block': 'table', 'blue_block': 'table', 'pink_block': 'slider_right', 'grasped': 0, 'contact': 0, 'red_block_lifted': 0, 'blue_block_lifted': 0, 'pink_block_lifted': 0}\n",
      "Evaluating task  grasp_blue_block\n",
      "grasp the blue block\n",
      "Evaluating task  lift_grasped_block\n",
      "lift the grasped block\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1/5 : 56.0% | 2/5 : 16.0% | 3/5 : 8.0% | 4/5 : 0.0% | 5/5 : 0.0% | Average: 0.8 |:  50%|     | 25/50 [10:35<11:10, 26.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating sequence:  ['grasp_blue_block', 'lift_grasped_block']\n",
      "Initial state:  {'led': 0, 'lightbulb': 0, 'slider': 'left', 'drawer': 'closed', 'red_block': 'table', 'blue_block': 'table', 'pink_block': 'slider_left', 'grasped': 0, 'contact': 0, 'red_block_lifted': 0, 'blue_block_lifted': 0, 'pink_block_lifted': 0}\n",
      "Evaluating task  grasp_blue_block\n",
      "grasp the blue block\n",
      "Evaluating task  lift_grasped_block\n",
      "lift the grasped block\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1/5 : 57.7% | 2/5 : 19.2% | 3/5 : 7.7% | 4/5 : 0.0% | 5/5 : 0.0% | Average: 0.8 |:  52%|    | 26/50 [10:42<08:16, 20.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating sequence:  ['contact_pink_block_right', 'push_block_left']\n",
      "Initial state:  {'led': 0, 'lightbulb': 0, 'slider': 'left', 'drawer': 'closed', 'red_block': 'table', 'blue_block': 'slider_right', 'pink_block': 'table', 'grasped': 0, 'contact': 0, 'red_block_lifted': 0, 'blue_block_lifted': 0, 'pink_block_lifted': 0}\n",
      "Evaluating task  contact_pink_block_right\n",
      "touch the pink block on its right side\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1/5 : 55.6% | 2/5 : 18.5% | 3/5 : 7.4% | 4/5 : 0.0% | 5/5 : 0.0% | Average: 0.8 |:  54%|    | 27/50 [11:11<08:54, 23.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating sequence:  ['contact_red_block_left', 'push_block_right']\n",
      "Initial state:  {'led': 0, 'lightbulb': 0, 'slider': 'left', 'drawer': 'closed', 'red_block': 'table', 'blue_block': 'slider_right', 'pink_block': 'slider_left', 'grasped': 0, 'contact': 0, 'red_block_lifted': 0, 'blue_block_lifted': 0, 'pink_block_lifted': 0}\n",
      "Evaluating task  contact_red_block_left\n",
      "touch the red block on its left side\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1/5 : 53.6% | 2/5 : 17.9% | 3/5 : 7.1% | 4/5 : 0.0% | 5/5 : 0.0% | Average: 0.8 |:  56%|    | 28/50 [11:34<08:33, 23.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating sequence:  ['grasp_pink_block', 'lift_grasped_block']\n",
      "Initial state:  {'led': 0, 'lightbulb': 0, 'slider': 'left', 'drawer': 'closed', 'red_block': 'table', 'blue_block': 'slider_left', 'pink_block': 'table', 'grasped': 0, 'contact': 0, 'red_block_lifted': 0, 'blue_block_lifted': 0, 'pink_block_lifted': 0}\n",
      "Evaluating task  grasp_pink_block\n",
      "grasp the pink block\n",
      "Evaluating task  lift_grasped_block\n",
      "lift the grasped block\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1/5 : 55.2% | 2/5 : 20.7% | 3/5 : 6.9% | 4/5 : 0.0% | 5/5 : 0.0% | Average: 0.8 |:  58%|    | 29/50 [11:38<06:09, 17.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating sequence:  ['grasp_red_block', 'lift_grasped_block']\n",
      "Initial state:  {'led': 0, 'lightbulb': 0, 'slider': 'left', 'drawer': 'closed', 'red_block': 'table', 'blue_block': 'slider_left', 'pink_block': 'slider_right', 'grasped': 0, 'contact': 0, 'red_block_lifted': 0, 'blue_block_lifted': 0, 'pink_block_lifted': 0}\n",
      "Evaluating task  grasp_red_block\n",
      "grasp the red block\n",
      "Evaluating task  lift_grasped_block\n",
      "lift the grasped block\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1/5 : 56.7% | 2/5 : 23.3% | 3/5 : 6.7% | 4/5 : 0.0% | 5/5 : 0.0% | Average: 0.9 |:  60%|    | 30/50 [11:43<04:35, 13.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating sequence:  ['grasp_red_block', 'lift_grasped_block']\n",
      "Initial state:  {'led': 0, 'lightbulb': 0, 'slider': 'left', 'drawer': 'closed', 'red_block': 'slider_right', 'blue_block': 'table', 'pink_block': 'table', 'grasped': 0, 'contact': 0, 'red_block_lifted': 0, 'blue_block_lifted': 0, 'pink_block_lifted': 0}\n",
      "Evaluating task  grasp_red_block\n",
      "grasp the red block\n",
      "Evaluating task  lift_grasped_block\n",
      "lift the grasped block\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1/5 : 58.1% | 2/5 : 22.6% | 3/5 : 6.5% | 4/5 : 0.0% | 5/5 : 0.0% | Average: 0.9 |:  62%|   | 31/50 [12:48<09:10, 28.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating sequence:  ['grasp_slider', 'move_slider_right', 'ungrasp_slider']\n",
      "Initial state:  {'led': 0, 'lightbulb': 0, 'slider': 'left', 'drawer': 'closed', 'red_block': 'slider_right', 'blue_block': 'table', 'pink_block': 'slider_left', 'grasped': 0, 'contact': 0, 'red_block_lifted': 0, 'blue_block_lifted': 0, 'pink_block_lifted': 0}\n",
      "Evaluating task  grasp_slider\n",
      "grasp the slider handle\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1/5 : 56.2% | 2/5 : 21.9% | 3/5 : 6.2% | 4/5 : 0.0% | 5/5 : 0.0% | Average: 0.8 |:  64%|   | 32/50 [13:09<07:58, 26.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating sequence:  ['grasp_slider', 'move_slider_right', 'ungrasp_slider']\n",
      "Initial state:  {'led': 0, 'lightbulb': 0, 'slider': 'left', 'drawer': 'closed', 'red_block': 'slider_right', 'blue_block': 'slider_left', 'pink_block': 'table', 'grasped': 0, 'contact': 0, 'red_block_lifted': 0, 'blue_block_lifted': 0, 'pink_block_lifted': 0}\n",
      "Evaluating task  grasp_slider\n",
      "grasp the slider handle\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1/5 : 54.5% | 2/5 : 21.2% | 3/5 : 6.1% | 4/5 : 0.0% | 5/5 : 0.0% | Average: 0.8 |:  66%|   | 33/50 [13:28<06:56, 24.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating sequence:  ['grasp_drawer', 'open_drawer', 'ungrasp_drawer']\n",
      "Initial state:  {'led': 0, 'lightbulb': 0, 'slider': 'left', 'drawer': 'closed', 'red_block': 'slider_left', 'blue_block': 'table', 'pink_block': 'table', 'grasped': 0, 'contact': 0, 'red_block_lifted': 0, 'blue_block_lifted': 0, 'pink_block_lifted': 0}\n",
      "Evaluating task  grasp_drawer\n",
      "grasp the drawer handle\n",
      "Evaluating task  open_drawer\n",
      "pull the handle\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1/5 : 55.9% | 2/5 : 20.6% | 3/5 : 5.9% | 4/5 : 0.0% | 5/5 : 0.0% | Average: 0.8 |:  68%|   | 34/50 [13:54<06:39, 24.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating sequence:  ['grasp_blue_block', 'rotate_grasped_block_right', 'ungrasp_block']\n",
      "Initial state:  {'led': 0, 'lightbulb': 0, 'slider': 'left', 'drawer': 'closed', 'red_block': 'slider_left', 'blue_block': 'table', 'pink_block': 'slider_right', 'grasped': 0, 'contact': 0, 'red_block_lifted': 0, 'blue_block_lifted': 0, 'pink_block_lifted': 0}\n",
      "Evaluating task  grasp_blue_block\n",
      "grasp the blue block\n",
      "Evaluating task  rotate_grasped_block_right\n",
      "rotate the grasped block 90 degrees to the right\n",
      "Evaluating task  ungrasp_block\n",
      "ungrasp the block\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1/5 : 57.1% | 2/5 : 22.9% | 3/5 : 8.6% | 4/5 : 0.0% | 5/5 : 0.0% | Average: 0.9 |:  70%|   | 35/50 [14:11<05:38, 22.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The grasped block is  block_blue\n",
      "Evaluating sequence:  ['grasp_pink_block', 'lift_grasped_block']\n",
      "Initial state:  {'led': 0, 'lightbulb': 0, 'slider': 'left', 'drawer': 'closed', 'red_block': 'slider_left', 'blue_block': 'slider_right', 'pink_block': 'table', 'grasped': 0, 'contact': 0, 'red_block_lifted': 0, 'blue_block_lifted': 0, 'pink_block_lifted': 0}\n",
      "Evaluating task  grasp_pink_block\n",
      "grasp the pink block\n",
      "Evaluating task  lift_grasped_block\n",
      "lift the grasped block\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1/5 : 58.3% | 2/5 : 25.0% | 3/5 : 8.3% | 4/5 : 0.0% | 5/5 : 0.0% | Average: 0.9 |:  72%|  | 36/50 [14:14<03:54, 16.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating sequence:  ['contact_blue_block_right', 'push_block_left']\n",
      "Initial state:  {'led': 0, 'lightbulb': 0, 'slider': 'left', 'drawer': 'open', 'red_block': 'table', 'blue_block': 'table', 'pink_block': 'slider_right', 'grasped': 0, 'contact': 0, 'red_block_lifted': 0, 'blue_block_lifted': 0, 'pink_block_lifted': 0}\n",
      "Evaluating task  contact_blue_block_right\n",
      "touch the blue block on its right side\n",
      "Evaluating task  push_block_left\n",
      "push the block towards the left\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1/5 : 59.5% | 2/5 : 24.3% | 3/5 : 8.1% | 4/5 : 0.0% | 5/5 : 0.0% | Average: 0.9 |:  74%|  | 37/50 [15:20<06:45, 31.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating sequence:  ['contact_red_block_right', 'push_into_drawer']\n",
      "Initial state:  {'led': 0, 'lightbulb': 0, 'slider': 'left', 'drawer': 'open', 'red_block': 'table', 'blue_block': 'table', 'pink_block': 'slider_left', 'grasped': 0, 'contact': 0, 'red_block_lifted': 0, 'blue_block_lifted': 0, 'pink_block_lifted': 0}\n",
      "Evaluating task  contact_red_block_right\n",
      "touch the red block on its right side\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1/5 : 57.9% | 2/5 : 23.7% | 3/5 : 7.9% | 4/5 : 0.0% | 5/5 : 0.0% | Average: 0.9 |:  76%|  | 38/50 [15:43<05:47, 28.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating sequence:  ['grasp_blue_block', 'lift_grasped_block', 'place_grasped_block_over_drawer', 'ungrasp_block']\n",
      "Initial state:  {'led': 0, 'lightbulb': 0, 'slider': 'left', 'drawer': 'open', 'red_block': 'table', 'blue_block': 'slider_right', 'pink_block': 'table', 'grasped': 0, 'contact': 0, 'red_block_lifted': 0, 'blue_block_lifted': 0, 'pink_block_lifted': 0}\n",
      "Evaluating task  grasp_blue_block\n",
      "grasp the blue block\n",
      "Evaluating task  lift_grasped_block\n",
      "lift the grasped block\n",
      "Evaluating task  place_grasped_block_over_drawer\n",
      "place the grasped block over the drawer\n",
      "Evaluating task  ungrasp_block\n",
      "ungrasp the block\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.24641285849208233\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.24851508198298983\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.24984228688560967\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.2503087179639859\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.2501661446412091\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.2500932333059605\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.2500558832093306\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.2500367327856443\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.2500269092206423\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.2500217258556495\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.2500188514088356\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.2500173332943055\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.25001623192291195\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.2500143612391313\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.2500131530862386\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.25001233518235655\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.25001242934818624\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.2500130071743908\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.25001362897447366\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.2500143372183132\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.25001445864928784\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.25001511117832764\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.2500152604485854\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.2500150494940217\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.25001469983171276\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.25001454781208293\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.2500144466778044\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.2500143943184534\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.2500143676598825\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.25001435382316084\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.25001434671987216\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.2500143430742641\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.2500143448971728\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.2500143417343502\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.25001434010796564\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.25001433927142974\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.25001433884235424\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.2500143386512384\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.25001433852893706\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.25001433846128457\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.25001433842647564\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.2500143384093903\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.2500143383998205\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.2500143383951067\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.25000208186693124\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.25000825934313925\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.2500136269973737\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.2500142088247815\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.25001427853891345\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.2500142977451866\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.25001430760220894\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.2500143126610231\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.2500143152573074\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.2500143165897752\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.2500143172736285\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.25001431762460063\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.25001431780473177\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.2500150007096204\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.25001563620870676\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.25001606637954055\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.25001639074239895\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.2500166608010811\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.2500168521507733\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.2500170975516847\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.25001673324692253\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.2500162291135276\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.2500159703779062\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.25001583758815965\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.2500157694382798\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.25001573446851777\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.2500157165213545\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.250015707310514\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.2500157025833201\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.25001570116377897\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.25001569940537494\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.2500156985029124\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.2500156990398431\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.2500157011119592\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.25001570202463547\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.2500156996870483\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.25001570100228215\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.2500157019644742\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.2500157025359905\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.25001570303440573\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.25001569118010936\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.2500156880380765\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.25001464014932323\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.2500154310057206\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.2500155468441237\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.2500420059233926\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.2500234185513685\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.2500178805880561\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.2500157653088273\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.2500161713535279\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.2500104095785384\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.2500065046449801\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.2500044681971109\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.2500048557269314\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.25001017263832254\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.25001290149019\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.25001430205971614\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.25001502085911637\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.2500153897606888\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.2500155790865646\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.2500156738416982\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.2500157241686615\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.2500175726779288\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.25001645966905334\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.2500160187106447\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.25001585597181547\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.25001579935379764\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.25001578175237393\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.25001577748166337\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.2500157763058866\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.2500157750035943\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.25001577470738756\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.2500140446633282\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.2500151700860785\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.250016099243803\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.25001594282481393\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.2500158625392082\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.2500158212998439\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.2500158001533486\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.2500157892756565\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.2500157836737401\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.25001578074482134\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.25001577916735435\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.25001577834016514\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.2500157779048397\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.2500156983874978\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.2500175823542925\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.2500172250257874\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.25001632748822183\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.2500162245174538\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.25001611802696433\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.25001600974421195\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.25001590054494977\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.250015790877035\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.2500156809694982\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.2500155709394619\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.2500154608468204\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.25001535072220216\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.2500153870517823\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.25001535583649553\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.2500153491139454\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.2500154798552097\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.25001542349027595\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.250015388574996\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.25001535824731114\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.2500153426138264\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.2500153346225785\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.2500153304757697\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.2500153282163388\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.25001532721627745\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.25001532669947896\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.25001532643219854\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.25001532628769557\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.25001532623018113\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.25001532586611525\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.25001532594310844\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.25001532598820275\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.2500153256557218\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.25001532582848174\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.25001532591709363\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.2500153259614308\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.2500153259766208\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.25001532598646387\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.2500153259890626\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.2500153259829129\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.25001532596311465\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.2500153259574922\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.25001532595317716\n",
      "The grasped block is  block_blue\n",
      "block_blue moved xy too much! by 0.25001587094095923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1/5 : 59.0% | 2/5 : 25.6% | 3/5 : 10.3% | 4/5 : 0.0% | 5/5 : 0.0% | Average: 0.9 |:  78%|  | 39/50 [17:03<08:06, 44.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating sequence:  ['grasp_blue_block', 'lift_grasped_block']\n",
      "Initial state:  {'led': 0, 'lightbulb': 0, 'slider': 'left', 'drawer': 'open', 'red_block': 'table', 'blue_block': 'slider_right', 'pink_block': 'slider_left', 'grasped': 0, 'contact': 0, 'red_block_lifted': 0, 'blue_block_lifted': 0, 'pink_block_lifted': 0}\n",
      "Evaluating task  grasp_blue_block\n",
      "grasp the blue block\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1/5 : 57.5% | 2/5 : 25.0% | 3/5 : 10.0% | 4/5 : 0.0% | 5/5 : 0.0% | Average: 0.9 |:  80%|  | 40/50 [17:28<06:25, 38.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating sequence:  ['grasp_red_block', 'rotate_grasped_block_left', 'ungrasp_block']\n",
      "Initial state:  {'led': 0, 'lightbulb': 0, 'slider': 'left', 'drawer': 'open', 'red_block': 'table', 'blue_block': 'slider_left', 'pink_block': 'table', 'grasped': 0, 'contact': 0, 'red_block_lifted': 0, 'blue_block_lifted': 0, 'pink_block_lifted': 0}\n",
      "Evaluating task  grasp_red_block\n",
      "grasp the red block\n",
      "Evaluating task  rotate_grasped_block_left\n",
      "rotate the grasped block 90 degrees to the left\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1/5 : 58.5% | 2/5 : 24.4% | 3/5 : 9.8% | 4/5 : 0.0% | 5/5 : 0.0% | Average: 0.9 |:  82%| | 41/50 [17:56<05:16, 35.21s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating sequence:  ['contact_red_block_left', 'push_block_right']\n",
      "Initial state:  {'led': 0, 'lightbulb': 0, 'slider': 'left', 'drawer': 'open', 'red_block': 'table', 'blue_block': 'slider_left', 'pink_block': 'slider_right', 'grasped': 0, 'contact': 0, 'red_block_lifted': 0, 'blue_block_lifted': 0, 'pink_block_lifted': 0}\n",
      "Evaluating task  contact_red_block_left\n",
      "touch the red block on its left side\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1/5 : 57.1% | 2/5 : 23.8% | 3/5 : 9.5% | 4/5 : 0.0% | 5/5 : 0.0% | Average: 0.9 |:  84%| | 42/50 [18:26<04:28, 33.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating sequence:  ['grasp_drawer', 'close_drawer', 'ungrasp_drawer']\n",
      "Initial state:  {'led': 0, 'lightbulb': 0, 'slider': 'left', 'drawer': 'open', 'red_block': 'slider_right', 'blue_block': 'table', 'pink_block': 'table', 'grasped': 0, 'contact': 0, 'red_block_lifted': 0, 'blue_block_lifted': 0, 'pink_block_lifted': 0}\n",
      "Evaluating task  grasp_drawer\n",
      "grasp the drawer handle\n",
      "Evaluating task  close_drawer\n",
      "push the handle \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1/5 : 58.1% | 2/5 : 23.3% | 3/5 : 9.3% | 4/5 : 0.0% | 5/5 : 0.0% | Average: 0.9 |:  86%| | 43/50 [19:08<04:13, 36.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating sequence:  ['grasp_blue_block', 'lift_grasped_block', 'place_grasped_block_over_slider', 'ungrasp_block']\n",
      "Initial state:  {'led': 0, 'lightbulb': 0, 'slider': 'left', 'drawer': 'open', 'red_block': 'slider_right', 'blue_block': 'table', 'pink_block': 'slider_left', 'grasped': 0, 'contact': 0, 'red_block_lifted': 0, 'blue_block_lifted': 0, 'pink_block_lifted': 0}\n",
      "Evaluating task  grasp_blue_block\n",
      "grasp the blue block\n",
      "Evaluating task  lift_grasped_block\n",
      "lift the grasped block\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1/5 : 59.1% | 2/5 : 22.7% | 3/5 : 9.1% | 4/5 : 0.0% | 5/5 : 0.0% | Average: 0.9 |:  88%| | 44/50 [19:37<03:23, 33.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating sequence:  ['grasp_red_block', 'lift_grasped_block', 'place_grasped_block_over_pink_block', 'ungrasp_block']\n",
      "Initial state:  {'led': 0, 'lightbulb': 0, 'slider': 'left', 'drawer': 'open', 'red_block': 'slider_right', 'blue_block': 'slider_left', 'pink_block': 'table', 'grasped': 0, 'contact': 0, 'red_block_lifted': 0, 'blue_block_lifted': 0, 'pink_block_lifted': 0}\n",
      "Evaluating task  grasp_red_block\n",
      "grasp the red block\n",
      "Evaluating task  lift_grasped_block\n",
      "lift the grasped block\n",
      "Evaluating task  place_grasped_block_over_pink_block\n",
      "place the grasped block over the pink block\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1/5 : 60.0% | 2/5 : 24.4% | 3/5 : 8.9% | 4/5 : 0.0% | 5/5 : 0.0% | Average: 0.9 |:  90%| | 45/50 [20:24<03:10, 38.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating sequence:  ['grasp_pink_block', 'lift_grasped_block']\n",
      "Initial state:  {'led': 0, 'lightbulb': 0, 'slider': 'left', 'drawer': 'open', 'red_block': 'slider_left', 'blue_block': 'table', 'pink_block': 'table', 'grasped': 0, 'contact': 0, 'red_block_lifted': 0, 'blue_block_lifted': 0, 'pink_block_lifted': 0}\n",
      "Evaluating task  grasp_pink_block\n",
      "grasp the pink block\n",
      "Evaluating task  lift_grasped_block\n",
      "lift the grasped block\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1/5 : 60.9% | 2/5 : 26.1% | 3/5 : 8.7% | 4/5 : 0.0% | 5/5 : 0.0% | Average: 1.0 |:  92%|| 46/50 [20:33<01:56, 29.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating sequence:  ['grasp_blue_block', 'lift_grasped_block']\n",
      "Initial state:  {'led': 0, 'lightbulb': 0, 'slider': 'left', 'drawer': 'open', 'red_block': 'slider_left', 'blue_block': 'table', 'pink_block': 'slider_right', 'grasped': 0, 'contact': 0, 'red_block_lifted': 0, 'blue_block_lifted': 0, 'pink_block_lifted': 0}\n",
      "Evaluating task  grasp_blue_block\n",
      "grasp the blue block\n",
      "Evaluating task  lift_grasped_block\n",
      "lift the grasped block\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1/5 : 61.7% | 2/5 : 25.5% | 3/5 : 8.5% | 4/5 : 0.0% | 5/5 : 0.0% | Average: 1.0 |:  94%|| 47/50 [20:58<01:24, 28.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating sequence:  ['contact_pink_block_left', 'push_block_right']\n",
      "Initial state:  {'led': 0, 'lightbulb': 0, 'slider': 'left', 'drawer': 'open', 'red_block': 'slider_left', 'blue_block': 'slider_right', 'pink_block': 'table', 'grasped': 0, 'contact': 0, 'red_block_lifted': 0, 'blue_block_lifted': 0, 'pink_block_lifted': 0}\n",
      "Evaluating task  contact_pink_block_left\n",
      "touch the pink block on its left side\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1/5 : 60.4% | 2/5 : 25.0% | 3/5 : 8.3% | 4/5 : 0.0% | 5/5 : 0.0% | Average: 0.9 |:  96%|| 48/50 [21:26<00:56, 28.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating sequence:  ['grasp_slider', 'move_slider_left', 'ungrasp_slider']\n",
      "Initial state:  {'led': 0, 'lightbulb': 1, 'slider': 'right', 'drawer': 'closed', 'red_block': 'table', 'blue_block': 'table', 'pink_block': 'slider_right', 'grasped': 0, 'contact': 0, 'red_block_lifted': 0, 'blue_block_lifted': 0, 'pink_block_lifted': 0}\n",
      "Evaluating task  grasp_slider\n",
      "grasp the slider handle\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1/5 : 59.2% | 2/5 : 24.5% | 3/5 : 8.2% | 4/5 : 0.0% | 5/5 : 0.0% | Average: 0.9 |:  98%|| 49/50 [21:51<00:26, 26.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating sequence:  ['grasp_pink_block', 'lift_grasped_block', 'place_grasped_block_over_red_block', 'ungrasp_block']\n",
      "Initial state:  {'led': 0, 'lightbulb': 1, 'slider': 'right', 'drawer': 'closed', 'red_block': 'table', 'blue_block': 'table', 'pink_block': 'slider_left', 'grasped': 0, 'contact': 0, 'red_block_lifted': 0, 'blue_block_lifted': 0, 'pink_block_lifted': 0}\n",
      "Evaluating task  grasp_pink_block\n",
      "grasp the pink block\n",
      "Evaluating task  lift_grasped_block\n",
      "lift the grasped block\n",
      "Evaluating task  place_grasped_block_over_red_block\n",
      "place the grasped block over the red block\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1/5 : 60.0% | 2/5 : 26.0% | 3/5 : 8.0% | 4/5 : 0.0% | 5/5 : 0.0% | Average: 0.9 |: 100%|| 50/50 [22:39<00:00, 27.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE SAVE LOCATION /ubc/cs/research/nlp/grigorii/projects/openvla/experiments/robot/calvin/debug_videos/experiment_2_20250207_133938_small_2/_long_horizon_sequence_0_0.gif\n",
      "THE SAVE LOCATION /ubc/cs/research/nlp/grigorii/projects/openvla/experiments/robot/calvin/debug_videos/experiment_2_20250207_133938_small_2/_long_horizon_sequence_1_0.gif\n",
      "THE SAVE LOCATION /ubc/cs/research/nlp/grigorii/projects/openvla/experiments/robot/calvin/debug_videos/experiment_2_20250207_133938_small_2/_long_horizon_sequence_2_0.gif\n",
      "THE SAVE LOCATION /ubc/cs/research/nlp/grigorii/projects/openvla/experiments/robot/calvin/debug_videos/experiment_2_20250207_133938_small_2/_long_horizon_sequence_3_0.gif\n",
      "THE SAVE LOCATION /ubc/cs/research/nlp/grigorii/projects/openvla/experiments/robot/calvin/debug_videos/experiment_2_20250207_133938_small_2/_long_horizon_sequence_4_0.gif\n",
      "THE SAVE LOCATION /ubc/cs/research/nlp/grigorii/projects/openvla/experiments/robot/calvin/debug_videos/experiment_2_20250207_133938_small_2/_long_horizon_sequence_5_0.gif\n",
      "THE SAVE LOCATION /ubc/cs/research/nlp/grigorii/projects/openvla/experiments/robot/calvin/debug_videos/experiment_2_20250207_133938_small_2/_long_horizon_sequence_6_0.gif\n",
      "THE SAVE LOCATION /ubc/cs/research/nlp/grigorii/projects/openvla/experiments/robot/calvin/debug_videos/experiment_2_20250207_133938_small_2/_long_horizon_sequence_7_0.gif\n",
      "THE SAVE LOCATION /ubc/cs/research/nlp/grigorii/projects/openvla/experiments/robot/calvin/debug_videos/experiment_2_20250207_133938_small_2/_long_horizon_sequence_8_0.gif\n",
      "THE SAVE LOCATION /ubc/cs/research/nlp/grigorii/projects/openvla/experiments/robot/calvin/debug_videos/experiment_2_20250207_133938_small_2/_long_horizon_sequence_9_0.gif\n",
      "THE SAVE LOCATION /ubc/cs/research/nlp/grigorii/projects/openvla/experiments/robot/calvin/debug_videos/experiment_2_20250207_133938_small_2/_long_horizon_sequence_10_0.gif\n",
      "THE SAVE LOCATION /ubc/cs/research/nlp/grigorii/projects/openvla/experiments/robot/calvin/debug_videos/experiment_2_20250207_133938_small_2/_long_horizon_sequence_11_0.gif\n",
      "THE SAVE LOCATION /ubc/cs/research/nlp/grigorii/projects/openvla/experiments/robot/calvin/debug_videos/experiment_2_20250207_133938_small_2/_long_horizon_sequence_12_0.gif\n",
      "THE SAVE LOCATION /ubc/cs/research/nlp/grigorii/projects/openvla/experiments/robot/calvin/debug_videos/experiment_2_20250207_133938_small_2/_long_horizon_sequence_13_0.gif\n",
      "THE SAVE LOCATION /ubc/cs/research/nlp/grigorii/projects/openvla/experiments/robot/calvin/debug_videos/experiment_2_20250207_133938_small_2/_long_horizon_sequence_14_0.gif\n",
      "THE SAVE LOCATION /ubc/cs/research/nlp/grigorii/projects/openvla/experiments/robot/calvin/debug_videos/experiment_2_20250207_133938_small_2/_long_horizon_sequence_15_0.gif\n",
      "THE SAVE LOCATION /ubc/cs/research/nlp/grigorii/projects/openvla/experiments/robot/calvin/debug_videos/experiment_2_20250207_133938_small_2/_long_horizon_sequence_16_0.gif\n",
      "THE SAVE LOCATION /ubc/cs/research/nlp/grigorii/projects/openvla/experiments/robot/calvin/debug_videos/experiment_2_20250207_133938_small_2/_long_horizon_sequence_17_0.gif\n",
      "THE SAVE LOCATION /ubc/cs/research/nlp/grigorii/projects/openvla/experiments/robot/calvin/debug_videos/experiment_2_20250207_133938_small_2/_long_horizon_sequence_18_0.gif\n",
      "THE SAVE LOCATION /ubc/cs/research/nlp/grigorii/projects/openvla/experiments/robot/calvin/debug_videos/experiment_2_20250207_133938_small_2/_long_horizon_sequence_19_0.gif\n",
      "THE SAVE LOCATION /ubc/cs/research/nlp/grigorii/projects/openvla/experiments/robot/calvin/debug_videos/experiment_2_20250207_133938_small_2/_long_horizon_sequence_20_0.gif\n",
      "THE SAVE LOCATION /ubc/cs/research/nlp/grigorii/projects/openvla/experiments/robot/calvin/debug_videos/experiment_2_20250207_133938_small_2/_long_horizon_sequence_21_0.gif\n",
      "THE SAVE LOCATION /ubc/cs/research/nlp/grigorii/projects/openvla/experiments/robot/calvin/debug_videos/experiment_2_20250207_133938_small_2/_long_horizon_sequence_22_0.gif\n",
      "THE SAVE LOCATION /ubc/cs/research/nlp/grigorii/projects/openvla/experiments/robot/calvin/debug_videos/experiment_2_20250207_133938_small_2/_long_horizon_sequence_23_0.gif\n",
      "THE SAVE LOCATION /ubc/cs/research/nlp/grigorii/projects/openvla/experiments/robot/calvin/debug_videos/experiment_2_20250207_133938_small_2/_long_horizon_sequence_24_0.gif\n",
      "THE SAVE LOCATION /ubc/cs/research/nlp/grigorii/projects/openvla/experiments/robot/calvin/debug_videos/experiment_2_20250207_133938_small_2/_long_horizon_sequence_25_0.gif\n",
      "THE SAVE LOCATION /ubc/cs/research/nlp/grigorii/projects/openvla/experiments/robot/calvin/debug_videos/experiment_2_20250207_133938_small_2/_long_horizon_sequence_26_0.gif\n",
      "THE SAVE LOCATION /ubc/cs/research/nlp/grigorii/projects/openvla/experiments/robot/calvin/debug_videos/experiment_2_20250207_133938_small_2/_long_horizon_sequence_27_0.gif\n",
      "THE SAVE LOCATION /ubc/cs/research/nlp/grigorii/projects/openvla/experiments/robot/calvin/debug_videos/experiment_2_20250207_133938_small_2/_long_horizon_sequence_28_0.gif\n",
      "THE SAVE LOCATION /ubc/cs/research/nlp/grigorii/projects/openvla/experiments/robot/calvin/debug_videos/experiment_2_20250207_133938_small_2/_long_horizon_sequence_29_0.gif\n",
      "THE SAVE LOCATION /ubc/cs/research/nlp/grigorii/projects/openvla/experiments/robot/calvin/debug_videos/experiment_2_20250207_133938_small_2/_long_horizon_sequence_30_0.gif\n",
      "THE SAVE LOCATION /ubc/cs/research/nlp/grigorii/projects/openvla/experiments/robot/calvin/debug_videos/experiment_2_20250207_133938_small_2/_long_horizon_sequence_31_0.gif\n",
      "THE SAVE LOCATION /ubc/cs/research/nlp/grigorii/projects/openvla/experiments/robot/calvin/debug_videos/experiment_2_20250207_133938_small_2/_long_horizon_sequence_32_0.gif\n",
      "THE SAVE LOCATION /ubc/cs/research/nlp/grigorii/projects/openvla/experiments/robot/calvin/debug_videos/experiment_2_20250207_133938_small_2/_long_horizon_sequence_33_0.gif\n",
      "THE SAVE LOCATION /ubc/cs/research/nlp/grigorii/projects/openvla/experiments/robot/calvin/debug_videos/experiment_2_20250207_133938_small_2/_long_horizon_sequence_34_0.gif\n",
      "THE SAVE LOCATION /ubc/cs/research/nlp/grigorii/projects/openvla/experiments/robot/calvin/debug_videos/experiment_2_20250207_133938_small_2/_long_horizon_sequence_35_0.gif\n",
      "THE SAVE LOCATION /ubc/cs/research/nlp/grigorii/projects/openvla/experiments/robot/calvin/debug_videos/experiment_2_20250207_133938_small_2/_long_horizon_sequence_36_0.gif\n",
      "THE SAVE LOCATION /ubc/cs/research/nlp/grigorii/projects/openvla/experiments/robot/calvin/debug_videos/experiment_2_20250207_133938_small_2/_long_horizon_sequence_37_0.gif\n",
      "THE SAVE LOCATION /ubc/cs/research/nlp/grigorii/projects/openvla/experiments/robot/calvin/debug_videos/experiment_2_20250207_133938_small_2/_long_horizon_sequence_38_0.gif\n",
      "THE SAVE LOCATION /ubc/cs/research/nlp/grigorii/projects/openvla/experiments/robot/calvin/debug_videos/experiment_2_20250207_133938_small_2/_long_horizon_sequence_39_0.gif\n",
      "THE SAVE LOCATION /ubc/cs/research/nlp/grigorii/projects/openvla/experiments/robot/calvin/debug_videos/experiment_2_20250207_133938_small_2/_long_horizon_sequence_40_0.gif\n",
      "THE SAVE LOCATION /ubc/cs/research/nlp/grigorii/projects/openvla/experiments/robot/calvin/debug_videos/experiment_2_20250207_133938_small_2/_long_horizon_sequence_41_0.gif\n",
      "THE SAVE LOCATION /ubc/cs/research/nlp/grigorii/projects/openvla/experiments/robot/calvin/debug_videos/experiment_2_20250207_133938_small_2/_long_horizon_sequence_42_0.gif\n",
      "THE SAVE LOCATION /ubc/cs/research/nlp/grigorii/projects/openvla/experiments/robot/calvin/debug_videos/experiment_2_20250207_133938_small_2/_long_horizon_sequence_43_0.gif\n",
      "THE SAVE LOCATION /ubc/cs/research/nlp/grigorii/projects/openvla/experiments/robot/calvin/debug_videos/experiment_2_20250207_133938_small_2/_long_horizon_sequence_44_0.gif\n",
      "THE SAVE LOCATION /ubc/cs/research/nlp/grigorii/projects/openvla/experiments/robot/calvin/debug_videos/experiment_2_20250207_133938_small_2/_long_horizon_sequence_45_0.gif\n",
      "THE SAVE LOCATION /ubc/cs/research/nlp/grigorii/projects/openvla/experiments/robot/calvin/debug_videos/experiment_2_20250207_133938_small_2/_long_horizon_sequence_46_0.gif\n",
      "THE SAVE LOCATION /ubc/cs/research/nlp/grigorii/projects/openvla/experiments/robot/calvin/debug_videos/experiment_2_20250207_133938_small_2/_long_horizon_sequence_47_0.gif\n",
      "THE SAVE LOCATION /ubc/cs/research/nlp/grigorii/projects/openvla/experiments/robot/calvin/debug_videos/experiment_2_20250207_133938_small_2/_long_horizon_sequence_48_0.gif\n",
      "THE SAVE LOCATION /ubc/cs/research/nlp/grigorii/projects/openvla/experiments/robot/calvin/debug_videos/experiment_2_20250207_133938_small_2/_long_horizon_sequence_49_0.gif\n",
      "High_started:  Counter({'place_in_slider': 5, 'move_slider_left': 4, 'stack_block': 4, 'lift_blue_block_table': 3, 'push_pink_block_right': 3, 'place_in_drawer': 3, 'lift_pink_block_table': 3, 'push_red_block_right': 3, 'lift_red_block_drawer': 2, 'lift_blue_block_slider': 2, 'push_blue_block_left': 2, 'push_blue_block_right': 2, 'lift_pink_block_drawer': 2, 'move_slider_right': 2, 'rotate_red_block_right': 1, 'rotate_blue_block_left': 1, 'push_pink_block_left': 1, 'lift_red_block_table': 1, 'open_drawer': 1, 'rotate_blue_block_right': 1, 'lift_pink_block_slider': 1, 'push_into_drawer': 1, 'rotate_red_block_left': 1, 'close_drawer': 1})\n",
      "High_completed:  Counter({'lift_blue_block_table': 2, 'lift_pink_block_drawer': 2, 'rotate_blue_block_left': 1, 'lift_red_block_drawer': 1, 'rotate_blue_block_right': 1, 'lift_pink_block_slider': 1})\n",
      "Low_started:  Counter({'lift_grasped_block': 22, 'grasp_blue_block': 13, 'grasp_pink_block': 9, 'grasp_red_block': 8, 'grasp_slider': 6, 'ungrasp_block': 4, 'contact_pink_block_left': 3, 'contact_red_block_left': 3, 'rotate_grasped_block_left': 2, 'contact_blue_block_right': 2, 'push_block_left': 2, 'contact_blue_block_left': 2, 'grasp_drawer': 2, 'place_grasped_block_over_slider': 1, 'move_slider_left': 1, 'ungrasp_slider': 1, 'contact_pink_block_right': 1, 'open_drawer': 1, 'rotate_grasped_block_right': 1, 'contact_red_block_right': 1, 'place_grasped_block_over_drawer': 1, 'close_drawer': 1, 'place_grasped_block_over_pink_block': 1, 'place_grasped_block_over_red_block': 1})\n",
      "Low_completed:  Counter({'grasp_blue_block': 11, 'lift_grasped_block': 10, 'grasp_pink_block': 8, 'grasp_red_block': 6, 'ungrasp_block': 2, 'contact_blue_block_right': 2, 'grasp_drawer': 2, 'place_grasped_block_over_slider': 1, 'rotate_grasped_block_left': 1, 'grasp_slider': 1, 'move_slider_left': 1, 'rotate_grasped_block_right': 1, 'place_grasped_block_over_drawer': 1})\n"
     ]
    }
   ],
   "source": [
    "calvin_cfg.ep_len = 200\n",
    "calvin_cfg.num_sequences = 50\n",
    "calvin_cfg.num_videos = 50\n",
    "\n",
    "results, average_rate, success_rates, counters = evaluate_policy(model, env, calvin_cfg, \n",
    "                processor, \n",
    "                num_videos=calvin_cfg.num_videos, \n",
    "                save_dir=video_save_dir\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "infos = env.get_info()\n",
    "print(infos['scene_info']['movable_objects']['block_red']['uid'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Low-level tasks conjunctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "def evaluate_policy(model, env, cfg, processor, num_videos=0, save_dir=None):\n",
    "    task_oracle_high_level = hydra.utils.instantiate(cfg.tasks)\n",
    "\n",
    "    val_annotations_low_level = cfg.low_level_annotations\n",
    "\n",
    "    # video stuff\n",
    "    if num_videos > 0:\n",
    "        rollout_video = RolloutVideo(\n",
    "            logger=logger,\n",
    "            empty_cache=False,\n",
    "            log_to_file=True,\n",
    "            save_dir=save_dir,\n",
    "            resolution_scale=1,\n",
    "        )\n",
    "    else:\n",
    "        rollout_video = None\n",
    "\n",
    "    eval_sequences = get_low_level_sequences(cfg.num_sequences)\n",
    "    \n",
    "    results = []\n",
    "    plans = defaultdict(list)\n",
    "    \n",
    "    high_level_started = Counter()\n",
    "    high_level_completed = Counter()\n",
    "    counters = {\n",
    "        'high_level_started': high_level_started,\n",
    "        'high_level_completed': high_level_completed,\n",
    "    }\n",
    "    \n",
    "    if not cfg.debug:\n",
    "        eval_sequences = tqdm(eval_sequences, position=0, leave=True)\n",
    "    for i, (initial_state, eval_sequence) in enumerate(eval_sequences):\n",
    "        record = i < num_videos\n",
    "        #eval_sequence = [['contact_pink_block_left', 'push_pink_block_right']]\n",
    "        #initial_state = {'led': 0, 'lightbulb': 0, 'slider': 'right', 'drawer': 'closed', 'red_block': 'table', 'blue_block': 'slider_left', 'pink_block': 'table', 'grasped': 0, 'contact': 0}\n",
    "        high_level_task, eval_seq = eval_sequence\n",
    "        high_level_started[high_level_task] += 1\n",
    "        result = evaluate_sequence(\n",
    "            env, model, task_oracle_high_level, initial_state, high_level_task, eval_seq, val_annotations_low_level, cfg, processor, record, rollout_video, i\n",
    "        )\n",
    "\n",
    "        results.append(result)\n",
    "        high_level_completed[high_level_task] += result\n",
    "\n",
    "        if record:\n",
    "            rollout_video.write_to_tmp()\n",
    "        if not cfg.debug:\n",
    "            success_rates = count_success(results)\n",
    "            average_rate = sum(success_rates) / len(success_rates) * 5\n",
    "            description = \" \".join([f\"{i + 1}/5 : {v * 100:.1f}% |\" for i, v in enumerate(success_rates)])\n",
    "            description += f\" Average: {average_rate:.1f} |\"\n",
    "            eval_sequences.set_description(description)\n",
    "\n",
    "    if num_videos > 0:\n",
    "        # log rollout videos\n",
    "        rollout_video._log_videos_to_file(0, save_as_video=False)\n",
    "    return results, average_rate, success_rates, counters\n",
    "\n",
    "\n",
    "def evaluate_sequence(\n",
    "    env, model, task_checker, initial_state, high_level_task, eval_sequence, val_annotations, cfg, processor, record, rollout_video, i\n",
    "):\n",
    "    robot_obs, scene_obs = get_env_state_for_initial_condition(initial_state)\n",
    "    env.reset(robot_obs=robot_obs, scene_obs=scene_obs)\n",
    "    if record:\n",
    "        caption = \" | \".join(eval_sequence)\n",
    "        rollout_video.new_video(tag=get_video_tag(i), caption=caption)\n",
    "\n",
    "    if cfg.debug:\n",
    "        time.sleep(1)\n",
    "        print()\n",
    "        print()\n",
    "        print(f\"Evaluating sequence: {' -> '.join(eval_sequence)}\")\n",
    "        print(\"Subtask: \", end=\"\")\n",
    "\n",
    "    print(\"Evaluating sequence: \", eval_sequence)\n",
    "    print('Initial state: ', initial_state)\n",
    "\n",
    "    full_instruction = \", then \".join([val_annotations[subtask][0] for subtask in eval_sequence])\n",
    "    print(\"The instruction \", full_instruction)\n",
    "\n",
    "    if record:\n",
    "        rollout_video.new_subtask()\n",
    "\n",
    "    success = rollout(env, model, task_checker, cfg, high_level_task, full_instruction, processor=processor, record=record, rollout_video=rollout_video)\n",
    "\n",
    "    if record:\n",
    "        rollout_video.draw_outcome(success)\n",
    "\n",
    "    return int(success)\n",
    "\n",
    "\n",
    "def rollout(env, model, task_oracle, cfg, high_level_task, lang_annotation, processor, record=False, rollout_video=None):\n",
    "    if cfg.debug:\n",
    "        print(f\"{subtask} \", end=\"\")\n",
    "        time.sleep(0.5)\n",
    "    obs = env.get_obs()\n",
    "    # get lang annotation for subtask\n",
    "    #print(\"Instruction: \", lang_annotation)\n",
    "    #print(\"Setting a different instruction!\")\n",
    "    #lang_annotation = 'put the blue block on top of red block'\n",
    "    # get language goal embedding\n",
    "    if processor == None:\n",
    "        tasks = model.create_tasks(texts=[lang_annotation])\n",
    "        from octo.utils.train_callbacks import supply_rng\n",
    "        policy_fn = supply_rng(\n",
    "                partial(\n",
    "                    model.sample_actions,\n",
    "                    unnormalization_statistics=model.dataset_statistics[\"action\"],\n",
    "                ),\n",
    "            )\n",
    "        window_size = 4\n",
    "        act_step = 4\n",
    "\n",
    "    #model.reset()\n",
    "    start_info = env.get_info()\n",
    "    past_obs = None\n",
    "\n",
    "    for step in range(cfg.ep_len):\n",
    "        if processor == None:\n",
    "            if act_step > 0 and act_step % window_size == 0:\n",
    "                act_step = 0\n",
    "                \n",
    "                static_2 = resize_image(obs['rgb_obs']['rgb_static'], (256, 256), primary_octo=True)\n",
    "                #gripper_2 = resize_image(obs['rgb_obs']['rgb_gripper'], (128, 128))\n",
    "                if past_obs:\n",
    "                    static_1 = resize_image(past_obs['rgb_obs']['rgb_static'], (256, 256), primary_octo=True)\n",
    "                    #gripper_1 = resize_image(past_obs['rgb_obs']['rgb_gripper'], (128, 128))\n",
    "                    image_primary = np.stack([static_1, static_2])\n",
    "                    #image_wrist = np.stack([gripper_1, gripper_2])\n",
    "                    timestep_pad_mask = np.array([[True, True]])\n",
    "\n",
    "                else:\n",
    "                    image_primary = np.stack([np.zeros((256, 256, 3)), static_2])\n",
    "                    #image_wrist = np.stack([np.zeros((128, 128, 3)), gripper_2])\n",
    "                    timestep_pad_mask = np.array([[False, True]])\n",
    "                pad_mask_dict = {\n",
    "                    \"image_primary\": np.array([[True, True]]),\n",
    "                    #\"image_wrist\": np.array([[True, True]]),\n",
    "                    \"timestep\": np.array([[False, False]]),\n",
    "                }\n",
    "                \n",
    "                #image_primary = np.expand_dims(resize_image(obs['rgb_obs']['rgb_static'], (256, 256), primary_octo=True), 0)\n",
    "                #image_wrist = np.expand_dims(resize_image(obs['rgb_obs']['rgb_gripper'], (128, 128)), 0)\n",
    "                #timestep_pad_mask = np.array([[True]])\n",
    "                #pad_mask_dict = {\n",
    "                #    \"image_primary\": np.array([[True]]),\n",
    "                    #\"image_wrist\": np.array([[True]]),\n",
    "                #    \"timestep\": np.array([[True]]),\n",
    "                #}\n",
    "                observation = {\n",
    "                        \"image_primary\": np.expand_dims(image_primary, 0),  # uint8\n",
    "                        #\"image_wrist\": np.expand_dims(image_wrist, 0),      # uint8\n",
    "                        \"timestep_pad_mask\": timestep_pad_mask,\n",
    "                        \"pad_mask_dict\": pad_mask_dict,\n",
    "                        \"timestep\": np.array([[step-1, step]]),\n",
    "                }\n",
    "                act_buffer = policy_fn(observation, tasks)\n",
    "                act_buffer = np.array(act_buffer[0])\n",
    "                action = act_buffer[act_step]\n",
    "            else:\n",
    "                action = act_buffer[act_step]\n",
    "            act_step += 1\n",
    "\n",
    "        else:\n",
    "            \n",
    "            observation = {\n",
    "                'full_image': resize_image(obs['rgb_obs']['rgb_static'], (224, 224))\n",
    "            }\n",
    "\n",
    "            action = get_action(\n",
    "                cfg,\n",
    "                model,\n",
    "                observation,\n",
    "                task_label=lang_annotation,\n",
    "                processor=processor,\n",
    "            )\n",
    "        \n",
    "        # Rescale actions from [0, 1] to [-1, 1] and binarize\n",
    "        action = normalize_gripper_action(action)\n",
    "        past_obs = obs\n",
    "        obs, _, _, current_info = env.step(action)\n",
    "        if record:\n",
    "            # update video\n",
    "            frame_aug = torch.zeros((3, 224, 448))\n",
    "            resize = Resize(224, antialias=True)\n",
    "            frame_aug[:, :, :224] = resize(torch.tensor(obs[\"rgb_obs\"][\"rgb_static\"]).permute(2, 0, 1))\n",
    "            closest_obs = 0\n",
    "            if isinstance(closest_obs, int):\n",
    "                closest_obs = torch.zeros((3, 224, 224))\n",
    "            frame_aug[:, :, 224:] = closest_obs.squeeze()\n",
    "            rollout_video.update(frame_aug.unsqueeze(0).unsqueeze(0))\n",
    "\n",
    "        # check if current step solves a task\n",
    "        current_task_info = task_oracle.get_task_info_for_set(start_info, current_info, {high_level_task})\n",
    "\n",
    "        if len(current_task_info) > 0:\n",
    "            if cfg.debug:\n",
    "                print(colored(\"success\", \"green\"), end=\" \")\n",
    "            if record:\n",
    "                rollout_video.add_language_instruction(lang_annotation)\n",
    "            return True\n",
    "    if cfg.debug:\n",
    "        print(colored(\"fail\", \"red\"), end=\" \")\n",
    "    if record:\n",
    "        rollout_video.add_language_instruction(lang_annotation)\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating sequence:  ['grasp_slider', 'move_slider_left', 'ungrasp_slider']\n",
      "Initial state:  {'led': 0, 'lightbulb': 0, 'slider': 'right', 'drawer': 'closed', 'red_block': 'table', 'blue_block': 'table', 'pink_block': 'slider_right', 'grasped': 0, 'contact': 0, 'red_block_lifted': 0, 'blue_block_lifted': 0, 'pink_block_lifted': 0}\n",
      "The instruction  grasp the slider handle, then move the handle to the left, then ungrasp the slider handle\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">02/11 [07:27:29] </span><span style=\"color: #808000; text-decoration-color: #808000\">WARNING </span> | &gt;&gt; <span style=\"color: #008000; text-decoration-color: #008000\">'observations'</span> is missing items compared to example_batch:         <a href=\"file:///ubc/cs/research/nlp/grigorii/projects/octo_original/octo/octo/model/octo_model.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">octo_model.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///ubc/cs/research/nlp/grigorii/projects/octo_original/octo/octo/model/octo_model.py#533\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">533</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                 </span>         <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'task_completed'</span><span style=\"font-weight: bold\">}</span>                                                      <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                 </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m02/11 [07:27:29]\u001b[0m\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m | >> \u001b[32m'observations'\u001b[0m is missing items compared to example_batch:         \u001b]8;id=919532;file:///ubc/cs/research/nlp/grigorii/projects/octo_original/octo/octo/model/octo_model.py\u001b\\\u001b[2mocto_model.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=817238;file:///ubc/cs/research/nlp/grigorii/projects/octo_original/octo/octo/model/octo_model.py#533\u001b\\\u001b[2m533\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                 \u001b[0m         \u001b[1m{\u001b[0m\u001b[32m'task_completed'\u001b[0m\u001b[1m}\u001b[0m                                                      \u001b[2m                 \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">02/11 [07:27:30] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> | &gt;&gt; No image inputs matching <span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'image_wrist'</span>,<span style=\"font-weight: bold\">)</span> were found.Skipping      <a href=\"file:///ubc/cs/research/nlp/grigorii/projects/octo_original/octo/octo/model/components/tokenizers.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">tokenizers.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///ubc/cs/research/nlp/grigorii/projects/octo_original/octo/octo/model/components/tokenizers.py#110\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">110</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                 </span>         tokenizer entirely.                                                     <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                 </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m02/11 [07:27:30]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m | >> No image inputs matching \u001b[1m(\u001b[0m\u001b[32m'image_wrist'\u001b[0m,\u001b[1m)\u001b[0m were found.Skipping      \u001b]8;id=324518;file:///ubc/cs/research/nlp/grigorii/projects/octo_original/octo/octo/model/components/tokenizers.py\u001b\\\u001b[2mtokenizers.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=125375;file:///ubc/cs/research/nlp/grigorii/projects/octo_original/octo/octo/model/components/tokenizers.py#110\u001b\\\u001b[2m110\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                 \u001b[0m         tokenizer entirely.                                                     \u001b[2m                 \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                 </span><span style=\"color: #808000; text-decoration-color: #808000\">WARNING </span> | &gt;&gt; Skipping observation tokenizer: obs_wrist                         <a href=\"file:///ubc/cs/research/nlp/grigorii/projects/octo_original/octo/octo/model/octo_module.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">octo_module.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///ubc/cs/research/nlp/grigorii/projects/octo_original/octo/octo/model/octo_module.py#195\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">195</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                \u001b[0m\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m | >> Skipping observation tokenizer: obs_wrist                         \u001b]8;id=510127;file:///ubc/cs/research/nlp/grigorii/projects/octo_original/octo/octo/model/octo_module.py\u001b\\\u001b[2mocto_module.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=791585;file:///ubc/cs/research/nlp/grigorii/projects/octo_original/octo/octo/model/octo_module.py#195\u001b\\\u001b[2m195\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                 </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> | &gt;&gt; repeating task tokens at each timestep to perform cross-modal     <a href=\"file:///ubc/cs/research/nlp/grigorii/projects/octo_original/octo/octo/model/octo_module.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">octo_module.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///ubc/cs/research/nlp/grigorii/projects/octo_original/octo/octo/model/octo_module.py#220\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">220</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                 </span>         attention                                                              <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                  </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m | >> repeating task tokens at each timestep to perform cross-modal     \u001b]8;id=352293;file:///ubc/cs/research/nlp/grigorii/projects/octo_original/octo/octo/model/octo_module.py\u001b\\\u001b[2mocto_module.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=520187;file:///ubc/cs/research/nlp/grigorii/projects/octo_original/octo/octo/model/octo_module.py#220\u001b\\\u001b[2m220\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                 \u001b[0m         attention                                                              \u001b[2m                  \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1/5 : 0.0% | 2/5 : 0.0% | 3/5 : 0.0% | 4/5 : 0.0% | 5/5 : 0.0% | Average: 0.0 |:   5%|         | 1/20 [00:26<08:29, 26.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating sequence:  ['grasp_blue_block', 'lift_grasped_block']\n",
      "Initial state:  {'led': 0, 'lightbulb': 0, 'slider': 'right', 'drawer': 'closed', 'red_block': 'table', 'blue_block': 'table', 'pink_block': 'slider_left', 'grasped': 0, 'contact': 0, 'red_block_lifted': 0, 'blue_block_lifted': 0, 'pink_block_lifted': 0}\n",
      "The instruction  grasp the blue block, then lift the grasped block\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1/5 : 50.0% | 2/5 : 0.0% | 3/5 : 0.0% | 4/5 : 0.0% | 5/5 : 0.0% | Average: 0.5 |:  10%|         | 2/20 [00:30<03:53, 12.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating sequence:  ['contact_pink_block_left', 'push_block_right']\n",
      "Initial state:  {'led': 0, 'lightbulb': 0, 'slider': 'right', 'drawer': 'closed', 'red_block': 'table', 'blue_block': 'slider_right', 'pink_block': 'table', 'grasped': 0, 'contact': 0, 'red_block_lifted': 0, 'blue_block_lifted': 0, 'pink_block_lifted': 0}\n",
      "The instruction  touch the pink block on its left side, then push the block towards the right\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1/5 : 33.3% | 2/5 : 0.0% | 3/5 : 0.0% | 4/5 : 0.0% | 5/5 : 0.0% | Average: 0.3 |:  15%|        | 3/20 [00:48<04:26, 15.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating sequence:  ['grasp_red_block', 'lift_grasped_block']\n",
      "Initial state:  {'led': 0, 'lightbulb': 0, 'slider': 'right', 'drawer': 'closed', 'red_block': 'table', 'blue_block': 'slider_right', 'pink_block': 'slider_left', 'grasped': 0, 'contact': 0, 'red_block_lifted': 0, 'blue_block_lifted': 0, 'pink_block_lifted': 0}\n",
      "The instruction  grasp the red block, then lift the grasped block\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1/5 : 25.0% | 2/5 : 0.0% | 3/5 : 0.0% | 4/5 : 0.0% | 5/5 : 0.0% | Average: 0.2 |:  20%|        | 4/20 [01:08<04:35, 17.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating sequence:  ['grasp_red_block', 'lift_grasped_block', 'place_grasped_block_over_slider', 'ungrasp_block']\n",
      "Initial state:  {'led': 0, 'lightbulb': 0, 'slider': 'right', 'drawer': 'closed', 'red_block': 'table', 'blue_block': 'slider_left', 'pink_block': 'table', 'grasped': 0, 'contact': 0, 'red_block_lifted': 0, 'blue_block_lifted': 0, 'pink_block_lifted': 0}\n",
      "The instruction  grasp the red block, then lift the grasped block, then place the grasped block over the sliding cabinet, then ungrasp the block\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1/5 : 20.0% | 2/5 : 0.0% | 3/5 : 0.0% | 4/5 : 0.0% | 5/5 : 0.0% | Average: 0.2 |:  25%|       | 5/20 [01:23<04:04, 16.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating sequence:  ['grasp_red_block', 'rotate_grasped_block_right', 'ungrasp_block']\n",
      "Initial state:  {'led': 0, 'lightbulb': 0, 'slider': 'right', 'drawer': 'closed', 'red_block': 'table', 'blue_block': 'slider_left', 'pink_block': 'slider_right', 'grasped': 0, 'contact': 0, 'red_block_lifted': 0, 'blue_block_lifted': 0, 'pink_block_lifted': 0}\n",
      "The instruction  grasp the red block, then rotate the grasped block 90 degrees to the right, then ungrasp the block\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1/5 : 16.7% | 2/5 : 0.0% | 3/5 : 0.0% | 4/5 : 0.0% | 5/5 : 0.0% | Average: 0.2 |:  30%|       | 6/20 [01:37<03:40, 15.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating sequence:  ['grasp_blue_block', 'lift_grasped_block']\n",
      "Initial state:  {'led': 0, 'lightbulb': 0, 'slider': 'right', 'drawer': 'closed', 'red_block': 'slider_right', 'blue_block': 'table', 'pink_block': 'table', 'grasped': 0, 'contact': 0, 'red_block_lifted': 0, 'blue_block_lifted': 0, 'pink_block_lifted': 0}\n",
      "The instruction  grasp the blue block, then lift the grasped block\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1/5 : 14.3% | 2/5 : 0.0% | 3/5 : 0.0% | 4/5 : 0.0% | 5/5 : 0.0% | Average: 0.1 |:  35%|      | 7/20 [01:52<03:19, 15.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating sequence:  ['grasp_blue_block', 'lift_grasped_block', 'place_grasped_block_over_slider', 'ungrasp_block']\n",
      "Initial state:  {'led': 0, 'lightbulb': 0, 'slider': 'right', 'drawer': 'closed', 'red_block': 'slider_right', 'blue_block': 'table', 'pink_block': 'slider_left', 'grasped': 0, 'contact': 0, 'red_block_lifted': 0, 'blue_block_lifted': 0, 'pink_block_lifted': 0}\n",
      "The instruction  grasp the blue block, then lift the grasped block, then place the grasped block over the sliding cabinet, then ungrasp the block\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1/5 : 12.5% | 2/5 : 0.0% | 3/5 : 0.0% | 4/5 : 0.0% | 5/5 : 0.0% | Average: 0.1 |:  40%|      | 8/20 [02:09<03:10, 15.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating sequence:  ['grasp_pink_block', 'lift_grasped_block', 'place_grasped_block_over_slider', 'ungrasp_block']\n",
      "Initial state:  {'led': 0, 'lightbulb': 0, 'slider': 'right', 'drawer': 'closed', 'red_block': 'slider_right', 'blue_block': 'slider_left', 'pink_block': 'table', 'grasped': 0, 'contact': 0, 'red_block_lifted': 0, 'blue_block_lifted': 0, 'pink_block_lifted': 0}\n",
      "The instruction  grasp the pink block, then lift the grasped block, then place the grasped block over the sliding cabinet, then ungrasp the block\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1/5 : 11.1% | 2/5 : 0.0% | 3/5 : 0.0% | 4/5 : 0.0% | 5/5 : 0.0% | Average: 0.1 |:  45%|     | 9/20 [02:26<03:00, 16.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating sequence:  ['grasp_blue_block', 'rotate_grasped_block_left', 'ungrasp_block']\n",
      "Initial state:  {'led': 0, 'lightbulb': 0, 'slider': 'right', 'drawer': 'closed', 'red_block': 'slider_left', 'blue_block': 'table', 'pink_block': 'table', 'grasped': 0, 'contact': 0, 'red_block_lifted': 0, 'blue_block_lifted': 0, 'pink_block_lifted': 0}\n",
      "The instruction  grasp the blue block, then rotate the grasped block 90 degrees to the left, then ungrasp the block\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1/5 : 10.0% | 2/5 : 0.0% | 3/5 : 0.0% | 4/5 : 0.0% | 5/5 : 0.0% | Average: 0.1 |:  50%|     | 10/20 [02:46<02:53, 17.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating sequence:  ['grasp_slider', 'move_slider_left', 'ungrasp_slider']\n",
      "Initial state:  {'led': 0, 'lightbulb': 0, 'slider': 'right', 'drawer': 'closed', 'red_block': 'slider_left', 'blue_block': 'table', 'pink_block': 'slider_right', 'grasped': 0, 'contact': 0, 'red_block_lifted': 0, 'blue_block_lifted': 0, 'pink_block_lifted': 0}\n",
      "The instruction  grasp the slider handle, then move the handle to the left, then ungrasp the slider handle\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1/5 : 9.1% | 2/5 : 0.0% | 3/5 : 0.0% | 4/5 : 0.0% | 5/5 : 0.0% | Average: 0.1 |:  55%|    | 11/20 [03:02<02:32, 16.97s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating sequence:  ['grasp_red_block', 'lift_grasped_block', 'place_grasped_block_over_pink_block', 'ungrasp_block']\n",
      "Initial state:  {'led': 0, 'lightbulb': 0, 'slider': 'right', 'drawer': 'closed', 'red_block': 'slider_left', 'blue_block': 'slider_right', 'pink_block': 'table', 'grasped': 0, 'contact': 0, 'red_block_lifted': 0, 'blue_block_lifted': 0, 'pink_block_lifted': 0}\n",
      "The instruction  grasp the red block, then lift the grasped block, then place the grasped block over the pink block, then ungrasp the block\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1/5 : 8.3% | 2/5 : 0.0% | 3/5 : 0.0% | 4/5 : 0.0% | 5/5 : 0.0% | Average: 0.1 |:  60%|    | 12/20 [03:22<02:22, 17.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating sequence:  ['grasp_blue_block', 'lift_grasped_block', 'place_grasped_block_over_drawer', 'ungrasp_block']\n",
      "Initial state:  {'led': 0, 'lightbulb': 0, 'slider': 'right', 'drawer': 'open', 'red_block': 'table', 'blue_block': 'table', 'pink_block': 'slider_right', 'grasped': 0, 'contact': 0, 'red_block_lifted': 0, 'blue_block_lifted': 0, 'pink_block_lifted': 0}\n",
      "The instruction  grasp the blue block, then lift the grasped block, then place the grasped block over the drawer, then ungrasp the block\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1/5 : 7.7% | 2/5 : 0.0% | 3/5 : 0.0% | 4/5 : 0.0% | 5/5 : 0.0% | Average: 0.1 |:  65%|   | 13/20 [03:35<01:55, 16.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating sequence:  ['contact_blue_block_right', 'push_block_left']\n",
      "Initial state:  {'led': 0, 'lightbulb': 0, 'slider': 'right', 'drawer': 'open', 'red_block': 'table', 'blue_block': 'table', 'pink_block': 'slider_left', 'grasped': 0, 'contact': 0, 'red_block_lifted': 0, 'blue_block_lifted': 0, 'pink_block_lifted': 0}\n",
      "The instruction  touch the blue block on its right side, then push the block towards the left\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1/5 : 7.1% | 2/5 : 0.0% | 3/5 : 0.0% | 4/5 : 0.0% | 5/5 : 0.0% | Average: 0.1 |:  70%|   | 14/20 [03:50<01:35, 15.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating sequence:  ['grasp_pink_block', 'lift_grasped_block']\n",
      "Initial state:  {'led': 0, 'lightbulb': 0, 'slider': 'right', 'drawer': 'open', 'red_block': 'table', 'blue_block': 'slider_right', 'pink_block': 'table', 'grasped': 0, 'contact': 0, 'red_block_lifted': 0, 'blue_block_lifted': 0, 'pink_block_lifted': 0}\n",
      "The instruction  grasp the pink block, then lift the grasped block\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1/5 : 13.3% | 2/5 : 0.0% | 3/5 : 0.0% | 4/5 : 0.0% | 5/5 : 0.0% | Average: 0.1 |:  75%|  | 15/20 [03:53<01:00, 12.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating sequence:  ['grasp_slider', 'move_slider_left', 'ungrasp_slider']\n",
      "Initial state:  {'led': 0, 'lightbulb': 0, 'slider': 'right', 'drawer': 'open', 'red_block': 'table', 'blue_block': 'slider_right', 'pink_block': 'slider_left', 'grasped': 0, 'contact': 0, 'red_block_lifted': 0, 'blue_block_lifted': 0, 'pink_block_lifted': 0}\n",
      "The instruction  grasp the slider handle, then move the handle to the left, then ungrasp the slider handle\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1/5 : 18.8% | 2/5 : 0.0% | 3/5 : 0.0% | 4/5 : 0.0% | 5/5 : 0.0% | Average: 0.2 |:  80%|  | 16/20 [03:58<00:39,  9.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating sequence:  ['grasp_pink_block', 'lift_grasped_block']\n",
      "Initial state:  {'led': 0, 'lightbulb': 0, 'slider': 'right', 'drawer': 'open', 'red_block': 'table', 'blue_block': 'slider_left', 'pink_block': 'table', 'grasped': 0, 'contact': 0, 'red_block_lifted': 0, 'blue_block_lifted': 0, 'pink_block_lifted': 0}\n",
      "The instruction  grasp the pink block, then lift the grasped block\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1/5 : 23.5% | 2/5 : 0.0% | 3/5 : 0.0% | 4/5 : 0.0% | 5/5 : 0.0% | Average: 0.2 |:  85%| | 17/20 [04:12<00:33, 11.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating sequence:  ['contact_red_block_left', 'push_block_right']\n",
      "Initial state:  {'led': 0, 'lightbulb': 0, 'slider': 'right', 'drawer': 'open', 'red_block': 'table', 'blue_block': 'slider_left', 'pink_block': 'slider_right', 'grasped': 0, 'contact': 0, 'red_block_lifted': 0, 'blue_block_lifted': 0, 'pink_block_lifted': 0}\n",
      "The instruction  touch the red block on its left side, then push the block towards the right\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1/5 : 22.2% | 2/5 : 0.0% | 3/5 : 0.0% | 4/5 : 0.0% | 5/5 : 0.0% | Average: 0.2 |:  90%| | 18/20 [04:33<00:28, 14.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating sequence:  ['grasp_pink_block', 'lift_grasped_block', 'place_grasped_block_over_drawer', 'ungrasp_block']\n",
      "Initial state:  {'led': 0, 'lightbulb': 0, 'slider': 'right', 'drawer': 'open', 'red_block': 'slider_right', 'blue_block': 'table', 'pink_block': 'table', 'grasped': 0, 'contact': 0, 'red_block_lifted': 0, 'blue_block_lifted': 0, 'pink_block_lifted': 0}\n",
      "The instruction  grasp the pink block, then lift the grasped block, then place the grasped block over the drawer, then ungrasp the block\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1/5 : 21.1% | 2/5 : 0.0% | 3/5 : 0.0% | 4/5 : 0.0% | 5/5 : 0.0% | Average: 0.2 |:  95%|| 19/20 [04:53<00:15, 15.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating sequence:  ['contact_blue_block_left', 'push_block_right']\n",
      "Initial state:  {'led': 0, 'lightbulb': 0, 'slider': 'right', 'drawer': 'open', 'red_block': 'slider_right', 'blue_block': 'table', 'pink_block': 'slider_left', 'grasped': 0, 'contact': 0, 'red_block_lifted': 0, 'blue_block_lifted': 0, 'pink_block_lifted': 0}\n",
      "The instruction  touch the blue block on its left side, then push the block towards the right\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1/5 : 20.0% | 2/5 : 0.0% | 3/5 : 0.0% | 4/5 : 0.0% | 5/5 : 0.0% | Average: 0.2 |: 100%|| 20/20 [05:13<00:00, 15.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE SAVE LOCATION /ubc/cs/research/nlp/grigorii/projects/openvla/experiments/robot/calvin/debug_videos/experiment_2_20250207_133938_small_2/_long_horizon_sequence_0_0.gif\n",
      "THE SAVE LOCATION /ubc/cs/research/nlp/grigorii/projects/openvla/experiments/robot/calvin/debug_videos/experiment_2_20250207_133938_small_2/_long_horizon_sequence_1_0.gif\n",
      "THE SAVE LOCATION /ubc/cs/research/nlp/grigorii/projects/openvla/experiments/robot/calvin/debug_videos/experiment_2_20250207_133938_small_2/_long_horizon_sequence_2_0.gif\n",
      "THE SAVE LOCATION /ubc/cs/research/nlp/grigorii/projects/openvla/experiments/robot/calvin/debug_videos/experiment_2_20250207_133938_small_2/_long_horizon_sequence_3_0.gif\n",
      "THE SAVE LOCATION /ubc/cs/research/nlp/grigorii/projects/openvla/experiments/robot/calvin/debug_videos/experiment_2_20250207_133938_small_2/_long_horizon_sequence_4_0.gif\n",
      "THE SAVE LOCATION /ubc/cs/research/nlp/grigorii/projects/openvla/experiments/robot/calvin/debug_videos/experiment_2_20250207_133938_small_2/_long_horizon_sequence_5_0.gif\n",
      "THE SAVE LOCATION /ubc/cs/research/nlp/grigorii/projects/openvla/experiments/robot/calvin/debug_videos/experiment_2_20250207_133938_small_2/_long_horizon_sequence_6_0.gif\n",
      "THE SAVE LOCATION /ubc/cs/research/nlp/grigorii/projects/openvla/experiments/robot/calvin/debug_videos/experiment_2_20250207_133938_small_2/_long_horizon_sequence_7_0.gif\n",
      "THE SAVE LOCATION /ubc/cs/research/nlp/grigorii/projects/openvla/experiments/robot/calvin/debug_videos/experiment_2_20250207_133938_small_2/_long_horizon_sequence_8_0.gif\n",
      "THE SAVE LOCATION /ubc/cs/research/nlp/grigorii/projects/openvla/experiments/robot/calvin/debug_videos/experiment_2_20250207_133938_small_2/_long_horizon_sequence_9_0.gif\n",
      "THE SAVE LOCATION /ubc/cs/research/nlp/grigorii/projects/openvla/experiments/robot/calvin/debug_videos/experiment_2_20250207_133938_small_2/_long_horizon_sequence_10_0.gif\n",
      "THE SAVE LOCATION /ubc/cs/research/nlp/grigorii/projects/openvla/experiments/robot/calvin/debug_videos/experiment_2_20250207_133938_small_2/_long_horizon_sequence_11_0.gif\n",
      "THE SAVE LOCATION /ubc/cs/research/nlp/grigorii/projects/openvla/experiments/robot/calvin/debug_videos/experiment_2_20250207_133938_small_2/_long_horizon_sequence_12_0.gif\n",
      "THE SAVE LOCATION /ubc/cs/research/nlp/grigorii/projects/openvla/experiments/robot/calvin/debug_videos/experiment_2_20250207_133938_small_2/_long_horizon_sequence_13_0.gif\n",
      "THE SAVE LOCATION /ubc/cs/research/nlp/grigorii/projects/openvla/experiments/robot/calvin/debug_videos/experiment_2_20250207_133938_small_2/_long_horizon_sequence_14_0.gif\n",
      "THE SAVE LOCATION /ubc/cs/research/nlp/grigorii/projects/openvla/experiments/robot/calvin/debug_videos/experiment_2_20250207_133938_small_2/_long_horizon_sequence_15_0.gif\n",
      "THE SAVE LOCATION /ubc/cs/research/nlp/grigorii/projects/openvla/experiments/robot/calvin/debug_videos/experiment_2_20250207_133938_small_2/_long_horizon_sequence_16_0.gif\n",
      "THE SAVE LOCATION /ubc/cs/research/nlp/grigorii/projects/openvla/experiments/robot/calvin/debug_videos/experiment_2_20250207_133938_small_2/_long_horizon_sequence_17_0.gif\n",
      "THE SAVE LOCATION /ubc/cs/research/nlp/grigorii/projects/openvla/experiments/robot/calvin/debug_videos/experiment_2_20250207_133938_small_2/_long_horizon_sequence_18_0.gif\n",
      "THE SAVE LOCATION /ubc/cs/research/nlp/grigorii/projects/openvla/experiments/robot/calvin/debug_videos/experiment_2_20250207_133938_small_2/_long_horizon_sequence_19_0.gif\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 4, got 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m calvin_cfg\u001b[38;5;241m.\u001b[39mnum_sequences \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m\n\u001b[1;32m      3\u001b[0m calvin_cfg\u001b[38;5;241m.\u001b[39mnum_videos \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m\n\u001b[0;32m----> 5\u001b[0m results, average_rate, success_rates, counters \u001b[38;5;241m=\u001b[39m evaluate_policy(model, env, calvin_cfg, \n\u001b[1;32m      6\u001b[0m                 processor, \n\u001b[1;32m      7\u001b[0m                 num_videos\u001b[38;5;241m=\u001b[39mcalvin_cfg\u001b[38;5;241m.\u001b[39mnum_videos, \n\u001b[1;32m      8\u001b[0m                 save_dir\u001b[38;5;241m=\u001b[39mvideo_save_dir\n\u001b[1;32m      9\u001b[0m                 )\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 4, got 3)"
     ]
    }
   ],
   "source": [
    "calvin_cfg.ep_len = 200\n",
    "calvin_cfg.num_sequences = 20\n",
    "calvin_cfg.num_videos = 20\n",
    "\n",
    "results, average_rate, success_rates, counters = evaluate_policy(model, env, calvin_cfg, \n",
    "                processor, \n",
    "                num_videos=calvin_cfg.num_videos, \n",
    "                save_dir=video_save_dir\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "default_config_file = \"/ubc/cs/research/nlp/grigorii/projects/octo_original/octo/scripts/configs/finetune_config.py\"\n",
    "config_flags.DEFINE_config_file(\n",
    "    \"config\",\n",
    "    default_config_file,\n",
    "    \"File path to the training hyperparameter configuration.\",\n",
    "    lock_config=False,\n",
    ")\n",
    "\n",
    "\n",
    "# create a 1D mesh with a single axis named \"batch\"\n",
    "mesh = Mesh(jax.devices(), axis_names=\"batch\")\n",
    "# Our batches will be data-parallel sharded -- each device will get a slice of the batch\n",
    "dp_sharding = NamedSharding(mesh, PartitionSpec(\"batch\"))\n",
    "# Our model will be replicated across devices (we are only doing data parallelism, not model parallelism)\n",
    "replicated_sharding = NamedSharding(mesh, PartitionSpec())\n",
    "\n",
    "# prevent tensorflow from using GPU memory since it's only used for data loading\n",
    "tf.config.set_visible_devices([], \"GPU\")\n",
    "\n",
    "from scripts.configs.finetune_config import get_config\n",
    "flags_config = get_config()\n",
    "flags_config.pretrained_path = \"hf://rail-berkeley/octo-small-1.5\"\n",
    "pretrained_model = OctoModel.load_pretrained(\n",
    "    flags_config.pretrained_path,\n",
    "    step=flags_config.pretrained_step,\n",
    ")\n",
    "\n",
    "flat_config = flax.traverse_util.flatten_dict(\n",
    "    pretrained_model.config, keep_empty_nodes=True\n",
    ")\n",
    "for d_key in flax.traverse_util.flatten_dict(\n",
    "    flags_config.get(\"config_delete_keys\", ConfigDict()).to_dict()\n",
    "):\n",
    "    for c_key in list(flat_config.keys()):\n",
    "        if \".\".join(c_key).startswith(\".\".join(d_key)):\n",
    "            del flat_config[c_key]\n",
    "\n",
    "config = ConfigDict(flax.traverse_util.unflatten_dict(flat_config))\n",
    "config.update(flags_config.get(\"update_config\", ConfigDict()))\n",
    "config = config.to_dict()\n",
    "check_config_diff(config, pretrained_model.config)\n",
    "flags_config.dataset_kwargs.proprio_obs_key = \"proprio\"\n",
    "if config[\"text_processor\"] is None:\n",
    "    text_processor = None\n",
    "else:\n",
    "    text_processor = ModuleSpec.instantiate(config[\"text_processor\"])()\n",
    "\n",
    "def process_batch(batch):\n",
    "    batch = process_text(batch, text_processor)\n",
    "    del batch[\"dataset_name\"]\n",
    "    return batch\n",
    "\n",
    "dataset = make_single_dataset(\n",
    "    flags_config.dataset_kwargs,\n",
    "    traj_transform_kwargs=flags_config.traj_transform_kwargs,\n",
    "    frame_transform_kwargs=flags_config.frame_transform_kwargs,\n",
    "    train=True,\n",
    ")\n",
    "\n",
    "train_data_iter = (\n",
    "    dataset.repeat()\n",
    "    .unbatch()\n",
    "    .shuffle(flags_config.shuffle_buffer_size)\n",
    "    .batch(flags_config.batch_size)\n",
    "    .iterator()\n",
    ")\n",
    "\n",
    "train_data_iter = map(process_batch, train_data_iter)\n",
    "example_batch = next(train_data_iter)\n",
    "\n",
    "#########\n",
    "#\n",
    "# Load Pretrained Model\n",
    "#\n",
    "#########\n",
    "\n",
    "rng = jax.random.PRNGKey(flags_config.seed)\n",
    "rng, init_rng = jax.random.split(rng)\n",
    "model = OctoModel.from_config(\n",
    "    config,\n",
    "    example_batch,\n",
    "    text_processor,\n",
    "    rng=init_rng,\n",
    "    dataset_statistics=dataset.dataset_statistics,\n",
    ")\n",
    "merged_params = merge_params(model.params, pretrained_model.params)\n",
    "model = model.replace(params=merged_params)\n",
    "del pretrained_model\n",
    "\n",
    "from experiments.robot.calvin.calvin_utils import get_calvin_env, add_text\n",
    "\n",
    "env, calvin_cfg, lang_embeddings = get_calvin_env(\n",
    "    \"/ubc/cs/research/nlp/grigorii/projects/openvla/experiments/robot/calvin/conf/med_tasks_config.yaml\",\n",
    "    device_id=0,\n",
    ")\n",
    "\n",
    "import jax.numpy as jnp\n",
    "\n",
    "import pyhash\n",
    "hasher = pyhash.fnv1_32()\n",
    "import numpy as  np\n",
    "import torch\n",
    "from torchvision.transforms import Resize\n",
    "\n",
    "\n",
    "def normalize_gripper_action(action, binarize=True):\n",
    "    \"\"\"\n",
    "    Changes gripper action (last dimension of action vector) from [0,1] to [-1,+1].\n",
    "    Necessary for some environments (not Bridge) because the dataset wrapper standardizes gripper actions to [0,1].\n",
    "    Note that unlike the other action dimensions, the gripper action is not normalized to [-1,+1] by default by\n",
    "    the dataset wrapper.\n",
    "\n",
    "    Normalization formula: y = 2 * (x - orig_low) / (orig_high - orig_low) - 1\n",
    "    \"\"\"\n",
    "    # Just normalize the last action to [-1,+1].\n",
    "    orig_low, orig_high = 0.0, 1.0\n",
    "    action[..., -1] = 2 * (action[..., -1] - orig_low) / (orig_high - orig_low) - 1\n",
    "\n",
    "    if binarize:\n",
    "        # Binarize to -1 or +1.\n",
    "        action[..., -1] = np.sign(action[..., -1])\n",
    "\n",
    "    return action\n",
    "\n",
    "\n",
    "def decode_actions(normalized_actions, model, unnorm_key, from_tokens=False):\n",
    "    unnormalization_statistics = model.dataset_statistics[\"action\"]\n",
    "    mask = unnormalization_statistics.get(\n",
    "                    \"mask\",\n",
    "                    jnp.ones_like(unnormalization_statistics[\"mean\"], dtype=bool),\n",
    "                )\n",
    "    action = normalized_actions[..., : len(mask)]\n",
    "    action = jnp.where(\n",
    "        mask,\n",
    "        (action * unnormalization_statistics[\"std\"])\n",
    "        + unnormalization_statistics[\"mean\"],\n",
    "        action,\n",
    "    )\n",
    "\n",
    "    return action\n",
    "\n",
    "\n",
    "def resize_image(img, resize_size):\n",
    "    \"\"\"\n",
    "    Takes numpy array corresponding to a single image and returns resized image as numpy array.\n",
    "\n",
    "    NOTE (Moo Jin): To make input images in distribution with respect to the inputs seen at training time, we follow\n",
    "                    the same resizing scheme used in the Octo dataloader, which OpenVLA uses for training.\n",
    "    \"\"\"\n",
    "    assert isinstance(resize_size, tuple)\n",
    "    # Resize to image size expected by model\n",
    "    img = tf.image.encode_jpeg(img)  # Encode as JPEG, as done in RLDS dataset builder\n",
    "    img = tf.io.decode_image(img, expand_animations=False, dtype=tf.uint8)  # Immediately decode back\n",
    "    img = tf.image.resize(img, resize_size, method=\"lanczos3\", antialias=True)\n",
    "    img = tf.cast(tf.clip_by_value(tf.round(img), 0, 255), tf.uint8)\n",
    "    img = img.numpy()\n",
    "    return img\n",
    "\n",
    "\n",
    "def join_vis_lang(img, lang_text):\n",
    "    \"\"\"Takes as input an image and a language instruction and visualizes them with cv2\"\"\"\n",
    "    img = img[:, :, ::-1].copy()\n",
    "    img = cv2.resize(img, (500, 500))\n",
    "    add_text(img, lang_text)\n",
    "    cv2.imshow(\"simulation cam\", img)\n",
    "    cv2.waitKey(1)\n",
    "\n",
    "\n",
    "def get_video_tag(i):\n",
    "    return f\"_long_horizon/sequence_{i}\"\n",
    "\n",
    "\n",
    "def get_env_state_for_initial_condition(initial_condition):\n",
    "    robot_obs = np.array(\n",
    "        [\n",
    "            0.02586889,\n",
    "            -0.2313129,\n",
    "            0.5712808,\n",
    "            3.09045411,\n",
    "            -0.02908596,\n",
    "            1.50013585,\n",
    "            0.07999963,\n",
    "            -1.21779124,\n",
    "            1.03987629,\n",
    "            2.11978254,\n",
    "            -2.34205014,\n",
    "            -0.87015899,\n",
    "            1.64119093,\n",
    "            0.55344928,\n",
    "            1.0,\n",
    "        ]\n",
    "    )\n",
    "    block_rot_z_range = (np.pi / 2 - np.pi / 8, np.pi / 2 + np.pi / 8)\n",
    "    block_slider_left = np.array([-2.40851662e-01, 9.24044687e-02, 4.60990009e-01])\n",
    "    block_slider_right = np.array([7.03416330e-02, 9.24044687e-02, 4.60990009e-01])\n",
    "    block_table = [\n",
    "        np.array([5.00000896e-02, -1.20000177e-01, 4.59990009e-01]),\n",
    "        np.array([2.29995412e-01, -1.19995140e-01, 4.59990010e-01]),\n",
    "    ]\n",
    "    # we want to have a \"deterministic\" random seed for each initial condition\n",
    "    # TODO: Figure this out\n",
    "    #seed = hash(str(initial_condition.values()))\n",
    "    seed = hasher(str(initial_condition.values()))\n",
    "    #print(\"the seed? \", seed)\n",
    "    with temp_seed(seed):\n",
    "        np.random.shuffle(block_table)\n",
    "\n",
    "        scene_obs = np.zeros(24)\n",
    "        if initial_condition[\"slider\"] == \"left\":\n",
    "            scene_obs[0] = 0.28\n",
    "        if initial_condition[\"drawer\"] == \"open\":\n",
    "            scene_obs[1] = 0.22\n",
    "        if initial_condition[\"lightbulb\"] == 1:\n",
    "            scene_obs[3] = 0.088\n",
    "        scene_obs[4] = initial_condition[\"lightbulb\"]\n",
    "        scene_obs[5] = initial_condition[\"led\"]\n",
    "        # red block\n",
    "        if initial_condition[\"red_block\"] == \"slider_right\":\n",
    "            scene_obs[6:9] = block_slider_right\n",
    "        elif initial_condition[\"red_block\"] == \"slider_left\":\n",
    "            scene_obs[6:9] = block_slider_left\n",
    "        else:\n",
    "            scene_obs[6:9] = block_table[0]\n",
    "        scene_obs[11] = np.random.uniform(*block_rot_z_range)\n",
    "        # blue block\n",
    "        if initial_condition[\"blue_block\"] == \"slider_right\":\n",
    "            scene_obs[12:15] = block_slider_right\n",
    "        elif initial_condition[\"blue_block\"] == \"slider_left\":\n",
    "            scene_obs[12:15] = block_slider_left\n",
    "        elif initial_condition[\"red_block\"] == \"table\":\n",
    "            scene_obs[12:15] = block_table[1]\n",
    "        else:\n",
    "            scene_obs[12:15] = block_table[0]\n",
    "        scene_obs[17] = np.random.uniform(*block_rot_z_range)\n",
    "        # pink block\n",
    "        if initial_condition[\"pink_block\"] == \"slider_right\":\n",
    "            scene_obs[18:21] = block_slider_right\n",
    "        elif initial_condition[\"pink_block\"] == \"slider_left\":\n",
    "            scene_obs[18:21] = block_slider_left\n",
    "        else:\n",
    "            scene_obs[18:21] = block_table[1]\n",
    "        scene_obs[23] = np.random.uniform(*block_rot_z_range)\n",
    "\n",
    "    return robot_obs, scene_obs\n",
    "\n",
    "\n",
    "def count_success(results):\n",
    "    count = Counter(results)\n",
    "    step_success = []\n",
    "    for i in range(1, 6):\n",
    "        n_success = sum(count[j] for j in reversed(range(i, 6)))\n",
    "        sr = n_success / len(results)\n",
    "        step_success.append(sr)\n",
    "    return step_success\n",
    "\n",
    "\n",
    "def evaluate_policy(model, env, traj_dataset, batch_transform, lang_embeddings, cfg, processor, num_videos=0, checkp_path=None, save_dir=None):\n",
    "    task_oracle = hydra.utils.instantiate(cfg.tasks)\n",
    "    val_annotations = cfg.annotations\n",
    "    # video stuff\n",
    "    if num_videos > 0:\n",
    "        rollout_video = RolloutVideo(\n",
    "            logger=logger,\n",
    "            empty_cache=False,\n",
    "            log_to_file=True,\n",
    "            save_dir=cfg.video_save_dir,\n",
    "            resolution_scale=1,\n",
    "        )\n",
    "    else:\n",
    "        rollout_video = None\n",
    "\n",
    "    evaluate_trajectories(\n",
    "        env, model, traj_dataset, batch_transform, task_oracle, val_annotations, cfg, processor, rollout_video, num_videos\n",
    "    )\n",
    "\n",
    "    if num_videos > 0:\n",
    "        # log rollout videos\n",
    "        rollout_video._log_videos_to_file(0, save_as_video=False)\n",
    "    return results, average_rate, plans\n",
    "\n",
    "def transform_traj(traj):\n",
    "    new_traj = []\n",
    "    for tstep in range(traj['observation']['image_primary'].shape[0]):\n",
    "        new_step = {}\n",
    "        new_step['observation'] = {}\n",
    "        new_step['observation']['image_primary'] = traj['observation']['image_primary'][tstep]\n",
    "        new_step['observation']['image_wrist'] = traj['observation']['image_wrist'][tstep]\n",
    "        new_step['observation']['timestep'] = traj['observation']['timestep'][tstep]\n",
    "        new_step['observation']['proprio'] = traj['observation']['proprio'][tstep]\n",
    "\n",
    "        new_step['observation']['pad_mask_dict'] = {}\n",
    "        new_step['observation']['pad_mask_dict']['image_primary'] = traj['observation']['pad_mask_dict']['image_primary'][tstep]\n",
    "        new_step['observation']['pad_mask_dict']['image_wrist'] = traj['observation']['pad_mask_dict']['image_wrist'][tstep]\n",
    "        new_step['observation']['pad_mask_dict']['timestep'] = traj['observation']['pad_mask_dict']['timestep'][tstep]\n",
    "\n",
    "        \n",
    "        new_step['observation']['timestep_pad_mask'] = traj['observation']['timestep_pad_mask'][tstep]\n",
    "        new_step['observation']['task_completed'] = traj['observation']['task_completed'][tstep]\n",
    "\n",
    "        new_step['task'] = {}\n",
    "        new_step['task']['language_instruction'] = traj['task']['language_instruction'][tstep]\n",
    "\n",
    "        new_step['task']['pad_mask_dict'] = {}\n",
    "        new_step['task']['pad_mask_dict']['language_instruction'] = traj['task']['pad_mask_dict']['language_instruction'][tstep]\n",
    "\n",
    "        new_step['action'] = traj['action'][tstep]\n",
    "        new_step['dataset_name'] = traj['dataset_name'][tstep]\n",
    "        new_step['action_pad_mask'] = traj['action_pad_mask'][tstep]\n",
    "        new_traj.append(new_step)\n",
    "    return new_traj\n",
    "\n",
    "\n",
    "def evaluate_trajectories(\n",
    "    env, model, traj_dataset, batch_transform, task_checker, val_annotations, cfg, processor, rollout_video, num_videos\n",
    "):\n",
    "\n",
    "    for i, traj in enumerate(traj_dataset):\n",
    "        if i > num_videos:\n",
    "            break\n",
    "        traj = transform_traj(traj)\n",
    "        annotation = traj[0]['task']['language_instruction']\n",
    "        annotation = annotation.numpy().decode(\"utf-8\")\n",
    "        rollout_video.new_video(tag=get_video_tag(i), caption=annotation)\n",
    "        rollout_video.new_subtask()\n",
    "\n",
    "        proprio = traj[0]['observation']['proprio']\n",
    "        \n",
    "        robot_obs, scene_obs = np.split(proprio.numpy()[0], indices_or_sections=[15])\n",
    "        env.reset(robot_obs=robot_obs, scene_obs=scene_obs)\n",
    "\n",
    "        rollout(env, traj, batch_transform, model, task_checker, cfg, annotation, processor=processor, record=True, rollout_video=rollout_video)\n",
    "\n",
    "        rollout_video.write_to_tmp()\n",
    "\n",
    "\n",
    "def rollout(env, traj, batch_transform, model, task_oracle, cfg, lang_annotation, processor, record=False, rollout_video=None):\n",
    "    if cfg.debug:\n",
    "        print(f\"{subtask} \", end=\"\")\n",
    "        time.sleep(0.5)\n",
    "\n",
    "    obs = env.get_obs()\n",
    "\n",
    "\n",
    "    start_info = env.get_info()\n",
    "\n",
    "    for step in range(len(traj)):\n",
    "        print(\"Step: \", step)\n",
    "        traj_step = traj[step]\n",
    "        norm_action = traj_step['action'].numpy()\n",
    "        unnorm_action = decode_actions(norm_action, model, unnorm_key=\"medium_level_tasks_dataset\")\n",
    "        unnorm_action = np.array(unnorm_action)\n",
    "        # Rescale actions from [0, 1] to [-1, 1] and binarize\n",
    "        action = normalize_gripper_action(unnorm_action)\n",
    "        action = action[0][0]\n",
    "        obs, _, _, current_info = env.step(action)\n",
    "\n",
    "        if cfg.debug:\n",
    "            img = env.render(mode=\"rgb_array\")\n",
    "            join_vis_lang(img, lang_annotation)\n",
    "            # time.sleep(0.1)\n",
    "        if record:\n",
    "            # update video\n",
    "            frame_aug = torch.zeros((3, 224, 448))\n",
    "            resize_transform = Resize(224, antialias=True)\n",
    "\n",
    "            frame_aug[:, :, :224] = resize_transform(torch.tensor(obs[\"rgb_obs\"][\"rgb_static\"]).permute(2, 0, 1))\n",
    "            closest_obs = 0\n",
    "            if isinstance(closest_obs, int):\n",
    "                closest_obs = torch.zeros((3, 224, 224))\n",
    "            frame_aug[:, :, 224:] = closest_obs.squeeze()\n",
    "\n",
    "            rollout_video.update(frame_aug.unsqueeze(0).unsqueeze(0))\n",
    "        \"\"\"\n",
    "        # check if current step solves a task\n",
    "        current_task_info = task_oracle.get_task_info_for_set(start_info, current_info, {subtask})\n",
    "        if len(current_task_info) > 0:\n",
    "            if cfg.debug:\n",
    "                print(colored(\"success\", \"green\"), end=\" \")\n",
    "            if record:\n",
    "                rollout_video.add_language_instruction(lang_annotation)\n",
    "            return True\n",
    "        \"\"\"\n",
    "    if cfg.debug:\n",
    "        print(colored(\"fail\", \"red\"), end=\" \")\n",
    "    if record:\n",
    "        rollout_video.add_language_instruction(lang_annotation)\n",
    "    return False\n",
    "\n",
    "import hydra \n",
    "from experiments.robot.calvin.rollout_video import RolloutVideo\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "calvin_cfg.video_save_dir = \"/ubc/cs/research/nlp/grigorii/projects/octo_original/octo/experiments/replay_video_saves\"\n",
    "\n",
    "\n",
    "_, avg_rate, _ = evaluate_policy(model=model, env=env, traj_dataset=dataset, batch_transform=process_batch, lang_embeddings=None, cfg=calvin_cfg, \n",
    "                processor=None, num_videos=50)\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_policy(model, env, lang_embeddings, cfg, processor, num_videos=0, save_dir=None):\n",
    "    task_oracle = hydra.utils.instantiate(cfg.tasks)\n",
    "    val_annotations = cfg.annotations\n",
    "    # video stuff\n",
    "    if num_videos > 0:\n",
    "        rollout_video = RolloutVideo(\n",
    "            logger=logger,\n",
    "            empty_cache=False,\n",
    "            log_to_file=True,\n",
    "            save_dir=save_dir,\n",
    "            resolution_scale=1,\n",
    "        )\n",
    "    else:\n",
    "        rollout_video = None\n",
    "\n",
    "    eval_sequences = get_sequences(cfg.num_sequences)\n",
    "    results = []\n",
    "    plans = defaultdict(list)\n",
    "    print(eval_sequences)\n",
    "\n",
    "    if not cfg.debug:\n",
    "        eval_sequences = tqdm(eval_sequences, position=0, leave=True)\n",
    "    for i, (initial_state, eval_sequence) in enumerate(eval_sequences):\n",
    "        record = i < num_videos\n",
    "        result = evaluate_sequence(\n",
    "            env, model, task_oracle, initial_state, eval_sequence, lang_embeddings, val_annotations, cfg, processor, record, rollout_video, i\n",
    "        )\n",
    "\n",
    "        results.append(result)\n",
    "        if record:\n",
    "            rollout_video.write_to_tmp()\n",
    "        if not cfg.debug:\n",
    "            success_rates = count_success(results)\n",
    "            average_rate = sum(success_rates) / len(success_rates) * 5\n",
    "            description = \" \".join([f\"{i + 1}/5 : {v * 100:.1f}% |\" for i, v in enumerate(success_rates)])\n",
    "            description += f\" Average: {average_rate:.1f} |\"\n",
    "            eval_sequences.set_description(description)\n",
    "\n",
    "    if num_videos > 0:\n",
    "        # log rollout videos\n",
    "        rollout_video._log_videos_to_file(0, save_as_video=False)\n",
    "    return results, average_rate, plans\n",
    "\n",
    "\n",
    "def evaluate_sequence(\n",
    "    env, model, task_checker, initial_state, eval_sequence, lang_embeddings, val_annotations, cfg, processor, record, rollout_video, i\n",
    "):\n",
    "    robot_obs, scene_obs = get_env_state_for_initial_condition(initial_state)\n",
    "    env.reset(robot_obs=robot_obs, scene_obs=scene_obs)\n",
    "    if record:\n",
    "        caption = \" | \".join(eval_sequence)\n",
    "        rollout_video.new_video(tag=get_video_tag(i), caption=caption)\n",
    "    success_counter = 0\n",
    "    if cfg.debug:\n",
    "        time.sleep(1)\n",
    "        print()\n",
    "        print()\n",
    "        print(f\"Evaluating sequence: {' -> '.join(eval_sequence)}\")\n",
    "        print(\"Subtask: \", end=\"\")\n",
    "    for subtask in eval_sequence:\n",
    "        print(eval_sequence)\n",
    "        if record:\n",
    "            rollout_video.new_subtask()\n",
    "        success = rollout(env, model, task_checker, cfg, subtask, lang_embeddings, val_annotations, processor=processor, record=record, rollout_video=rollout_video)\n",
    "        if record:\n",
    "            rollout_video.draw_outcome(success)\n",
    "        if success:\n",
    "            success_counter += 1\n",
    "        else:\n",
    "            return success_counter\n",
    "    return success_counter\n",
    "\n",
    "def rollout(env, model, task_oracle, cfg, subtask, lang_embeddings, val_annotations, processor, record=False, rollout_video=None):\n",
    "    if cfg.debug:\n",
    "        print(f\"{subtask} \", end=\"\")\n",
    "        time.sleep(0.5)\n",
    "    obs = env.get_obs()\n",
    "    # get lang annotation for subtask\n",
    "    lang_annotation = val_annotations[subtask][0]\n",
    "    # get language goal embedding\n",
    "    goal = lang_embeddings.get_lang_goal(lang_annotation)\n",
    "    goal['lang_text'] = lang_annotation\n",
    "    if processor == None:\n",
    "        tasks = model.create_tasks(texts=[lang_annotation])\n",
    "        from octo.utils.train_callbacks import supply_rng\n",
    "        policy_fn = supply_rng(\n",
    "                partial(\n",
    "                    model.sample_actions,\n",
    "                    unnormalization_statistics=model.dataset_statistics[\"action\"],\n",
    "                ),\n",
    "            )\n",
    "        window_size = 4\n",
    "        act_step = 4\n",
    "\n",
    "    #model.reset()\n",
    "    start_info = env.get_info()\n",
    "    first_step = True\n",
    "    past_obs = None\n",
    "\n",
    "\n",
    "    for step in range(cfg.ep_len):\n",
    "        if processor == None:\n",
    "            if act_step > 0 and act_step % window_size == 0:\n",
    "                act_step = 0\n",
    "                \"\"\"\n",
    "                static_2 = resize_image(obs['rgb_obs']['rgb_static'], (256, 256), primary_octo=True)\n",
    "                gripper_2 = resize_image(obs['rgb_obs']['rgb_gripper'], (128, 128))\n",
    "\n",
    "                if past_obs:\n",
    "                    static_1 = resize_image(past_obs['rgb_obs']['rgb_static'], (256, 256), primary_octo=True)\n",
    "                    gripper_1 = resize_image(past_obs['rgb_obs']['rgb_gripper'], (128, 128))\n",
    "                    image_primary = np.stack([static_1, static_2])\n",
    "                    image_wrist = np.stack([gripper_1, gripper_2])\n",
    "                    timestep_pad_mask = np.array([[False, True]])\n",
    "                else:\n",
    "                    image_primary = np.stack([np.zeros((256, 256, 3)), static_2])\n",
    "                    image_wrist = np.stack([np.zeros((128, 128, 3)), gripper_2])\n",
    "                    timestep_pad_mask = np.array([[False, True]])\n",
    "                pad_mask_dict = {\n",
    "                    \"image_primary\": np.array([[True, True]]),\n",
    "                    \"image_wrist\": np.array([[True, True]]),\n",
    "                    \"timestep\": np.array([[False, False]]),\n",
    "                }\n",
    "                \"\"\"\n",
    "                image_primary = np.expand_dims(resize_image(obs['rgb_obs']['rgb_static'], (256, 256), primary_octo=True), 0)\n",
    "                image_wrist = np.expand_dims(resize_image(obs['rgb_obs']['rgb_gripper'], (128, 128)), 0)\n",
    "                timestep_pad_mask = np.array([[True]])\n",
    "                pad_mask_dict = {\n",
    "                    \"image_primary\": np.array([[True]]),\n",
    "                    \"image_wrist\": np.array([[True]]),\n",
    "                    \"timestep\": np.array([[True]]),\n",
    "                }\n",
    "                observation = {\n",
    "                        \"image_primary\": np.expand_dims(image_primary, 0),  # uint8\n",
    "                        \"image_wrist\": np.expand_dims(image_wrist, 0),      # uint8\n",
    "                        \"timestep_pad_mask\": timestep_pad_mask,\n",
    "                        \"pad_mask_dict\": pad_mask_dict,\n",
    "                        \"timestep\": np.array([[step]]),\n",
    "                }\n",
    "                act_buffer = policy_fn(observation, tasks)\n",
    "                # Perform simple receding-horizon control (select the first action)\n",
    "                act_buffer = np.array(act_buffer[0])\n",
    "                action = act_buffer[act_step]\n",
    "            else:\n",
    "                action = act_buffer[act_step]\n",
    "            act_step += 1\n",
    "\n",
    "        else:\n",
    "            observation = {\n",
    "                'full_image': resize_image(obs['rgb_obs']['rgb_static'], (224, 224))\n",
    "            }\n",
    "            action = get_action(\n",
    "                cfg,\n",
    "                model,\n",
    "                observation,\n",
    "                task_label=lang_annotation,\n",
    "                processor=processor,\n",
    "            )\n",
    "        \n",
    "        # Rescale actions from [0, 1] to [-1, 1] and binarize\n",
    "        action = normalize_gripper_action(action)\n",
    "        past_obs = obs\n",
    "        obs, _, _, current_info = env.step(action)\n",
    "        if record:\n",
    "            # update video\n",
    "            frame_aug = torch.zeros((3, 224, 448))\n",
    "            resize = Resize(224, antialias=True)\n",
    "            frame_aug[:, :, :224] = resize(torch.tensor(obs[\"rgb_obs\"][\"rgb_static\"]).permute(2, 0, 1))\n",
    "            closest_obs = 0\n",
    "            if isinstance(closest_obs, int):\n",
    "                closest_obs = torch.zeros((3, 224, 224))\n",
    "            frame_aug[:, :, 224:] = closest_obs.squeeze()\n",
    "\n",
    "            rollout_video.update(frame_aug.unsqueeze(0).unsqueeze(0))\n",
    "        # check if current step solves a task\n",
    "        current_task_info = task_oracle.get_task_info_for_set(start_info, current_info, {subtask})\n",
    "        if len(current_task_info) > 0:\n",
    "            if cfg.debug:\n",
    "                print(colored(\"success\", \"green\"), end=\" \")\n",
    "            if record:\n",
    "                rollout_video.add_language_instruction(lang_annotation)\n",
    "            return True\n",
    "    if cfg.debug:\n",
    "        print(colored(\"fail\", \"red\"), end=\" \")\n",
    "    if record:\n",
    "        rollout_video.add_language_instruction(lang_annotation)\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Unknown checkpoint type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 93\u001b[0m\n\u001b[1;32m     91\u001b[0m             medium_res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhigh_level_completed\u001b[39m\u001b[38;5;124m'\u001b[39m][res_key]\u001b[38;5;241m.\u001b[39mappend(counters[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhigh_level_completed\u001b[39m\u001b[38;5;124m'\u001b[39m])   \n\u001b[1;32m     92\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 93\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnknown checkpoint type\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     96\u001b[0m high_task_categories \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrotate_red_block_right\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrotate\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrotate_red_block_left\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrotate\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstack_block\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstacking\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    132\u001b[0m }\n\u001b[1;32m    135\u001b[0m low_task_categories \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrasp_red_block\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgrasp_block\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrasp_blue_block\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgrasp_block\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplace_grasped_block_over_table\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mplace\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    166\u001b[0m }\n",
      "\u001b[0;31mException\u001b[0m: Unknown checkpoint type"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "ll_scores_path = '/ubc/cs/research/nlp/grigorii/projects/openvla/low_level_scores_sockeye/'\n",
    "\n",
    "onebyone_res = {\n",
    "    'avg_rates': defaultdict(list),\n",
    "    'high_level_started': defaultdict(list),\n",
    "    'high_level_completed': defaultdict(list),\n",
    "    'low_level_started': defaultdict(list),\n",
    "    'low_level_completed': defaultdict(list)\n",
    "}\n",
    "conjunction_res = {\n",
    "    'avg_rates': defaultdict(list),\n",
    "    'high_level_started': defaultdict(list),\n",
    "    'high_level_completed': defaultdict(list)\n",
    "}\n",
    "random_res = {\n",
    "    'avg_rates': defaultdict(list),\n",
    "    'low_level_started': defaultdict(list),\n",
    "    'low_level_completed': defaultdict(list),\n",
    "    'chain_results': defaultdict(list)\n",
    "}\n",
    "\n",
    "medium_res = {\n",
    "    'avg_rates': defaultdict(list),\n",
    "    'chain_results': defaultdict(list)\n",
    "}\n",
    "\n",
    "medium_single_res = {\n",
    "    'avg_rates': defaultdict(list),\n",
    "    'low_level_started': defaultdict(list),\n",
    "    'low_level_completed': defaultdict(list),\n",
    "}\n",
    "\n",
    "\n",
    "def get_model_size_key(res_path):\n",
    "    if 'small' in res_path:\n",
    "        return 'small'\n",
    "    elif 'resnet' in res_path:\n",
    "        return 'resnet'\n",
    "    elif 'voltron' in res_path:\n",
    "        return 'voltron'\n",
    "    elif 'base' in res_path:\n",
    "        return 'base'\n",
    "    else:\n",
    "        raise Exception(\"Unknown model\")\n",
    "\n",
    "for res_dict_name in os.listdir(ll_scores_path):\n",
    "    if \"_conj_\" in res_dict_name:\n",
    "        continue\n",
    "    with open(ll_scores_path + res_dict_name, 'rb') as res_dict_file:\n",
    "        res_dict = pickle.load(res_dict_file)\n",
    "        if 'low_onebyone' in res_dict_name:\n",
    "            results = res_dict[list(res_dict.keys())[0]]\n",
    "            print(results)\n",
    "            succ_rate, _, counters  = results\n",
    "            res_key = get_model_size_key(res_dict_name)\n",
    "            onebyone_res['avg_rates'][res_key].append(succ_rate)\n",
    "            onebyone_res['high_level_started'][res_key].append(counters['high_level_started'])\n",
    "            onebyone_res['high_level_completed'][res_key].append(counters['high_level_completed'])    \n",
    "            onebyone_res['low_level_started'][res_key].append(counters['low_level_started'])\n",
    "            onebyone_res['low_level_completed'][res_key].append(counters['low_level_completed'])    \n",
    "        elif 'conjunction' in res_dict_name:\n",
    "            results = res_dict[list(res_dict.keys())[0]]\n",
    "            succ_rate, _, counters  = results\n",
    "            res_key = get_model_size_key(res_dict_name)\n",
    "            conjunction_res['avg_rates'][res_key].append(succ_rate)\n",
    "            conjunction_res['high_level_started'][res_key].append(counters['high_level_started'])\n",
    "            conjunction_res['high_level_completed'][res_key].append(counters['high_level_completed'])     \n",
    "        elif 'low_random' in res_dict_name:\n",
    "            results = res_dict[list(res_dict.keys())[0]]\n",
    "            succ_rate, chain, counters  = results\n",
    "            res_key = get_model_size_key(res_dict_name)\n",
    "            random_res['avg_rates'][res_key].append(succ_rate)\n",
    "            random_res['chain_results'][res_key].append(chain)\n",
    "            random_res['low_level_started'][res_key].append(counters['low_level_started'])\n",
    "            random_res['low_level_completed'][res_key].append(counters['low_level_completed'])    \n",
    "        elif 'medium_random' in res_dict_name:\n",
    "            results = res_dict[list(res_dict.keys())[0]]\n",
    "            succ_rate, chain, counters  = results\n",
    "            res_key = get_model_size_key(res_dict_name)\n",
    "            medium_res['avg_rates'][res_key].append(succ_rate)\n",
    "            medium_res['chain_results'][res_key].append(chain)\n",
    "            medium_res['low_level_started'][res_key].append(counters['low_level_started'])\n",
    "            medium_res['low_level_completed'][res_key].append(counters['low_level_completed'])   \n",
    "        elif 'medium_single' in res_dict_name:\n",
    "            results = res_dict[list(res_dict.keys())[0]]\n",
    "            succ_rate, chain, counters  = results\n",
    "            res_key = get_model_size_key(res_dict_name)\n",
    "            medium_single_res['avg_rates'][res_key].append(succ_rate)\n",
    "            medium_single_res['high_level_started'][res_key].append(counters['high_level_started'])\n",
    "            medium_single_res['high_level_completed'][res_key].append(counters['high_level_completed'])   \n",
    "        else:\n",
    "            raise Exception('Unknown checkpoint type')\n",
    "\n",
    "\n",
    "high_task_categories = {\n",
    "    \"rotate_red_block_right\": \"rotate\",\n",
    "    \"rotate_red_block_left\": \"rotate\",\n",
    "    \"rotate_blue_block_right\": \"rotate\",\n",
    "    \"rotate_blue_block_left\": \"rotate\",\n",
    "    \"rotate_pink_block_right\": \"rotate\",\n",
    "    \"rotate_pink_block_left\": \"rotate\",\n",
    "    \"push_red_block_right\": \"push\",\n",
    "    \"push_red_block_left\": \"push\",\n",
    "    \"push_blue_block_right\": \"push\",\n",
    "    \"push_blue_block_left\": \"push\",\n",
    "    \"push_pink_block_right\": \"push\",\n",
    "    \"push_pink_block_left\": \"push\",\n",
    "    \"move_slider_left\": \"slider/drawer\",\n",
    "    \"move_slider_right\": \"slider/drawer\",\n",
    "    \"open_drawer\": \"slider/drawer\",\n",
    "    \"close_drawer\": \"slider/drawer\",\n",
    "    \"lift_red_block_table\": \"lift\",\n",
    "    \"lift_red_block_slider\": \"lift\",\n",
    "    \"lift_red_block_drawer\": \"lift\",\n",
    "    \"lift_blue_block_table\": \"lift\",\n",
    "    \"lift_blue_block_slider\": \"lift\",\n",
    "    \"lift_blue_block_drawer\": \"lift\",\n",
    "    \"lift_pink_block_table\": \"lift\",\n",
    "    \"lift_pink_block_slider\": \"lift\",\n",
    "    \"lift_pink_block_drawer\": \"lift\",\n",
    "    \"place_in_slider\": \"place\",\n",
    "    \"place_red_block_in_slider\": \"place\",\n",
    "    \"place_blue_block_in_slider\": \"place\",\n",
    "    \"place_pink_block_in_slider\": \"place\",\n",
    "    \"place_in_drawer\": \"place\",\n",
    "    \"place_red_block_in_drawer\": \"place\",\n",
    "    \"place_blue_block_in_drawer\": \"place\",\n",
    "    \"place_pink_block_in_drawer\": \"place\",\n",
    "    \"push_into_drawer\": \"push\",\n",
    "    \"stack_block\": \"stacking\",\n",
    "}\n",
    "\n",
    "\n",
    "low_task_categories = {\n",
    "    \"grasp_red_block\": 'grasp_block',\n",
    "    \"grasp_blue_block\": 'grasp_block',\n",
    "    \"grasp_pink_block\": 'grasp_block',\n",
    "    \"grasp_slider\": 'grasp_door',\n",
    "    \"grasp_drawer\": 'grasp_door',\n",
    "    \"ungrasp_block\": 'ungrasp',\n",
    "    \"ungrasp_slider\": 'ungrasp',\n",
    "    \"ungrasp_drawer\": 'ungrasp',\n",
    "    \"contact_red_block_left\": 'contact',\n",
    "    \"contact_blue_block_left\": 'contact',\n",
    "    \"contact_pink_block_left\": 'contact',\n",
    "    \"contact_red_block_right\":'contact',\n",
    "    \"contact_blue_block_right\": 'contact',\n",
    "    \"contact_pink_block_right\": 'contact',\n",
    "    \"place_grasped_block_over_red_block\": 'place',\n",
    "    \"place_grasped_block_over_blue_block\": 'place',\n",
    "    \"place_grasped_block_over_pink_block\": 'place',\n",
    "    \"push_block_right\": 'push',\n",
    "    \"push_block_left\": 'push',\n",
    "    \"move_slider_left\": 'drawer',\n",
    "    \"move_slider_right\": 'drawer',\n",
    "    \"open_drawer\": 'drawer',\n",
    "    \"close_drawer\": 'drawer',\n",
    "    \"push_into_drawer\": 'push',\n",
    "    \"lift_grasped_block\": 'lift',\n",
    "    \"rotate_grasped_block_right\": 'rotate',\n",
    "    \"rotate_grasped_block_left\": 'rotate',\n",
    "    \"place_grasped_block_over_drawer\": 'place',\n",
    "    \"place_grasped_block_over_slider\": 'place',\n",
    "    \"place_grasped_block_over_table\": 'place',\n",
    "}\n",
    "ordered_high_categories = ['slider/drawer', 'push', 'lift', 'rotate', 'place', 'stacking']\n",
    "ordered_low_categories = ['grasp_door', 'drawer', 'grasp_block', 'contact', 'ungrasp', 'lift', 'push', 'rotate', 'place']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {'voltron': [Counter({'lift_grasped_block': 142,\n",
       "                       'grasp_red_block': 110,\n",
       "                       'grasp_pink_block': 93,\n",
       "                       'grasp_blue_block': 92,\n",
       "                       'grasp_slider': 50,\n",
       "                       'grasp_drawer': 49,\n",
       "                       'ungrasp_block': 45,\n",
       "                       'close_drawer': 36,\n",
       "                       'rotate_grasped_block_left': 30,\n",
       "                       'rotate_grasped_block_right': 27,\n",
       "                       'contact_blue_block_left': 23,\n",
       "                       'contact_red_block_left': 22,\n",
       "                       'contact_pink_block_right': 17,\n",
       "                       'contact_red_block_right': 15,\n",
       "                       'contact_pink_block_left': 15,\n",
       "                       'push_block_right': 14,\n",
       "                       'contact_blue_block_right': 14,\n",
       "                       'open_drawer': 13,\n",
       "                       'move_slider_left': 11,\n",
       "                       'push_block_left': 10,\n",
       "                       'ungrasp_slider': 9,\n",
       "                       'place_grasped_block_over_slider': 7,\n",
       "                       'place_grasped_block_over_pink_block': 2,\n",
       "                       'place_grasped_block_over_red_block': 1}),\n",
       "              Counter({'lift_grasped_block': 138,\n",
       "                       'grasp_red_block': 110,\n",
       "                       'grasp_pink_block': 93,\n",
       "                       'grasp_blue_block': 92,\n",
       "                       'grasp_slider': 50,\n",
       "                       'grasp_drawer': 49,\n",
       "                       'ungrasp_block': 45,\n",
       "                       'close_drawer': 36,\n",
       "                       'rotate_grasped_block_right': 28,\n",
       "                       'rotate_grasped_block_left': 27,\n",
       "                       'contact_blue_block_left': 23,\n",
       "                       'contact_red_block_left': 22,\n",
       "                       'contact_pink_block_right': 17,\n",
       "                       'contact_red_block_right': 15,\n",
       "                       'contact_pink_block_left': 15,\n",
       "                       'contact_blue_block_right': 14,\n",
       "                       'open_drawer': 13,\n",
       "                       'place_grasped_block_over_slider': 11,\n",
       "                       'move_slider_left': 10,\n",
       "                       'ungrasp_slider': 9,\n",
       "                       'push_block_right': 7,\n",
       "                       'place_grasped_block_over_pink_block': 3,\n",
       "                       'push_block_left': 2,\n",
       "                       'place_grasped_block_over_blue_block': 2,\n",
       "                       'ungrasp_drawer': 2,\n",
       "                       'move_slider_right': 1,\n",
       "                       'place_grasped_block_over_red_block': 1}),\n",
       "              Counter({'lift_grasped_block': 163,\n",
       "                       'grasp_red_block': 110,\n",
       "                       'grasp_pink_block': 93,\n",
       "                       'grasp_blue_block': 92,\n",
       "                       'grasp_slider': 50,\n",
       "                       'grasp_drawer': 49,\n",
       "                       'ungrasp_drawer': 43,\n",
       "                       'ungrasp_slider': 37,\n",
       "                       'close_drawer': 36,\n",
       "                       'ungrasp_block': 33,\n",
       "                       'push_block_right': 26,\n",
       "                       'contact_blue_block_left': 23,\n",
       "                       'contact_red_block_left': 22,\n",
       "                       'rotate_grasped_block_right': 22,\n",
       "                       'rotate_grasped_block_left': 21,\n",
       "                       'move_slider_right': 21,\n",
       "                       'move_slider_left': 18,\n",
       "                       'contact_pink_block_right': 17,\n",
       "                       'contact_red_block_right': 15,\n",
       "                       'contact_pink_block_left': 15,\n",
       "                       'contact_blue_block_right': 14,\n",
       "                       'open_drawer': 13,\n",
       "                       'place_grasped_block_over_slider': 1,\n",
       "                       'place_grasped_block_over_red_block': 1,\n",
       "                       'push_block_left': 1,\n",
       "                       'place_grasped_block_over_blue_block': 1})],\n",
       "             'resnet': [Counter({'lift_grasped_block': 133,\n",
       "                       'grasp_red_block': 110,\n",
       "                       'grasp_pink_block': 93,\n",
       "                       'grasp_blue_block': 92,\n",
       "                       'grasp_slider': 50,\n",
       "                       'ungrasp_block': 50,\n",
       "                       'grasp_drawer': 49,\n",
       "                       'close_drawer': 36,\n",
       "                       'rotate_grasped_block_right': 28,\n",
       "                       'rotate_grasped_block_left': 25,\n",
       "                       'contact_blue_block_left': 23,\n",
       "                       'contact_red_block_left': 22,\n",
       "                       'contact_pink_block_right': 17,\n",
       "                       'contact_red_block_right': 15,\n",
       "                       'contact_pink_block_left': 15,\n",
       "                       'contact_blue_block_right': 14,\n",
       "                       'open_drawer': 13,\n",
       "                       'ungrasp_drawer': 10,\n",
       "                       'push_block_right': 1,\n",
       "                       'place_grasped_block_over_slider': 1,\n",
       "                       'move_slider_left': 1,\n",
       "                       'push_block_left': 1}),\n",
       "              Counter({'lift_grasped_block': 118,\n",
       "                       'grasp_red_block': 110,\n",
       "                       'grasp_pink_block': 93,\n",
       "                       'grasp_blue_block': 92,\n",
       "                       'grasp_slider': 50,\n",
       "                       'grasp_drawer': 49,\n",
       "                       'ungrasp_block': 45,\n",
       "                       'close_drawer': 36,\n",
       "                       'rotate_grasped_block_right': 29,\n",
       "                       'rotate_grasped_block_left': 24,\n",
       "                       'contact_blue_block_left': 23,\n",
       "                       'contact_red_block_left': 22,\n",
       "                       'contact_pink_block_right': 17,\n",
       "                       'contact_red_block_right': 15,\n",
       "                       'contact_pink_block_left': 15,\n",
       "                       'contact_blue_block_right': 14,\n",
       "                       'open_drawer': 13,\n",
       "                       'move_slider_left': 10,\n",
       "                       'ungrasp_slider': 10,\n",
       "                       'ungrasp_drawer': 9,\n",
       "                       'push_block_left': 2,\n",
       "                       'push_block_right': 1}),\n",
       "              Counter({'lift_grasped_block': 122,\n",
       "                       'grasp_red_block': 110,\n",
       "                       'grasp_pink_block': 93,\n",
       "                       'grasp_blue_block': 92,\n",
       "                       'grasp_slider': 50,\n",
       "                       'grasp_drawer': 49,\n",
       "                       'ungrasp_block': 41,\n",
       "                       'close_drawer': 36,\n",
       "                       'rotate_grasped_block_right': 28,\n",
       "                       'rotate_grasped_block_left': 25,\n",
       "                       'contact_blue_block_left': 23,\n",
       "                       'contact_red_block_left': 22,\n",
       "                       'contact_pink_block_right': 17,\n",
       "                       'contact_red_block_right': 15,\n",
       "                       'contact_pink_block_left': 15,\n",
       "                       'contact_blue_block_right': 14,\n",
       "                       'open_drawer': 13,\n",
       "                       'push_block_right': 3,\n",
       "                       'push_block_left': 1,\n",
       "                       'ungrasp_drawer': 1})]})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onebyone_res['low_level_started']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ungrasp', 'place', 'grasp_block', 'push', 'grasp_door', 'rotate', 'contact', 'lift', 'drawer'}\n"
     ]
    }
   ],
   "source": [
    "print(set(low_task_categories.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_results(models, \n",
    "                  avg_rates, \n",
    "                  chain_results=None,\n",
    "                  low_level_started=None, \n",
    "                  low_level_completed=None, \n",
    "                  high_level_started=None, \n",
    "                  high_level_completed=None, \n",
    "                  high_categories=None, \n",
    "                  low_categories=None,\n",
    "                  ordered_high=None,\n",
    "                  ordered_low=None):\n",
    "    \n",
    "    for model in models:\n",
    "        mean, sd = np.mean(avg_rates[model]), np.std(avg_rates[model])\n",
    "        print(model)\n",
    "        total_mean_sd = '${}'.format(round(mean, 2)) + \" \\pm \" + '{}$'.format(round(sd, 2))\n",
    "        \n",
    "        if chain_results is not None:\n",
    "            chain_all = chain_results[model]\n",
    "            chain_means = np.mean(chain_all, axis=0)\n",
    "            chain_sd = np.std(chain_all, axis=0)\n",
    "            chain_mean_sd = [\"${} \\pm {}$\".format(round(chain_means[i], 2), round(chain_sd[i], 2)) for i in range(5)]\n",
    "            chain_mean_sd_str = \" & \" + \" & \".join(chain_mean_sd) + \" & \" + total_mean_sd + \" \\\\\\\\\"\n",
    "            print(\"Chain values:\")\n",
    "            print(chain_mean_sd_str)\n",
    "\n",
    "\n",
    "        high_started_per_category = {task_category: np.array([0, 0, 0]) for task_category in set(high_categories.values())}\n",
    "        high_completed_per_category = {task_category: np.array([0, 0, 0]) for task_category in set(high_categories.values())}\n",
    "        low_started_per_category = {task_category: np.array([0, 0, 0]) for task_category in set(low_categories.values())}\n",
    "        low_completed_per_category = {task_category: np.array([0, 0, 0]) for task_category in set(low_categories.values())}\n",
    "\n",
    "        high_mean_sd_per_category = {}\n",
    "        low_mean_sd_per_category = {}\n",
    "\n",
    "        # For each task in each category, get the number of times it was started and completed.\n",
    "        for i in range(3):\n",
    "            if high_level_started != None:\n",
    "                for task in high_categories.keys():\n",
    "                    high_started_counter = high_level_started[model][i]\n",
    "                    if task in high_started_counter:\n",
    "                        high_started_per_category[high_categories[task]][i] += high_started_counter[task]\n",
    "                    high_finished_counter = high_level_completed[model][i]\n",
    "                    high_completed_per_category[high_categories[task]][i] += high_finished_counter[task]\n",
    "\n",
    "            if low_level_started != None:\n",
    "                for task in low_categories.keys():\n",
    "                    low_started_counter = low_level_started[model][i]\n",
    "                    if task in low_started_counter:\n",
    "                        low_started_per_category[low_categories[task]][i] += low_started_counter[task]\n",
    "\n",
    "                    low_finished_counter = low_level_completed[model][i]\n",
    "                    low_completed_per_category[low_categories[task]][i] += low_finished_counter[task]\n",
    "\n",
    "        if high_level_started != None:\n",
    "            avg_per_category = {}\n",
    "            high_mean_across_categories = np.array([0., 0., 0.])\n",
    "\n",
    "            for task_category in ordered_high:\n",
    "                avg_per_category[task_category] = high_completed_per_category[task_category] / high_started_per_category[task_category] \n",
    "                high_mean_across_categories += avg_per_category[task_category]\n",
    "                high_mean_sd_per_category[task_category] = '${} \\pm {}$'.format(round(np.mean(avg_per_category[task_category]), 2), \n",
    "                                                                        round(np.std(avg_per_category[task_category]), 2))\n",
    "            high_mean_across_categories /= len(ordered_high)\n",
    "            high_mean_sd_per_category['avg'] = '${} \\pm {}$'.format(round(np.mean(high_mean_across_categories), 2), round(np.std(high_mean_across_categories), 2))\n",
    "            print(\"High:\")\n",
    "            print(\" & \" + \" & \".join([high_mean_sd_per_category[category] for category in high_mean_sd_per_category.keys()]) + '\\\\')\n",
    "\n",
    "        if low_level_started != None:\n",
    "\n",
    "            avg_per_category = {}\n",
    "            low_mean_across_categories =  np.array([0., 0., 0.])\n",
    "            for task_category in ordered_low:\n",
    "                print(task_category, low_completed_per_category[task_category], low_started_per_category[task_category] )\n",
    "                avg_per_category[task_category] = low_completed_per_category[task_category] / low_started_per_category[task_category] \n",
    "                low_mean_across_categories += avg_per_category[task_category]\n",
    "                low_mean_sd_per_category[task_category] = '${} \\pm {}$'.format(round(np.mean(avg_per_category[task_category]), 2), \n",
    "                                                                        round(np.std(avg_per_category[task_category]), 2))\n",
    "            low_mean_across_categories /= len(ordered_low)\n",
    "            low_mean_sd_per_category['avg'] = '${} \\pm {}$'.format(round(np.mean(low_mean_across_categories), 2), round(np.std(low_mean_across_categories), 2))\n",
    "            print(\"Low:\")\n",
    "            print(\" & \" + \" & \".join([low_mean_sd_per_category[category] for category in low_mean_sd_per_category.keys()]) + '\\\\')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "====== ONE BY ONE ======\n",
      "resnet\n",
      "High:\n",
      " & $0.07 \\pm 0.04$ & $0.02 \\pm 0.0$ & $0.0 \\pm 0.0$ & $0.49 \\pm 0.07$ & $0.01 \\pm 0.01$ & $0.0 \\pm 0.0$ & $0.1 \\pm 0.02$\\\n",
      "grasp_door [50 59 49] [99 99 99]\n",
      "drawer [10 19  1] [50 59 49]\n",
      "grasp_block [186 171 175] [295 295 295]\n",
      "contact [2 3 4] [106 106 106]\n",
      "ungrasp [42 46 25] [60 64 42]\n",
      "lift [2 0 1] [133 118 122]\n",
      "push [2 3 2] [2 3 4]\n",
      "rotate [49 45 41] [53 53 53]\n",
      "place [1 0 0] [1 0 0]\n",
      "Low:\n",
      " & $0.53 \\pm 0.05$ & $0.18 \\pm 0.12$ & $0.6 \\pm 0.02$ & $0.03 \\pm 0.01$ & $0.67 \\pm 0.05$ & $0.01 \\pm 0.01$ & $0.83 \\pm 0.24$ & $0.85 \\pm 0.06$ & $nan \\pm nan$ & $nan \\pm nan$\\\n",
      "voltron\n",
      "High:\n",
      " & $0.16 \\pm 0.15$ & $0.08 \\pm 0.06$ & $0.11 \\pm 0.08$ & $0.33 \\pm 0.07$ & $0.06 \\pm 0.05$ & $0.0 \\pm 0.0$ & $0.12 \\pm 0.02$\\\n",
      "grasp_door [60 60 88] [99 99 99]\n",
      "drawer [ 9 11 80] [60 60 88]\n",
      "grasp_block [199 193 206] [295 295 295]\n",
      "contact [24  9 27] [106 106 106]\n",
      "ungrasp [36 25 58] [ 54  56 113]\n",
      "lift [20 47  8] [142 138 163]\n",
      "push [ 6  2 18] [24  9 27]\n",
      "rotate [42 38 32] [57 55 43]\n",
      "place [3 7 1] [10 17  3]\n",
      "Low:\n",
      " & $0.7 \\pm 0.13$ & $0.41 \\pm 0.35$ & $0.68 \\pm 0.02$ & $0.19 \\pm 0.07$ & $0.54 \\pm 0.09$ & $0.18 \\pm 0.12$ & $0.38 \\pm 0.2$ & $0.72 \\pm 0.02$ & $0.35 \\pm 0.05$ & $0.46 \\pm 0.07$\\\n",
      "\n",
      "\n",
      "====== CONJUNCTION ======\n",
      "resnet\n",
      "High:\n",
      " & $0.47 \\pm 0.15$ & $0.2 \\pm 0.02$ & $0.07 \\pm 0.02$ & $0.22 \\pm 0.03$ & $0.0 \\pm 0.0$ & $0.0 \\pm 0.0$ & $0.16 \\pm 0.03$\\\n",
      "voltron\n",
      "High:\n",
      " & $0.67 \\pm 0.2$ & $0.25 \\pm 0.21$ & $0.17 \\pm 0.03$ & $0.24 \\pm 0.05$ & $0.0 \\pm 0.0$ & $0.02 \\pm 0.02$ & $0.22 \\pm 0.08$\\\n",
      "\n",
      "\n",
      "====== RANDOM ======\n",
      "resnet\n",
      "Chain values:\n",
      " & $0.38 \\pm 0.02$ & $0.22 \\pm 0.02$ & $0.1 \\pm 0.01$ & $0.04 \\pm 0.0$ & $0.02 \\pm 0.01$ & $0.75 \\pm 0.04$ \\\\\n",
      "grasp_door [73 59 55] [130 136 135]\n",
      "drawer [12  9  0] [22 20 15]\n",
      "grasp_block [148 149 136] [254 255 252]\n",
      "contact [4 5 7] [189 195 186]\n",
      "ungrasp [77 88 69] [126 116  99]\n",
      "lift [0 2 0] [72 72 65]\n",
      "push [1 3 5] [3 5 7]\n",
      "rotate [74 75 71] [79 82 77]\n",
      "place [0 0 0] [0 0 0]\n",
      "Low:\n",
      " & $0.47 \\pm 0.07$ & $0.33 \\pm 0.24$ & $0.57 \\pm 0.02$ & $0.03 \\pm 0.01$ & $0.69 \\pm 0.06$ & $0.01 \\pm 0.01$ & $0.55 \\pm 0.16$ & $0.92 \\pm 0.01$ & $nan \\pm nan$ & $nan \\pm nan$\\\n",
      "voltron\n",
      "Chain values:\n",
      " & $0.51 \\pm 0.05$ & $0.26 \\pm 0.05$ & $0.1 \\pm 0.01$ & $0.05 \\pm 0.01$ & $0.02 \\pm 0.01$ & $0.94 \\pm 0.11$ \\\\\n",
      "grasp_door [ 67  72 105] [125 132 145]\n",
      "drawer [ 6  3 28] [20 20 34]\n",
      "grasp_block [171 169 182] [259 255 277]\n",
      "contact [19 21 41] [185 194 215]\n",
      "ungrasp [ 63  77 117] [134 125 172]\n",
      "lift [17 11  3] [82 83 92]\n",
      "push [ 6  6 24] [19 21 40]\n",
      "rotate [81 74 46] [94 87 55]\n",
      "place [4 2 2] [9 5 5]\n",
      "Low:\n",
      " & $0.6 \\pm 0.09$ & $0.42 \\pm 0.29$ & $0.66 \\pm 0.0$ & $0.13 \\pm 0.04$ & $0.59 \\pm 0.09$ & $0.12 \\pm 0.07$ & $0.4 \\pm 0.14$ & $0.85 \\pm 0.01$ & $0.41 \\pm 0.02$ & $0.47 \\pm 0.06$\\\n",
      "\n",
      "\n",
      "====== MEDIUM ======\n",
      "resnet\n",
      "Chain values:\n",
      " & $0.66 \\pm 0.02$ & $0.32 \\pm 0.02$ & $0.11 \\pm 0.02$ & $0.03 \\pm 0.0$ & $0.01 \\pm 0.0$ & $1.12 \\pm 0.06$ \\\\\n",
      "slider/drawer [368 343 318] [402 382 391]\n",
      "push [107 102  96] [186 183 178]\n",
      "lift [61 57 57] [290 279 257]\n",
      "rotate [28 24 33] [157 151 142]\n",
      "place [36 28 27] [56 54 51]\n",
      "stacking [0 0 0] [5 2 6]\n",
      "Low:\n",
      " & $0.88 \\pm 0.04$ & $0.56 \\pm 0.01$ & $0.21 \\pm 0.01$ & $0.19 \\pm 0.03$ & $0.56 \\pm 0.06$ & $0.0 \\pm 0.0$ & $0.4 \\pm 0.01$\\\n",
      "voltron\n",
      "Chain values:\n",
      " & $0.58 \\pm 0.18$ & $0.3 \\pm 0.14$ & $0.14 \\pm 0.08$ & $0.06 \\pm 0.04$ & $0.02 \\pm 0.02$ & $1.11 \\pm 0.45$ \\\\\n",
      "slider/drawer [124 374 375] [299 399 394]\n",
      "push [ 32 100  90] [135 201 193]\n",
      "lift [ 46 178 114] [146 309 294]\n",
      "rotate [11 35 31] [118 158 165]\n",
      "place [31 67 61] [ 45 155 104]\n",
      "stacking [0 0 0] [ 0 12  5]\n",
      "Low:\n",
      " & $0.77 \\pm 0.25$ & $0.4 \\pm 0.12$ & $0.43 \\pm 0.11$ & $0.17 \\pm 0.05$ & $0.57 \\pm 0.11$ & $nan \\pm nan$ & $nan \\pm nan$\\\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch-ssd/slurm-jobs/gguz/206561/ipykernel_72182/4155326532.py:75: RuntimeWarning: invalid value encountered in divide\n",
      "  avg_per_category[task_category] = low_completed_per_category[task_category] / low_started_per_category[task_category]\n"
     ]
    }
   ],
   "source": [
    "print()\n",
    "print()\n",
    "print(\"====== ONE BY ONE ======\")\n",
    "parse_results(\n",
    "    models=[\"resnet\", \"voltron\"],\n",
    "    avg_rates=onebyone_res['avg_rates'],\n",
    "    low_categories=low_task_categories,\n",
    "    high_categories=high_task_categories,\n",
    "    low_level_started=onebyone_res['low_level_started'],\n",
    "    low_level_completed=onebyone_res['low_level_completed'],\n",
    "    high_level_started=onebyone_res['high_level_started'],\n",
    "    high_level_completed=onebyone_res['high_level_completed'],\n",
    "    ordered_high=ordered_high_categories,\n",
    "    ordered_low=ordered_low_categories\n",
    ")\n",
    "print()\n",
    "print()\n",
    "print(\"====== CONJUNCTION ======\")\n",
    "parse_results(\n",
    "    models=[\"resnet\", \"voltron\"],\n",
    "    avg_rates=conjunction_res['avg_rates'],\n",
    "    high_level_started=conjunction_res['high_level_started'],\n",
    "    high_level_completed=conjunction_res['high_level_completed'],\n",
    "    low_categories=low_task_categories,\n",
    "    high_categories=high_task_categories,\n",
    "    ordered_high=ordered_high_categories,\n",
    "    ordered_low=ordered_low_categories\n",
    ")\n",
    "\n",
    "print()\n",
    "print()\n",
    "print(\"====== RANDOM ======\")\n",
    "parse_results(\n",
    "    models=[\"resnet\", \"voltron\"],\n",
    "    avg_rates=random_res['avg_rates'],\n",
    "    chain_results=random_res['chain_results'],\n",
    "    low_categories=low_task_categories,\n",
    "    high_categories=high_task_categories,\n",
    "    low_level_started=random_res['low_level_started'],\n",
    "    low_level_completed=random_res['low_level_completed'],\n",
    "    ordered_high=ordered_high_categories,\n",
    "    ordered_low=ordered_low_categories\n",
    ")\n",
    "\n",
    "print()\n",
    "print()\n",
    "print(\"====== MEDIUM CHAIN ======\")\n",
    "parse_results(\n",
    "    models=[\"resnet\", \"voltron\"],\n",
    "    avg_rates=medium_res['avg_rates'],\n",
    "    chain_results=medium_res['chain_results'],\n",
    ")\n",
    "\n",
    "print()\n",
    "print()\n",
    "print(\"====== MEDIUM SINGLE ======\")\n",
    "parse_results(\n",
    "    models=[\"resnet\", \"voltron\"],\n",
    "    avg_rates=medium_single_res['avg_rates'],\n",
    "    low_categories=high_task_categories,\n",
    "    low_level_started=medium_single_res['low_level_started'],\n",
    "    low_level_completed=medium_single_res['low_level_completed'],\n",
    "    ordered_low=ordered_high_categories\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-by-one results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resnet $0.66 \\pm 0.04$\n",
      "High:\n",
      " & $0.07 \\pm 0.04$ & $0.02 \\pm 0.0$ & $0.0 \\pm 0.0$ & $0.49 \\pm 0.07$ & $0.01 \\pm 0.01$ & $0.0 \\pm 0.0$ & $0.1 \\pm 0.02$\\\n",
      "grasp_door [50 59 49] [99 99 99]\n",
      "drawer [10 19  1] [50 59 49]\n",
      "grasp_block [186 171 175] [295 295 295]\n",
      "contact [2 3 4] [106 106 106]\n",
      "ungrasp [42 46 25] [60 64 42]\n",
      "lift [2 0 1] [133 118 122]\n",
      "push [2 3 2] [2 3 4]\n",
      "rotate [49 45 41] [53 53 53]\n",
      "place [1 0 0] [1 0 0]\n",
      "Low:\n",
      " & $0.53 \\pm 0.05$ & $0.18 \\pm 0.12$ & $0.6 \\pm 0.02$ & $0.03 \\pm 0.01$ & $0.67 \\pm 0.05$ & $0.01 \\pm 0.01$ & $0.83 \\pm 0.24$ & $0.85 \\pm 0.06$ & $nan \\pm nan$ & $nan \\pm nan$\\\n",
      "voltron $0.87 \\pm 0.1$\n",
      "High:\n",
      " & $0.16 \\pm 0.15$ & $0.08 \\pm 0.06$ & $0.11 \\pm 0.08$ & $0.33 \\pm 0.07$ & $0.06 \\pm 0.05$ & $0.0 \\pm 0.0$ & $0.12 \\pm 0.02$\\\n",
      "grasp_door [60 60 88] [99 99 99]\n",
      "drawer [ 9 11 80] [60 60 88]\n",
      "grasp_block [199 193 206] [295 295 295]\n",
      "contact [24  9 27] [106 106 106]\n",
      "ungrasp [36 25 58] [ 54  56 113]\n",
      "lift [20 47  8] [142 138 163]\n",
      "push [ 6  2 18] [24  9 27]\n",
      "rotate [42 38 32] [57 55 43]\n",
      "place [3 7 1] [10 17  3]\n",
      "Low:\n",
      " & $0.7 \\pm 0.13$ & $0.41 \\pm 0.35$ & $0.68 \\pm 0.02$ & $0.19 \\pm 0.07$ & $0.54 \\pm 0.09$ & $0.18 \\pm 0.12$ & $0.38 \\pm 0.2$ & $0.72 \\pm 0.02$ & $0.35 \\pm 0.05$ & $0.46 \\pm 0.07$\\\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch-ssd/slurm-jobs/gguz/206505/ipykernel_3698611/3724112925.py:51: RuntimeWarning: invalid value encountered in divide\n",
      "  avg_per_category[task_category] = low_completed_per_category[task_category] / low_started_per_category[task_category]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#for model in ['small', 'base']:\n",
    "for model in ['resnet', 'voltron']:\n",
    "    mean, sd = np.mean(onebyone_res['avg_rates'][model]), np.std(onebyone_res['avg_rates'][model])\n",
    "    print(model, '${:.2}'.format(mean), \"\\pm\", '{:.1}$'.format(sd))\n",
    "    \n",
    "    high_started_per_category = {task_category: np.array([0, 0, 0]) for task_category in set(high_task_categories.values())}\n",
    "    high_completed_per_category = {task_category: np.array([0, 0, 0]) for task_category in set(high_task_categories.values())}\n",
    "    low_started_per_category = {task_category: np.array([0, 0, 0]) for task_category in set(low_task_categories.values())}\n",
    "    low_completed_per_category = {task_category: np.array([0, 0, 0]) for task_category in set(low_task_categories.values())}\n",
    "\n",
    "    high_mean_sd_per_category = {}\n",
    "    low_mean_sd_per_category = {}\n",
    "\n",
    "    # 1) Get the counts for each category, within a seed.\n",
    "    for i in range(3):\n",
    "        for task in high_task_categories.keys():\n",
    "            high_started_counter = onebyone_res['high_level_started'][model][i]\n",
    "            if task in high_started_counter:\n",
    "                high_started_per_category[high_task_categories[task]][i] += high_started_counter[task]\n",
    "            high_finished_counter = onebyone_res['high_level_completed'][model][i]\n",
    "            high_completed_per_category[high_task_categories[task]][i] += high_finished_counter[task]\n",
    "\n",
    "        for task in low_task_categories.keys():\n",
    "            low_started_counter = onebyone_res['low_level_started'][model][i]\n",
    "            if task in low_started_counter:\n",
    "                low_started_per_category[low_task_categories[task]][i] += low_started_counter[task]\n",
    "\n",
    "            low_finished_counter = onebyone_res['low_level_completed'][model][i]\n",
    "            low_completed_per_category[low_task_categories[task]][i] += low_finished_counter[task]\n",
    "\n",
    "\n",
    "    high_categories = ['slider/drawer', 'push', 'lift', 'rotate', 'place', 'stacking']\n",
    "    avg_per_category = {}\n",
    "    high_mean_across_categories = np.array([0., 0., 0.])\n",
    "\n",
    "    for task_category in high_categories:\n",
    "        avg_per_category[task_category] = high_completed_per_category[task_category] / high_started_per_category[task_category] \n",
    "        high_mean_across_categories += avg_per_category[task_category]\n",
    "        high_mean_sd_per_category[task_category] = '${} \\pm {}$'.format(round(np.mean(avg_per_category[task_category]), 2), \n",
    "                                                                   round(np.std(avg_per_category[task_category]), 2))\n",
    "    high_mean_across_categories /= len(high_categories)\n",
    "    high_mean_sd_per_category['avg'] = '${} \\pm {}$'.format(round(np.mean(high_mean_across_categories), 2), round(np.std(high_mean_across_categories), 2))\n",
    "    print(\"High:\")\n",
    "    print(\" & \" + \" & \".join([high_mean_sd_per_category[category] for category in high_mean_sd_per_category.keys()]) + '\\\\')\n",
    "\n",
    "    low_categories = ['grasp_door', 'drawer', 'grasp_block', 'contact', 'ungrasp', 'lift', 'push', 'rotate', 'place']\n",
    "    avg_per_category = {}\n",
    "    low_mean_across_categories =  np.array([0., 0., 0.])\n",
    "    for task_category in low_categories:\n",
    "        print(task_category, low_completed_per_category[task_category], low_started_per_category[task_category] )\n",
    "        avg_per_category[task_category] = low_completed_per_category[task_category] / low_started_per_category[task_category] \n",
    "        low_mean_across_categories += avg_per_category[task_category]\n",
    "        low_mean_sd_per_category[task_category] = '${} \\pm {}$'.format(round(np.mean(avg_per_category[task_category]), 2), \n",
    "                                                                   round(np.std(avg_per_category[task_category]), 2))\n",
    "    low_mean_across_categories /= len(low_categories)\n",
    "    low_mean_sd_per_category['avg'] = '${} \\pm {}$'.format(round(np.mean(low_mean_across_categories), 2), round(np.std(low_mean_across_categories), 2))\n",
    "    print(\"Low:\")\n",
    "    print(\" & \" + \" & \".join([low_mean_sd_per_category[category] for category in low_mean_sd_per_category.keys()]) + '\\\\')\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conjunction results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resnet $0.18 \\pm 0.03$\n",
      "slider/drawer [38 67 34] [99 99 99]\n",
      "push [23 22 19] [106 106 106]\n",
      "lift [ 7 13 10] [139 139 139]\n",
      "rotate [16 12 12] [62 62 62]\n",
      "place [0 0 0] [54 54 54]\n",
      "stacking [0 0 0] [40 40 40]\n",
      " & $0.47 \\pm 0.15$ & $0.2 \\pm 0.02$ & $0.07 \\pm 0.02$ & $0.22 \\pm 0.03$ & $0.0 \\pm 0.0$ & $0.0 \\pm 0.0$ & $0.16 \\pm 0.03$\\\n",
      "voltron $0.26 \\pm 0.09$\n",
      "slider/drawer [46 93 61] [99 99 99]\n",
      "push [10 58 11] [106 106 106]\n",
      "lift [26 26 18] [139 139 139]\n",
      "rotate [12 19 13] [62 62 62]\n",
      "place [0 0 0] [54 54 54]\n",
      "stacking [0 0 2] [40 40 40]\n",
      " & $0.67 \\pm 0.2$ & $0.25 \\pm 0.21$ & $0.17 \\pm 0.03$ & $0.24 \\pm 0.05$ & $0.0 \\pm 0.0$ & $0.02 \\pm 0.02$ & $0.22 \\pm 0.08$\\\n"
     ]
    }
   ],
   "source": [
    "#for model in ['small', 'base']:\n",
    "for model in ['resnet', 'voltron']:\n",
    "    mean, sd = np.mean(conjunction_res['avg_rates'][model]), np.std(conjunction_res['avg_rates'][model])\n",
    "    print(model, '${:.2}'.format(mean), \"\\pm\", '{:.1}$'.format(sd))\n",
    "    \n",
    "    started_per_category = {task_category: np.array([0, 0, 0]) for task_category in set(high_task_categories.values())}\n",
    "    completed_per_category = {task_category: np.array([0, 0, 0]) for task_category in set(high_task_categories.values())}\n",
    "    fraction_per_category = {}\n",
    "    mean_sd_per_category = {}\n",
    "\n",
    "    # 1) Get the counts for each category, within a seed.\n",
    "    for i in range(3):\n",
    "\n",
    "        for task in high_task_categories.keys():\n",
    "\n",
    "            started_counter = conjunction_res['high_level_started'][model][i]\n",
    "            if task in started_counter:\n",
    "                started_per_category[high_task_categories[task]][i] += started_counter[task]\n",
    "            finished_counter = conjunction_res['high_level_completed'][model][i]\n",
    "            completed_per_category[high_task_categories[task]][i] += finished_counter[task]\n",
    "        #rate_counter = started_counter / finished_counter\n",
    "\n",
    "\n",
    "\n",
    "    high_categories = ['slider/drawer', 'push', 'lift', 'rotate', 'place', 'stacking']\n",
    "    mean_across_categories = np.array([0., 0., 0.])\n",
    "\n",
    "    for task_category in high_categories:\n",
    "        print(task_category, completed_per_category[task_category], started_per_category[task_category])\n",
    "        fraction_per_category[task_category] = completed_per_category[task_category] / started_per_category[task_category] \n",
    "        mean_across_categories += fraction_per_category[task_category]\n",
    "        mean_sd_per_category[task_category] = '${} \\pm {}$'.format(round(np.mean(fraction_per_category[task_category]), 2), \n",
    "                                                                   round(np.std(fraction_per_category[task_category]), 2))\n",
    "        \n",
    "    mean_across_categories = mean_across_categories / len(high_categories)\n",
    "\n",
    "    mean_sd_per_category['avg'] = '${} \\pm {}$'.format(round(np.mean(mean_across_categories), 2), round(np.std(mean_across_categories), 2))\n",
    "\n",
    "    print(\" & \" + \" & \".join([mean_sd_per_category[category] for category in mean_sd_per_category.keys()]) + '\\\\')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random sequences results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resnet\n",
      "Chain values:\n",
      "$0.38 \\pm 0.02$ & $0.22 \\pm 0.02$ & $0.1 \\pm 0.01$ & $0.04 \\pm 0.0$ & $0.02 \\pm 0.01$ & $0.75 \\pm 0.04$ \\\\\n",
      "grasp_door [110 119 137] [198 198 198]\n",
      "drawer [19 30 81] [110 119 137]\n",
      "grasp_block [385 364 381] [590 590 590]\n",
      "contact [26 12 31] [212 212 212]\n",
      "ungrasp [78 71 83] [114 120 155]\n",
      "lift [22 47  9] [275 256 285]\n",
      "push [ 8  5 20] [26 12 31]\n",
      "rotate [91 83 73] [110 108  96]\n",
      "place [4 7 1] [11 17  3]\n",
      "Low:\n",
      " & $0.62 \\pm 0.06$ & $0.34 \\pm 0.18$ & $0.64 \\pm 0.02$ & $0.11 \\pm 0.04$ & $0.6 \\pm 0.06$ & $0.1 \\pm 0.06$ & $0.46 \\pm 0.14$ & $0.79 \\pm 0.03$ & $0.37 \\pm 0.03$ & $0.45 \\pm 0.03$\\\n",
      "voltron\n",
      "Chain values:\n",
      "$0.51 \\pm 0.05$ & $0.26 \\pm 0.05$ & $0.1 \\pm 0.01$ & $0.05 \\pm 0.01$ & $0.02 \\pm 0.01$ & $0.94 \\pm 0.11$ \\\\\n",
      "grasp_door [170 179 225] [297 297 297]\n",
      "drawer [ 28  41 161] [170 179 225]\n",
      "grasp_block [584 557 587] [885 885 885]\n",
      "contact [50 21 58] [318 318 318]\n",
      "ungrasp [114  96 141] [168 176 268]\n",
      "lift [42 94 17] [417 394 448]\n",
      "push [14  7 38] [50 21 58]\n",
      "rotate [133 121 105] [167 163 139]\n",
      "place [ 7 14  2] [21 34  6]\n",
      "Low:\n",
      " & $0.64 \\pm 0.08$ & $0.37 \\pm 0.25$ & $0.65 \\pm 0.02$ & $0.14 \\pm 0.05$ & $0.58 \\pm 0.07$ & $0.13 \\pm 0.08$ & $0.42 \\pm 0.17$ & $0.76 \\pm 0.02$ & $0.36 \\pm 0.04$ & $0.45 \\pm 0.04$\\\n"
     ]
    }
   ],
   "source": [
    "#for model in ['small', 'base']:\n",
    "for model in ['resnet', 'voltron']:   \n",
    "    mean, sd = np.mean(random_res['avg_rates'][model]), np.std(random_res['avg_rates'][model])\n",
    "    print(model)\n",
    "    total_mean_sd = '${}'.format(round(mean, 2)) + \" \\pm \" + '{}$'.format(round(sd, 2))\n",
    "    chain_all = random_res['chain_results'][model]\n",
    "    chain_means = np.mean(chain_all, axis=0)\n",
    "    chain_sd = np.std(chain_all, axis=0)\n",
    "    chain_mean_sd = [\"${} \\pm {}$\".format(round(chain_means[i], 2), round(chain_sd[i], 2)) for i in range(5)]\n",
    "    chain_mean_sd_str = \" & \".join(chain_mean_sd) + \" & \" + total_mean_sd + \" \\\\\\\\\"\n",
    "    print(\"Chain values:\")\n",
    "    print(chain_mean_sd_str)\n",
    "\n",
    "    started_per_category = {task_category: np.array([0, 0, 0]) for task_category in set(low_task_categories.values())}\n",
    "    completed_per_category = {task_category: np.array([0, 0, 0]) for task_category in set(low_task_categories.values())}\n",
    "    fraction_per_category = {}\n",
    "    mean_sd_per_category = {}\n",
    "\n",
    "    # 1) Get the counts for each category, within a seed.\n",
    "    for i in range(3):\n",
    "        for task in low_task_categories.keys():\n",
    "            started_counter = random_res['low_level_started'][model][i]\n",
    "            if task in low_started_counter:\n",
    "                started_per_category[low_task_categories[task]][i] += started_counter[task]\n",
    "\n",
    "            finished_counter = random_res['low_level_completed'][model][i]\n",
    "            completed_per_category[low_task_categories[task]][i] += low_finished_counter[task]\n",
    "\n",
    "\n",
    "    low_categories = ['grasp_door', 'drawer', 'grasp_block', 'contact', 'ungrasp', 'lift', 'push', 'rotate', 'place']\n",
    "    avg_per_category = {}\n",
    "    low_mean_across_categories =  np.array([0., 0., 0.])\n",
    "    low_mean_sd_per_category = {}\n",
    "    for task_category in low_categories:\n",
    "        print(task_category, completed_per_category[task_category],  started_per_category[task_category] )\n",
    "        avg_per_category[task_category] = completed_per_category[task_category] / started_per_category[task_category] \n",
    "        low_mean_across_categories += avg_per_category[task_category]\n",
    "        low_mean_sd_per_category[task_category] = '${} \\pm {}$'.format(round(np.mean(avg_per_category[task_category]), 2), \n",
    "                                                                   round(np.std(avg_per_category[task_category]), 2))\n",
    "    low_mean_across_categories /= len(low_categories)\n",
    "    low_mean_sd_per_category['avg'] = '${} \\pm {}$'.format(round(np.mean(low_mean_across_categories), 2), round(np.std(low_mean_across_categories), 2))\n",
    "    print(\"Low:\")\n",
    "    print(\" & \" + \" & \".join([low_mean_sd_per_category[category] for category in low_mean_sd_per_category.keys()]) + '\\\\')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Random sequences results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Medium results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resnet $1.1 \\pm 0.06$\n",
      "slider/drawer [368 343 318] [402 382 391]\n",
      "push [107 102  96] [186 183 178]\n",
      "lift [61 57 57] [290 279 257]\n",
      "rotate [28 24 33] [157 151 142]\n",
      "place [0 0 0] [0 0 0]\n",
      "stacking [0 0 0] [5 2 6]\n",
      " & $0.88 \\pm 0.04$ & $0.56 \\pm 0.01$ & $0.21 \\pm 0.01$ & $0.19 \\pm 0.03$ & $nan \\pm nan$ & $0.0 \\pm 0.0$ & $nan \\pm nan$\\\n",
      "voltron $1.1 \\pm 0.4$\n",
      "slider/drawer [124 374 375] [299 399 394]\n",
      "push [ 32 100  90] [135 201 193]\n",
      "lift [ 46 178 114] [146 309 294]\n",
      "rotate [11 35 31] [118 158 165]\n",
      "place [0 0 0] [0 0 0]\n",
      "stacking [0 0 0] [ 0 12  5]\n",
      " & $0.77 \\pm 0.25$ & $0.4 \\pm 0.12$ & $0.43 \\pm 0.11$ & $0.17 \\pm 0.05$ & $nan \\pm nan$ & $nan \\pm nan$ & $nan \\pm nan$\\\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch-ssd/slurm-jobs/gguz/206505/ipykernel_3698611/2869242101.py:30: RuntimeWarning: invalid value encountered in divide\n",
      "  fraction_per_category[task_category] = completed_per_category[task_category] / started_per_category[task_category]\n"
     ]
    }
   ],
   "source": [
    "#for model in ['small', 'base']:\n",
    "for model in ['resnet', 'voltron']:\n",
    "    mean, sd = np.mean(medium_res['avg_rates'][model]), np.std(medium_res['avg_rates'][model])\n",
    "    print(model, '${:.2}'.format(mean), \"\\pm\", '{:.1}$'.format(sd))\n",
    "    \n",
    "    started_per_category = {task_category: np.array([0, 0, 0]) for task_category in set(high_task_categories.values())}\n",
    "    completed_per_category = {task_category: np.array([0, 0, 0]) for task_category in set(high_task_categories.values())}\n",
    "    fraction_per_category = {}\n",
    "    mean_sd_per_category = {}\n",
    "\n",
    "    # 1) Get the counts for each category, within a seed.\n",
    "    for i in range(3):\n",
    "\n",
    "        for task in high_task_categories.keys():\n",
    "\n",
    "            started_counter = medium_res['low_level_started'][model][i]\n",
    "            if task in started_counter:\n",
    "                started_per_category[high_task_categories[task]][i] += started_counter[task]\n",
    "            finished_counter = medium_res['low_level_completed'][model][i]\n",
    "            completed_per_category[high_task_categories[task]][i] += finished_counter[task]\n",
    "        #rate_counter = started_counter / finished_counter\n",
    "\n",
    "\n",
    "\n",
    "    high_categories = ['slider/drawer', 'push', 'lift', 'rotate', 'place', 'stacking']\n",
    "    mean_across_categories = np.array([0., 0., 0.])\n",
    "\n",
    "    for task_category in high_categories:\n",
    "        print(task_category, completed_per_category[task_category], started_per_category[task_category])\n",
    "        fraction_per_category[task_category] = completed_per_category[task_category] / started_per_category[task_category] \n",
    "        mean_across_categories += fraction_per_category[task_category]\n",
    "        mean_sd_per_category[task_category] = '${} \\pm {}$'.format(round(np.mean(fraction_per_category[task_category]), 2), \n",
    "                                                                   round(np.std(fraction_per_category[task_category]), 2))\n",
    "        \n",
    "    mean_across_categories = mean_across_categories / len(high_categories)\n",
    "\n",
    "    mean_sd_per_category['avg'] = '${} \\pm {}$'.format(round(np.mean(mean_across_categories), 2), round(np.std(mean_across_categories), 2))\n",
    "\n",
    "    print(\" & \" + \" & \".join([mean_sd_per_category[category] for category in mean_sd_per_category.keys()]) + '\\\\')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
